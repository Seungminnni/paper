#!/usr/bin/env python3
"""
Word2Vec Mosaic Information Preservation Test
ëª©ì : í…ìŠ¤íŠ¸ â†’ Word2Vec â†’ ì´ë¯¸ì§€ â†’ í•™ìŠµ â†’ ë³µì› ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Reshape, Conv2D, Flatten, Dropout, Conv2DTranspose
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# GPU ì„¤ì • ë° í™•ì¸
print("=== GPU ì„¤ì • í™•ì¸ ===")
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        # GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš© ì„¤ì •
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU ê°ì§€ë¨: {len(gpus)}ê°œ")
        for i, gpu in enumerate(gpus):
            print(f"   GPU {i}: {gpu.name}")
    except RuntimeError as e:
        print(f"âŒ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
else:
    print("âš ï¸  GPUê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ. CPU ì‚¬ìš©")

print(f"ğŸ”§ TensorFlow ë²„ì „: {tf.__version__}")
print(f"ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {tf.config.list_logical_devices()}")

print("\n=== Word2Vec Mosaic Information Preservation Test ===")
print("í…ìŠ¤íŠ¸ â†’ Word2Vec â†’ ëª¨ìì´í¬ ì´ë¯¸ì§€ â†’ ì˜¤í† ì¸ì½”ë” â†’ ì›ë³¸ ë³µì› í…ŒìŠ¤íŠ¸")

class Word2VecProcessor:
    """Word2Vec ë²¡í„°í™” ì²˜ë¦¬"""
    def __init__(self, vector_size=512, window=5, min_count=1):
        self.vector_size = vector_size
        self.window = window
        self.min_count = min_count
        self.w2v_model = None

    def train_word2vec(self, sentences):
        print(f"ğŸ§  Word2Vec í•™ìŠµ ì¤‘ (ë²¡í„° í¬ê¸°: {self.vector_size})...")
        self.w2v_model = Word2Vec(sentences, vector_size=self.vector_size, 
                                  window=self.window, min_count=self.min_count, workers=4)
        print("âœ… Word2Vec í•™ìŠµ ì™„ë£Œ")

    def vectorize_data(self, df, text_column):
        # ë¬¸ì¥ ì¤€ë¹„
        sentences = [row.split() for row in df[text_column].astype(str)]
        
        print(f"ğŸ“Š ë¬¸ì¥ í†µê³„:")
        print(f"  - ì´ ë¬¸ì¥ ìˆ˜: {len(sentences)}")
        print(f"  - í‰ê·  ê¸¸ì´: {np.mean([len(s) for s in sentences]):.1f} ë‹¨ì–´")
        
        self.train_word2vec(sentences)
        
        # ğŸ”§ ë²¡í„°í™” ë°©ë²• ì„ íƒ
        print("ğŸ”§ ë²¡í„°í™” ë°©ë²•: ì—°ê²°(Concatenation) ë°©ì‹ ì‚¬ìš©")
        print("   - ê¸°ì¡´ ë¬¸ì œ: í‰ê· í™”ë¡œ ì¸í•œ ì •ë³´ ì†ì‹¤")
        print("   - ê°œì„  ë°©ë²•: ë‹¨ì–´ ë²¡í„°ë“¤ì„ ì—°ê²°í•˜ì—¬ ê³ ìœ ì„± ë³´ì¡´")
        
        vectors = []
        max_words = 8  # ìµœëŒ€ ë‹¨ì–´ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ê³ ë ¤)
        
        for sentence in sentences:
            # ë‹¨ì–´ë³„ ë²¡í„° ìˆ˜ì§‘
            word_vectors = []
            for word in sentence[:max_words]:  # ì²˜ìŒ 8ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
                if word in self.w2v_model.wv:
                    word_vectors.append(self.w2v_model.wv[word])
            
            # ì—°ê²° ë°©ì‹: ë‹¨ì–´ ë²¡í„°ë“¤ì„ ì´ì–´ ë¶™ì„
            if word_vectors:
                # 8ê°œ ë‹¨ì–´ Ã— vector_size ì°¨ì›ìœ¼ë¡œ ê³ ì •
                combined_vector = np.zeros(max_words * self.vector_size)
                for i, vec in enumerate(word_vectors):
                    start_idx = i * self.vector_size
                    end_idx = start_idx + self.vector_size
                    combined_vector[start_idx:end_idx] = vec
                vectors.append(combined_vector)
            else:
                # ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ 0 ë²¡í„°
                vectors.append(np.zeros(max_words * self.vector_size))
        
        vectors = np.array(vectors)
        print(f"âœ… ë²¡í„°í™” ì™„ë£Œ. í˜•íƒœ: {vectors.shape}")
        print(f"   - ê° ë²¡í„° í¬ê¸°: {max_words} ë‹¨ì–´ Ã— {self.vector_size} = {max_words * self.vector_size} ì°¨ì›")
        return vectors

class MosaicGenerator:
    """ë²¡í„°ë¥¼ ëª¨ìì´í¬ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    def __init__(self, image_size=32):
        self.image_size = image_size

    def vectors_to_mosaics(self, vectors):
        print(f"ğŸ–¼ï¸  ëª¨ìì´í¬ ì´ë¯¸ì§€ ìƒì„± ì¤‘ ({self.image_size}x{self.image_size})...")
        
        mosaics = []
        for vector in vectors:
            mosaic = self.vector_to_mosaic(vector)
            mosaics.append(mosaic)
        
        mosaics = np.array(mosaics)
        print(f"âœ… ëª¨ìì´í¬ ìƒì„± ì™„ë£Œ. í˜•íƒœ: {mosaics.shape}")
        return mosaics

    def vector_to_mosaic(self, vector):
        # ğŸ¨ 1024ì°¨ì› ë²¡í„°ë¥¼ 32x32 í”½ì…€ì— ì§ì ‘ ë§¤í•‘
        target_len = self.image_size * self.image_size  # 32*32 = 1024
        
        if len(vector) < target_len:
            padded_vector = np.pad(vector, (0, target_len - len(vector)), 'constant')
        else:
            padded_vector = vector[:target_len]
            
        # ì •ê·œí™”
        if padded_vector.std() > 0:
            padded_vector = (padded_vector - padded_vector.mean()) / padded_vector.std()
        
        # 32x32 ì´ë¯¸ì§€ë¡œ reshape
        mosaic = padded_vector.reshape((self.image_size, self.image_size, 1))
        return mosaic

class AutoEncoder:
    """ì˜¤í† ì¸ì½”ë”: ì´ë¯¸ì§€ â†’ ì••ì¶• â†’ ë³µì›"""
    def __init__(self, image_size=32, latent_dim=128):
        self.image_size = image_size
        self.latent_dim = latent_dim
        self.encoder, self.decoder, self.autoencoder = self._build_models()

    def _build_models(self):
        print("ğŸ—ï¸  ì˜¤í† ì¸ì½”ë” êµ¬ì¶• ì¤‘...")
        
        # ì¸ì½”ë”
        input_img = Input(shape=(self.image_size, self.image_size, 1))
        
        # ì¸ì½”ë” ë¶€ë¶„
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)
        x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)
        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)
        x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
        x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)
        
        # ì ì¬ ê³µê°„
        x = Flatten()(x)
        latent = Dense(self.latent_dim, activation='relu', name='latent')(x)
        
        encoder = Model(input_img, latent, name='encoder')
        
        # ë””ì½”ë”
        latent_input = Input(shape=(self.latent_dim,))
        
        # ì ì ˆí•œ í˜•íƒœë¡œ reshape
        size_after_conv = self.image_size // 8  # 3ë²ˆì˜ MaxPooling2D
        x = Dense(size_after_conv * size_after_conv * 256, activation='relu')(latent_input)
        x = Reshape((size_after_conv, size_after_conv, 256))(x)
        
        # ë””ì½”ë” ë¶€ë¶„
        x = Conv2DTranspose(256, (3, 3), activation='relu', padding='same')(x)
        x = tf.keras.layers.UpSampling2D((2, 2))(x)
        x = Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(x)
        x = tf.keras.layers.UpSampling2D((2, 2))(x)
        x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)
        x = tf.keras.layers.UpSampling2D((2, 2))(x)
        
        decoded = Conv2D(1, (3, 3), activation='linear', padding='same', name='decoded')(x)
        
        decoder = Model(latent_input, decoded, name='decoder')
        
        # ì „ì²´ ì˜¤í† ì¸ì½”ë”
        autoencoder = Model(input_img, decoder(encoder(input_img)), name='autoencoder')
        
        autoencoder.compile(optimizer=Adam(learning_rate=0.001),
                          loss='mse',
                          metrics=['mae'])
        
        print("âœ… ì˜¤í† ì¸ì½”ë” êµ¬ì¶• ì™„ë£Œ")
        encoder.summary()
        decoder.summary()
        
        return encoder, decoder, autoencoder

    def train(self, X_train, X_val, epochs=50, batch_size=32):
        print("ğŸš€ ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì‹œì‘...")
        print(f"ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {tf.config.list_logical_devices()}")
        
        # ë””ë°”ì´ìŠ¤ ì „ëµ í™•ì¸
        strategy = tf.distribute.get_strategy()
        print(f"ğŸ”§ ë¶„ì‚° ì „ëµ: {strategy}")
        
        # ëª©í‘œ: ì…ë ¥ ì´ë¯¸ì§€ = ì¶œë ¥ ì´ë¯¸ì§€
        history = self.autoencoder.fit(X_train, X_train,
                                     epochs=epochs,
                                     batch_size=batch_size,
                                     validation_data=(X_val, X_val),
                                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],
                                     verbose=1)
        
        print("âœ… í•™ìŠµ ì™„ë£Œ")
        return history

def analyze_reconstruction_quality(original, reconstructed):
    """ë³µì› í’ˆì§ˆ ë¶„ì„"""
    print("\nğŸ” ë³µì› í’ˆì§ˆ ë¶„ì„:")
    
    # ì „ì²´ í†µê³„
    mse = mean_squared_error(original.flatten(), reconstructed.flatten())
    mae = mean_absolute_error(original.flatten(), reconstructed.flatten())
    
    # ìƒê´€ê´€ê³„
    correlation = np.corrcoef(original.flatten(), reconstructed.flatten())[0, 1]
    
    print(f"  ğŸ“Š MSE (í‰ê·  ì œê³± ì˜¤ì°¨): {mse:.6f}")
    print(f"  ğŸ“Š MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨): {mae:.6f}")
    print(f"  ğŸ“Š ìƒê´€ê´€ê³„: {correlation:.4f}")
    
    # ë³µì›ìœ¨ ê³„ì‚° (ì„ê³„ê°’ ê¸°ë°˜)
    threshold = 0.1
    good_reconstruction = np.abs(original - reconstructed) < threshold
    reconstruction_rate = np.mean(good_reconstruction) * 100
    print(f"  ğŸ“Š ë³µì›ìœ¨ (Â±{threshold} ì´ë‚´): {reconstruction_rate:.2f}%")
    
    return {
        'mse': mse,
        'mae': mae,
        'correlation': correlation,
        'reconstruction_rate': reconstruction_rate
    }

def visualize_results(original, reconstructed, vectors=None, num_samples=5):
    """ê²°ê³¼ ì‹œê°í™” - ë²¡í„°ê°’ì„ ëª…ì•”ìœ¼ë¡œ í‘œí˜„"""
    print(f"\nğŸ¨ ê²°ê³¼ ì‹œê°í™” (ìƒ˜í”Œ {num_samples}ê°œ)...")
    
    # 3í–‰ìœ¼ë¡œ í™•ì¥: ì›ë³¸ ë²¡í„°ê°’, ì›ë³¸ ì´ë¯¸ì§€, ë³µì› ì´ë¯¸ì§€
    if vectors is not None:
        fig, axes = plt.subplots(3, num_samples, figsize=(20, 12))
        
        for i in range(num_samples):
            # ì›ë³¸ ë²¡í„°ê°’ì„ 32x32 ì´ë¯¸ì§€ë¡œ ì‹œê°í™”
            vector_img = vectors[i][:1024].reshape(32, 32)  # 1024ì°¨ì›ì„ 32x32ë¡œ
            im1 = axes[0, i].imshow(vector_img, cmap='gray', vmin=vector_img.min(), vmax=vector_img.max())
            axes[0, i].set_title(f'ì›ë³¸ ë²¡í„°ê°’ {i+1}\n(min:{vector_img.min():.3f}, max:{vector_img.max():.3f})')
            axes[0, i].axis('off')
            plt.colorbar(im1, ax=axes[0, i], fraction=0.046, pad=0.04)
            
            # ì •ê·œí™”ëœ ì›ë³¸ ì´ë¯¸ì§€
            im2 = axes[1, i].imshow(original[i].squeeze(), cmap='gray')
            axes[1, i].set_title(f'ì •ê·œí™”ëœ ì›ë³¸ {i+1}')
            axes[1, i].axis('off')
            plt.colorbar(im2, ax=axes[1, i], fraction=0.046, pad=0.04)
            
            # ë³µì› ì´ë¯¸ì§€
            im3 = axes[2, i].imshow(reconstructed[i].squeeze(), cmap='gray')
            axes[2, i].set_title(f'ë³µì› {i+1}')
            axes[2, i].axis('off')
            plt.colorbar(im3, ax=axes[2, i], fraction=0.046, pad=0.04)
    else:
        # ê¸°ì¡´ 2í–‰ ë°©ì‹
        fig, axes = plt.subplots(2, num_samples, figsize=(20, 8))
        
        for i in range(num_samples):
            # ì›ë³¸
            im1 = axes[0, i].imshow(original[i].squeeze(), cmap='gray')
            axes[0, i].set_title(f'ì›ë³¸ {i+1}')
            axes[0, i].axis('off')
            plt.colorbar(im1, ax=axes[0, i], fraction=0.046, pad=0.04)
            
            # ë³µì›
            im2 = axes[1, i].imshow(reconstructed[i].squeeze(), cmap='gray')
            axes[1, i].set_title(f'ë³µì› {i+1}')
            axes[1, i].axis('off')
            plt.colorbar(im2, ax=axes[1, i], fraction=0.046, pad=0.04)
    
    plt.tight_layout()
    plt.show()

def visualize_vector_patterns(vectors, mosaics=None, num_samples=10):
    """Vector pattern visualization - English labels to avoid font issues"""
    print(f"\nğŸ¨ Vector Pattern Visualization ({num_samples} samples)...")
    
    # Set font for better display
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'sans-serif']
    
    # 2x5 layout for better comparison
    rows = 2
    cols = num_samples//2
    fig, axes = plt.subplots(rows, cols, figsize=(25, 10))
    
    # Ensure axes is 2D
    if rows == 1:
        axes = axes.reshape(1, -1)
    elif cols == 1:
        axes = axes.reshape(-1, 1)
    
    # Temporary mosaic generator
    mosaic_gen = MosaicGenerator(image_size=32)
    
    for i in range(cols):
        # Row 1: Original vector values (before normalization)
        vector_raw = vectors[i][:1024].reshape(32, 32)
        im1 = axes[0, i].imshow(vector_raw, cmap='viridis', vmin=vector_raw.min(), vmax=vector_raw.max())
        axes[0, i].set_title(f'Original Vector {i+1}\n(Before Normalization)\nmin:{vector_raw.min():.3f}, max:{vector_raw.max():.3f}')
        axes[0, i].axis('off')
        plt.colorbar(im1, ax=axes[0, i], fraction=0.046, pad=0.04)
        
        # Row 2: Normalized mosaic (actual process)
        normalized_mosaic = mosaic_gen.vector_to_mosaic(vectors[i]).squeeze()
        im2 = axes[1, i].imshow(normalized_mosaic, cmap='gray', vmin=-3, vmax=3)
        axes[1, i].set_title(f'Normalized Mosaic {i+1}\n(After Processing)\nstd:{normalized_mosaic.std():.3f}')
        axes[1, i].axis('off')
        plt.colorbar(im2, ax=axes[1, i], fraction=0.046, pad=0.04)
    
    plt.suptitle('Word2Vec Vector â†’ Mosaic Transformation Process', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # Detailed similarity analysis
    print("\nğŸ” Detailed Vector Similarity Analysis:")
    
    # Check raw vector similarities
    print("Raw Vector Similarities (first 5 samples):")
    for i in range(5):
        for j in range(i+1, 5):
            raw_corr = np.corrcoef(vectors[i][:1024], vectors[j][:1024])[0,1]
            print(f"  Vector {i+1} vs Vector {j+1}: {raw_corr:.6f}")
    
    # Check specific sections of vectors
    print("\nSection-wise Analysis (8 words Ã— 128 dims):")
    for section in range(8):
        start_idx = section * 128
        end_idx = start_idx + 128
        section_similarities = []
        
        for i in range(5):
            for j in range(i+1, 5):
                sect_corr = np.corrcoef(vectors[i][start_idx:end_idx], vectors[j][start_idx:end_idx])[0,1]
                section_similarities.append(sect_corr)
        
        avg_sect_sim = np.mean(section_similarities)
        print(f"  Word {section+1} section (dims {start_idx}-{end_idx}): {avg_sect_sim:.6f}")
    
    # Check zero padding effect
    print("\nZero Padding Analysis:")
    for i in range(5):
        non_zero_count = np.count_nonzero(vectors[i])
        zero_ratio = (1024 - non_zero_count) / 1024
        print(f"  Vector {i+1}: {non_zero_count}/1024 non-zero ({zero_ratio:.2%} zeros)")
    
    return
    
    plt.suptitle('Word2Vec ë²¡í„° â†’ ëª¨ìì´í¬ ë³€í™˜ ê³¼ì • ì‹œê°í™”', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # ë²¡í„°ê°’ íˆìŠ¤í† ê·¸ë¨ë„ ì¶”ê°€
    print("\nğŸ“Š ë²¡í„°ê°’ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨...")
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    axes = axes.flatten()
    
    for i in range(10):
        # 1024ì°¨ì› ë²¡í„° ì „ì²´ì˜ íˆìŠ¤í† ê·¸ë¨
        axes[i].hist(vectors[i][:1024], bins=50, alpha=0.7, color='blue')
        axes[i].set_title(f'ë²¡í„° {i+1} ë¶„í¬ (1024ì°¨ì›)')
        axes[i].set_xlabel('ê°’')
        axes[i].set_ylabel('ë¹ˆë„')
        axes[i].grid(True, alpha=0.3)
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        mean_val = vectors[i][:1024].mean()
        std_val = vectors[i][:1024].std()
        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'í‰ê· : {mean_val:.3f}')
        axes[i].legend()
    
    plt.suptitle('1024ì°¨ì› ë²¡í„°ê°’ íˆìŠ¤í† ê·¸ë¨ ë¶„í¬', fontsize=16)
    plt.tight_layout()
    plt.show()

def main():
    print("\n--- 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© ---")
    
    # 2000ê°œ ë°ì´í„° ì‚¬ìš©
    df = pd.read_csv('ncvotera.csv', nrows=2000, encoding='latin1')
    print(f"âœ… {len(df)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
    
    # ëª¨ë“  ì»¬ëŸ¼ ê²°í•© (age, zip_code í¬í•¨í•œ ì „ì²´ 12ê°œ ì»¬ëŸ¼)
    df['full_text'] = df.astype(str).agg(' '.join, axis=1)
    
    # ì´ˆê¸° 10ê°œ ë°ì´í„° ì¶œë ¥
    print("\nğŸ“‹ Initial Data Samples (10 records):")
    for i in range(10):
        print(f"  Data {i+1}: {df['full_text'].iloc[i][:100]}...")  # First 100 characters only
    
    # ì‹¤ì œ ì´ë¦„ê³¼ ë°ì´í„° ë‹¤ì–‘ì„± í™•ì¸
    print("\nğŸ” Data Diversity Check:")
    print("First names extracted:")
    for i in range(10):
        first_word = df['full_text'].iloc[i].split()[0] if df['full_text'].iloc[i].split() else "None"
        print(f"  Record {i+1}: '{first_word}'")
    
    # ë‹¨ì–´ ìˆ˜ì™€ ê¸¸ì´ ë¶„í¬ í™•ì¸
    text_lengths = [len(text.split()) for text in df['full_text']]
    print(f"\nText length statistics:")
    print(f"  Min length: {min(text_lengths)} words")
    print(f"  Max length: {max(text_lengths)} words")
    print(f"  Average length: {np.mean(text_lengths):.1f} words")
    print(f"  Median length: {np.median(text_lengths):.1f} words")
    
    print("\n--- 2ë‹¨ê³„: Word2Vec ë²¡í„°í™” ---")
    processor = Word2VecProcessor(vector_size=128)  # 8ë‹¨ì–´ Ã— 128 = 1024ì°¨ì›
    vectors = processor.vectorize_data(df, 'full_text')
    
    print("\n--- 3ë‹¨ê³„: ëª¨ìì´í¬ ì´ë¯¸ì§€ ìƒì„± ---")
    mosaic_gen = MosaicGenerator(image_size=32)  # 32x32 = 1024 (ë²¡í„° í¬ê¸°ì™€ ì¼ì¹˜)
    mosaics = mosaic_gen.vectors_to_mosaics(vectors)
    
    # ğŸ” ëª¨ìì´í¬ ì´ë¯¸ì§€ ë‹¤ì–‘ì„± ê²€ì¦
    print("\nğŸ” ëª¨ìì´í¬ ì´ë¯¸ì§€ ë‹¤ì–‘ì„± ê²€ì¦:")
    print(f"  - ìƒì„±ëœ ì´ë¯¸ì§€ í˜•íƒœ: {mosaics.shape}")
    
    # ì²˜ìŒ 10ê°œ ì´ë¯¸ì§€ì˜ ë‹¤ì–‘ì„± í™•ì¸
    print("\nğŸ“Š ì²˜ìŒ 10ê°œ ëª¨ìì´í¬ ì´ë¯¸ì§€ ë¶„ì„:")
    for i in range(10):
        img = mosaics[i].squeeze()
        print(f"    ì´ë¯¸ì§€ {i+1}:")
        print(f"      - í‰ê· : {img.mean():.6f}")
        print(f"      - í‘œì¤€í¸ì°¨: {img.std():.6f}")
        print(f"      - ìµœì†Œê°’: {img.min():.6f}")
        print(f"      - ìµœëŒ€ê°’: {img.max():.6f}")
        print(f"      - ê³ ìœ ê°’ ê°œìˆ˜: {len(np.unique(np.round(img, 3)))}")
        print(f"      - ì²« 20ê°œ í”½ì…€: {img.flatten()[:20]}")
    
    # ì´ë¯¸ì§€ ê°„ ìƒê´€ê´€ê³„ í™•ì¸
    print("\nğŸ” ì´ë¯¸ì§€ ê°„ ìœ ì‚¬ë„ ë¶„ì„:")
    similarities = []
    for i in range(10):
        for j in range(i+1, 10):
            corr = np.corrcoef(mosaics[i].flatten(), mosaics[j].flatten())[0,1]
            similarities.append(corr)
            if i < 3 and j < 4:  # ì²˜ìŒ ëª‡ ê°œë§Œ ì¶œë ¥
                print(f"    ì´ë¯¸ì§€ {i+1} vs ì´ë¯¸ì§€ {j+1}: ìƒê´€ê´€ê³„ = {corr:.6f}")
    
    avg_similarity = np.mean(similarities)
    print(f"    Average image similarity: {avg_similarity:.6f}")
    
    if avg_similarity > 0.7:  # 0.95ì—ì„œ 0.7ë¡œ ë³€ê²½ - ë” ì—„ê²©í•œ ê¸°ì¤€
        print("    âš ï¸  WARNING: Images are too similar! Possible information loss")
        
        # ì›ë³¸ ë²¡í„°ì˜ ë‹¤ì–‘ì„± í™•ì¸
        print("\nğŸ” Original Word2Vec Vector Diversity Re-verification:")
        vector_similarities = []
        for i in range(10):
            for j in range(i+1, 10):
                corr = np.corrcoef(vectors[i], vectors[j])[0,1]
                vector_similarities.append(corr)
                if i < 3 and j < 4:
                    print(f"    Vector {i+1} vs Vector {j+1}: correlation = {corr:.6f}")
        
        avg_vector_similarity = np.mean(vector_similarities)
        print(f"    Average vector similarity: {avg_vector_similarity:.6f}")
        
        if avg_vector_similarity > 0.7:
            print("    âŒ PROBLEM: Original vectors are also too similar - Word2Vec training issue")
        else:
            print("    âŒ PROBLEM: Information loss in vectorâ†’image conversion process")
    else:
        print("    âœ… Images show appropriate diversity")
    
    # ë²¡í„° íŒ¨í„´ ëª…ì•” ì‹œê°í™” ì¶”ê°€ - ì‹¤ì œ ëª¨ìì´í¬ì™€ í•¨ê»˜
    print("\n--- ë²¡í„° íŒ¨í„´ ì‹œê°í™” ---")
    visualize_vector_patterns(vectors, mosaics, num_samples=10)
    
    print("\n--- 4ë‹¨ê³„: ë°ì´í„° ë¶„í•  ---")
    X_train, X_test = train_test_split(mosaics, test_size=0.2, random_state=42)
    print(f"  - í•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
    
    print("\n--- 5ë‹¨ê³„: ì˜¤í† ì¸ì½”ë” í•™ìŠµ ---")
    autoencoder = AutoEncoder(image_size=32, latent_dim=256)
    history = autoencoder.train(X_train, X_test, epochs=30, batch_size=32)
    
    print("\n=== í•™ìŠµ ì™„ë£Œ! ===")
    print("âœ… ì˜¤í† ì¸ì½”ë” í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ï¿½ ìµœì¢… í•™ìŠµ ì†ì‹¤: {history.history['loss'][-1]:.6f}")
    print(f"ğŸ“Š ìµœì¢… ê²€ì¦ ì†ì‹¤: {history.history['val_loss'][-1]:.6f}")
    print(f"ğŸ“Š ìµœì¢… í•™ìŠµ MAE: {history.history['mae'][-1]:.6f}")
    print(f"ğŸ“Š ìµœì¢… ê²€ì¦ MAE: {history.history['val_mae'][-1]:.6f}")
    
    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    print("\nğŸ¨ í•™ìŠµ ê³¡ì„  ì‹œê°í™”...")
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('ëª¨ë¸ ì†ì‹¤ (Loss)')
    plt.xlabel('ì—í¬í¬')
    plt.ylabel('ì†ì‹¤')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Training MAE')
    plt.plot(history.history['val_mae'], label='Validation MAE')
    plt.title('í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE)')
    plt.xlabel('ì—í¬í¬')
    plt.ylabel('MAE')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    print("\nğŸ” ê°„ë‹¨í•œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
    sample_predictions = autoencoder.autoencoder.predict(X_test[:5], verbose=0)
    print(f"  - ì…ë ¥ ì´ë¯¸ì§€ í˜•íƒœ: {X_test[:5].shape}")
    print(f"  - ì˜ˆì¸¡ ì´ë¯¸ì§€ í˜•íƒœ: {sample_predictions.shape}")
    print("  âœ… ì˜ˆì¸¡ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
    
    print("\n=== ìµœì¢… ê²°ë¡  ===")
    print("ğŸ‰ í•™ìŠµ ë‹¨ê³„ ì™„ë£Œ!")
    print("âœ… Word2Vec â†’ ì´ë¯¸ì§€ â†’ ì˜¤í† ì¸ì½”ë” í•™ìŠµ íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™")
    print("ğŸ“ˆ ëª¨ë¸ì´ ì´ë¯¸ì§€ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
