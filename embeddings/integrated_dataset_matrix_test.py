#!/usr/bin/env python3
"""
ì „ì²´ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ í° í–‰ë ¬ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” Word2Vec ëª¨ìì´í¬ í…ŒìŠ¤íŠ¸
- ê°œë³„ í…ìŠ¤íŠ¸ë§ˆë‹¤ ëª¨ìì´í¬ê°€ ì•„ë‹Œ, ì „ì²´ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ í° ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬
- 1000ëª… í™˜ì â†’ 1000Ã—256 í–‰ë ¬ â†’ í•˜ë‚˜ì˜ í° ëª¨ìì´í¬ ì´ë¯¸ì§€
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from simple_word2vec_mosaic import *
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

def load_patients_data():
    """patients.csv ë°ì´í„° ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    print("ğŸ“Š Patients ë°ì´í„° ë¡œë”©...")
    
    # CSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv('patients.csv')
    
    # ë¹ˆ í–‰ ì œê±°
    df = df.dropna(subset=['FIRST', 'LAST'])
    
    print(f"âœ… ì´ {len(df)}ê°œ í™˜ì ë°ì´í„° ë¡œë“œ")
    print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
    
    # í…ìŠ¤íŠ¸ ë°ì´í„° ì¡°í•© (ì´ë¦„, ì£¼ì†Œ, ì¸ì¢…, ì„±ë³„ ë“±)
    texts = []
    for _, row in df.iterrows():
        # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ í•„ë“œë“¤ì„ ì¡°í•©
        text_parts = []
        
        # ì´ë¦„
        if pd.notna(row['PREFIX']): text_parts.append(str(row['PREFIX']))
        if pd.notna(row['FIRST']): text_parts.append(str(row['FIRST']))
        if pd.notna(row['LAST']): text_parts.append(str(row['LAST']))
        if pd.notna(row['SUFFIX']): text_parts.append(str(row['SUFFIX']))
        
        # ìœ„ì¹˜ ì •ë³´
        if pd.notna(row['CITY']): text_parts.append(str(row['CITY']))
        if pd.notna(row['STATE']): text_parts.append(str(row['STATE']))
        if pd.notna(row['COUNTY']): text_parts.append(str(row['COUNTY']))
        
        # ì¸êµ¬í†µê³„í•™ì  ì •ë³´
        if pd.notna(row['RACE']): text_parts.append(str(row['RACE']))
        if pd.notna(row['ETHNICITY']): text_parts.append(str(row['ETHNICITY']))
        if pd.notna(row['GENDER']): text_parts.append(str(row['GENDER']))
        if pd.notna(row['MARITAL']): text_parts.append(str(row['MARITAL']))
        
        # ì¶œìƒì§€
        if pd.notna(row['BIRTHPLACE']): 
            # ì¶œìƒì§€ ì •ë³´ ë¶„í•  ì¶”ê°€
            birthplace_parts = str(row['BIRTHPLACE']).split()
            text_parts.extend(birthplace_parts)
        
        # í…ìŠ¤íŠ¸ ì¡°í•©
        combined_text = ' '.join(text_parts).lower()
        texts.append(combined_text)
    
    # ì²˜ìŒ 5ê°œ ìƒ˜í”Œ í™•ì¸
    print("\nğŸ“ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
    for i, text in enumerate(texts[:5]):
        print(f"  {i+1}: {text}")
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„
    text_lengths = [len(text.split()) for text in texts]
    print(f"\nğŸ“Š í…ìŠ¤íŠ¸ í†µê³„:")
    print(f"  í‰ê·  ë‹¨ì–´ ìˆ˜: {np.mean(text_lengths):.1f}")
    print(f"  ìµœì†Œ/ìµœëŒ€ ë‹¨ì–´ ìˆ˜: {min(text_lengths)}/{max(text_lengths)}")
    print(f"  ì¤‘ì•™ê°’ ë‹¨ì–´ ìˆ˜: {np.median(text_lengths):.1f}")
    
    return texts

class DatasetMatrixGenerator:
    """ì „ì²´ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ í° í–‰ë ¬ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” ìƒì„±ê¸°"""
    
    def __init__(self, vector_size=256):
        self.vector_size = vector_size
        self.processor = SimpleWord2VecProcessor(vector_size=vector_size)
        print(f"ğŸ—‚ï¸  ë°ì´í„°ì…‹ í–‰ë ¬ ìƒì„±ê¸°: ì „ì²´ ë°ì´í„° â†’ NÃ—{vector_size} í–‰ë ¬ â†’ í•˜ë‚˜ì˜ í° ì´ë¯¸ì§€")
    
    def create_dataset_matrix_image(self, texts):
        """ì „ì²´ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ í° í–‰ë ¬ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        print(f"\nğŸ”„ ì „ì²´ ë°ì´í„°ì…‹ ì²˜ë¦¬ ê³¼ì •:")
        print(f"1ë‹¨ê³„: {len(texts)}ê°œ í…ìŠ¤íŠ¸ â†’ Word2Vec ë²¡í„°í™”")
        
        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ Word2Vecìœ¼ë¡œ ë²¡í„°í™”
        vectors = self.processor.train_and_vectorize(texts)
        print(f"   ê²°ê³¼: {vectors.shape} ë²¡í„° í–‰ë ¬")
        
        print(f"2ë‹¨ê³„: {vectors.shape} í–‰ë ¬ â†’ í•˜ë‚˜ì˜ í° ì´ë¯¸ì§€ë¡œ ë³€í™˜")
        
        # í–‰ë ¬ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        matrix_image = self._vectors_to_matrix_image(vectors)
        print(f"   ê²°ê³¼: {matrix_image.shape} ì´ë¯¸ì§€")
        
        return matrix_image, vectors
    
    def _vectors_to_matrix_image(self, vectors):
        """ë²¡í„° í–‰ë ¬ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        # ì •ê·œí™” (0~1 ë²”ìœ„)
        normalized_vectors = self._normalize_vectors(vectors)
        
        # ì±„ë„ ì°¨ì› ì¶”ê°€ (N, vector_size, 1)
        matrix_image = np.expand_dims(normalized_vectors, axis=-1)
        
        print(f"   ğŸ“Š ë²¡í„° í†µê³„:")
        print(f"      ì›ë³¸ ë²”ìœ„: [{vectors.min():.4f}, {vectors.max():.4f}]")
        print(f"      ì •ê·œí™” í›„: [{matrix_image.min():.4f}, {matrix_image.max():.4f}]")
        
        return matrix_image
    
    def _normalize_vectors(self, vectors):
        """ë²¡í„° ì •ê·œí™”"""
        # í‘œì¤€í™” í›„ 0~1 ë²”ìœ„ë¡œ ë³€í™˜
        scaler = StandardScaler()
        normalized = scaler.fit_transform(vectors)
        
        # Min-max ì •ê·œí™”ë¡œ 0~1 ë²”ìœ„
        min_val = normalized.min()
        max_val = normalized.max()
        if max_val > min_val:
            normalized = (normalized - min_val) / (max_val - min_val)
        
        return normalized

class DatasetMatrixCNN:
    """ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•˜ëŠ” CNN"""
    
    def __init__(self, matrix_shape, vector_size):
        self.matrix_shape = matrix_shape  # (N, vector_size, 1)
        self.vector_size = vector_size
        self.n_samples = matrix_shape[0]
        self.model = self._build_model()
    
    def _build_model(self):
        """ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬ í•™ìŠµ ëª¨ë¸ êµ¬ì¶•"""
        print(f"ğŸ—ï¸  ë°ì´í„°ì…‹ í–‰ë ¬ CNN êµ¬ì¶• ({self.matrix_shape})")
        
        # ì…ë ¥: ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬ ì´ë¯¸ì§€
        inputs = Input(shape=self.matrix_shape)
        
        # 1D ì»¨ë³¼ë£¨ì…˜ (ë²¡í„° ì°¨ì›ì— ëŒ€í•´)
        x = Conv2D(32, (1, 3), activation='relu', padding='same')(inputs)
        x = Conv2D(64, (1, 5), activation='relu', padding='same')(x)
        x = Conv2D(128, (1, 7), activation='relu', padding='same')(x)
        
        # ê³µê°„ì  íŠ¹ì§• ì¶”ì¶œ (ìƒ˜í”Œ ì°¨ì›ì— ëŒ€í•´)
        x = Conv2D(64, (3, 1), activation='relu', padding='same')(x)
        x = Conv2D(32, (5, 1), activation='relu', padding='same')(x)
        
        # ê¸€ë¡œë²Œ íŠ¹ì§• ì¶”ì¶œ
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        
        # ì™„ì „ì—°ê²°ì¸µ
        x = Dense(512, activation='relu')(x)
        x = Dense(256, activation='relu')(x)
        x = Dense(128, activation='relu')(x)
        
        # ì¶œë ¥: ë°ì´í„°ì…‹ ì „ì²´ì˜ í‘œí˜„ ë²¡í„°
        outputs = Dense(64, activation='linear', name='dataset_embedding')(x)
        
        model = Model(inputs, outputs)
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        
        print("âœ… ë°ì´í„°ì…‹ í–‰ë ¬ CNN êµ¬ì¶• ì™„ë£Œ")
        model.summary()
        
        return model
    
    def create_target_vectors(self, original_vectors):
        """í•™ìŠµ ëª©í‘œê°€ ë  ë²¡í„° ìƒì„± (ì˜ˆ: ë°ì´í„°ì…‹ í†µê³„ì  íŠ¹ì„±)"""
        print("ğŸ¯ í•™ìŠµ ëª©í‘œ ë²¡í„° ìƒì„±...")
        
        # ì—¬ëŸ¬ í†µê³„ì  íŠ¹ì„±ì„ ê²°í•©í•œ ëª©í‘œ ë²¡í„°
        mean_vec = np.mean(original_vectors, axis=0)[:16]  # í‰ê· 
        std_vec = np.std(original_vectors, axis=0)[:16]    # í‘œì¤€í¸ì°¨
        min_vec = np.min(original_vectors, axis=0)[:16]    # ìµœì†Ÿê°’
        max_vec = np.max(original_vectors, axis=0)[:16]    # ìµœëŒ“ê°’
        
        target_vector = np.concatenate([mean_vec, std_vec, min_vec, max_vec])
        
        print(f"   ëª©í‘œ ë²¡í„° í¬ê¸°: {target_vector.shape}")
        print(f"   ëª©í‘œ ë²¡í„° ë²”ìœ„: [{target_vector.min():.4f}, {target_vector.max():.4f}]")
        
        return target_vector
    
    def train_dataset_learning(self, matrix_image, target_vector, epochs=50):
        """ë°ì´í„°ì…‹ ì „ì²´ë¥¼ í•™ìŠµ"""
        print(f"ğŸš€ ë°ì´í„°ì…‹ ì „ì²´ í•™ìŠµ ì‹œì‘...")
        print(f"   ì…ë ¥: {matrix_image.shape} (ì „ì²´ ë°ì´í„°ì…‹ ì´ë¯¸ì§€)")
        print(f"   ëª©í‘œ: {target_vector.shape} (ë°ì´í„°ì…‹ íŠ¹ì„± ë²¡í„°)")
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        X = np.expand_dims(matrix_image, axis=0)  # (1, N, vector_size, 1)
        y = np.expand_dims(target_vector, axis=0)  # (1, 64)
        
        history = self.model.fit(X, y,
                                epochs=epochs,
                                batch_size=1,
                                verbose=1)
        
        print("âœ… ë°ì´í„°ì…‹ í•™ìŠµ ì™„ë£Œ")
        return history
    
    def predict_dataset_features(self, matrix_image):
        """ë°ì´í„°ì…‹ ì´ë¯¸ì§€ì—ì„œ íŠ¹ì„± ì˜ˆì¸¡"""
        X = np.expand_dims(matrix_image, axis=0)
        prediction = self.model.predict(X, verbose=0)
        return prediction[0]

def visualize_dataset_matrix(matrix_image, vectors, texts, title="Dataset Matrix Visualization"):
    """ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬ ì‹œê°í™”"""
    print(f"ğŸ¨ ë°ì´í„°ì…‹ í–‰ë ¬ ì‹œê°í™”: {title}")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle(title, fontsize=16, fontweight='bold')
    
    # 1. ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬ ì´ë¯¸ì§€
    im1 = axes[0, 0].imshow(matrix_image.squeeze(), cmap='viridis', aspect='auto')
    axes[0, 0].set_title(f'ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬\n{matrix_image.shape[0]}Ã—{matrix_image.shape[1]} ì´ë¯¸ì§€')
    axes[0, 0].set_xlabel('ë²¡í„° ì°¨ì› (256ì°¨ì›)')
    axes[0, 0].set_ylabel('í™˜ì ë²ˆí˜¸ (Nëª…)')
    plt.colorbar(im1, ax=axes[0, 0])
    
    # 2. ë²¡í„° ì°¨ì›ë³„ ë¶„í¬
    axes[0, 1].hist(vectors.flatten(), bins=50, alpha=0.7, color='blue')
    axes[0, 1].set_title('ì „ì²´ ë²¡í„°ê°’ ë¶„í¬')
    axes[0, 1].set_xlabel('ë²¡í„° ê°’')
    axes[0, 1].set_ylabel('ë¹ˆë„')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. í™˜ìë³„ ë²¡í„° ë…¸ë¦„
    vector_norms = np.linalg.norm(vectors, axis=1)
    axes[0, 2].plot(vector_norms, 'b-', alpha=0.7)
    axes[0, 2].set_title('í™˜ìë³„ ë²¡í„° í¬ê¸°')
    axes[0, 2].set_xlabel('í™˜ì ë²ˆí˜¸')
    axes[0, 2].set_ylabel('ë²¡í„° ë…¸ë¦„')
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. ì°¨ì›ë³„ í‰ê· ê°’
    dim_means = np.mean(vectors, axis=0)
    axes[1, 0].plot(dim_means, 'r-', alpha=0.7)
    axes[1, 0].set_title('ì°¨ì›ë³„ í‰ê· ê°’')
    axes[1, 0].set_xlabel('ë²¡í„° ì°¨ì›')
    axes[1, 0].set_ylabel('í‰ê· ê°’')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. PCA ë¶„ì„
    if len(vectors) > 2:
        pca = PCA(n_components=2)
        pca_result = pca.fit_transform(vectors)
        scatter = axes[1, 1].scatter(pca_result[:, 0], pca_result[:, 1], 
                                   c=range(len(pca_result)), cmap='viridis', alpha=0.6)
        axes[1, 1].set_title(f'PCA ë¶„ì„\n(ì„¤ëª… ë¶„ì‚°: {pca.explained_variance_ratio_.sum():.3f})')
        axes[1, 1].set_xlabel('PC1')
        axes[1, 1].set_ylabel('PC2')
        plt.colorbar(scatter, ax=axes[1, 1])
    
    # 6. í…ìŠ¤íŠ¸ ìƒ˜í”Œ ì •ë³´
    axes[1, 2].axis('off')
    axes[1, 2].text(0.1, 0.9, f"ë°ì´í„°ì…‹ ì •ë³´:", fontsize=12, fontweight='bold', 
                   transform=axes[1, 2].transAxes)
    axes[1, 2].text(0.1, 0.8, f"â€¢ ì´ í™˜ì ìˆ˜: {len(texts)}", fontsize=10, 
                   transform=axes[1, 2].transAxes)
    axes[1, 2].text(0.1, 0.7, f"â€¢ ë²¡í„° ì°¨ì›: {vectors.shape[1]}", fontsize=10, 
                   transform=axes[1, 2].transAxes)
    axes[1, 2].text(0.1, 0.6, f"â€¢ í–‰ë ¬ í¬ê¸°: {matrix_image.shape[0]}Ã—{matrix_image.shape[1]}", fontsize=10, 
                   transform=axes[1, 2].transAxes)
    axes[1, 2].text(0.1, 0.5, f"â€¢ ë²¡í„° ë²”ìœ„: [{vectors.min():.3f}, {vectors.max():.3f}]", fontsize=10, 
                   transform=axes[1, 2].transAxes)
    
    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ í‘œì‹œ
    axes[1, 2].text(0.1, 0.4, "ìƒ˜í”Œ í…ìŠ¤íŠ¸:", fontsize=10, fontweight='bold', 
                   transform=axes[1, 2].transAxes)
    for i, text in enumerate(texts[:3]):
        axes[1, 2].text(0.1, 0.3-i*0.08, f"{i+1}: {text[:40]}...", fontsize=8, 
                       transform=axes[1, 2].transAxes)
    
    plt.tight_layout()
    plt.savefig('dataset_matrix_visualization.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… ì‹œê°í™” ì™„ë£Œ: dataset_matrix_visualization.png ì €ì¥")

def test_dataset_matrix_approach():
    """ì „ì²´ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ í–‰ë ¬ ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬í•˜ëŠ” ì ‘ê·¼ë²• í…ŒìŠ¤íŠ¸"""
    
    print("="*80)
    print("ğŸ§ª ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬ ì´ë¯¸ì§€ ì ‘ê·¼ë²• í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    texts = load_patients_data()
    
    print(f"\nğŸ¯ ì „ì²´ ë°ì´í„°ì…‹: {len(texts)}ê°œ í™˜ì")
    print("ğŸ“‹ ì ‘ê·¼ ë°©ì‹: ê°œë³„ ëª¨ìì´í¬ê°€ ì•„ë‹Œ ì „ì²´ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ í° ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬")
    
    # 2. ì „ì²´ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ í–‰ë ¬ ì´ë¯¸ì§€ë¡œ ë³€í™˜
    print(f"\n{'='*50}")
    print("ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹ â†’ í•˜ë‚˜ì˜ í–‰ë ¬ ì´ë¯¸ì§€ ë³€í™˜")
    print(f"{'='*50}")
    
    generator = DatasetMatrixGenerator(vector_size=256)
    matrix_image, vectors = generator.create_dataset_matrix_image(texts)
    
    print(f"\nâœ… ë³€í™˜ ì™„ë£Œ:")
    print(f"   ğŸ“Š ì›ë³¸: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
    print(f"   ğŸ“Š ë²¡í„° í–‰ë ¬: {vectors.shape}")
    print(f"   ğŸ“Š í–‰ë ¬ ì´ë¯¸ì§€: {matrix_image.shape}")
    print(f"   ğŸ“Š ì´ë¯¸ì§€ í¬ê¸°: {matrix_image.shape[0]}Ã—{matrix_image.shape[1]} í”½ì…€")
    
    # 3. ì‹œê°í™”
    visualize_dataset_matrix(matrix_image, vectors, texts, 
                           title=f"Patients ë°ì´í„°ì…‹ ì „ì²´ í–‰ë ¬ ({len(texts)}Ã—{vectors.shape[1]})")
    
    # 4. ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ í…ŒìŠ¤íŠ¸
    print(f"\n{'='*50}")
    print("ğŸ¤– ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ í…ŒìŠ¤íŠ¸")
    print(f"{'='*50}")
    
    # CNN ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    cnn = DatasetMatrixCNN(matrix_shape=matrix_image.shape, vector_size=256)
    
    # í•™ìŠµ ëª©í‘œ ë²¡í„° ìƒì„±
    target_vector = cnn.create_target_vectors(vectors)
    
    # í•™ìŠµ ì‹¤í–‰
    history = cnn.train_dataset_learning(matrix_image, target_vector, epochs=30)
    
    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    predicted_features = cnn.predict_dataset_features(matrix_image)
    
    # ê²°ê³¼ ë¶„ì„
    mae = np.mean(np.abs(target_vector - predicted_features))
    correlation = np.corrcoef(target_vector, predicted_features)[0, 1]
    
    print(f"\nğŸ“Š í•™ìŠµ ê²°ê³¼:")
    print(f"   MAE: {mae:.4f}")
    print(f"   ìƒê´€ê´€ê³„: {correlation:.4f}")
    print(f"   ëª©í‘œ ë²¡í„° ë²”ìœ„: [{target_vector.min():.4f}, {target_vector.max():.4f}]")
    print(f"   ì˜ˆì¸¡ ë²¡í„° ë²”ìœ„: [{predicted_features.min():.4f}, {predicted_features.max():.4f}]")
    
    # í•™ìŠµ ê°€ëŠ¥ì„± íŒì •
    if mae < 0.1 and correlation > 0.8:
        learning_status = "âœ… ìš°ìˆ˜í•œ í•™ìŠµ"
        explanation = "ì „ì²´ ë°ì´í„°ì…‹ì˜ íŒ¨í„´ì„ ì˜ í•™ìŠµí•¨"
    elif mae < 0.3 and correlation > 0.5:
        learning_status = "ğŸ”„ ì–‘í˜¸í•œ í•™ìŠµ"
        explanation = "ì „ì²´ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ íŒ¨í„´ì„ í•™ìŠµí•¨"
    else:
        learning_status = "âš ï¸  ê¸°ë³¸ í•™ìŠµ"
        explanation = "ë” ë§ì€ í•™ìŠµì´ë‚˜ ë‹¤ë¥¸ ì ‘ê·¼ í•„ìš”"
    
    print(f"\nğŸ” í•™ìŠµ ê°€ëŠ¥ì„±: {learning_status}")
    print(f"   ğŸ“ ì„¤ëª…: {explanation}")
    
    # 5. ê²°ë¡ 
    print(f"\n{'='*80}")
    print("ğŸ‰ ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬ ì´ë¯¸ì§€ ì ‘ê·¼ë²• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"âœ… ì„±ê³µì ìœ¼ë¡œ {len(texts)}ê°œ í™˜ì ë°ì´í„°ë¥¼ {matrix_image.shape[0]}Ã—{matrix_image.shape[1]} ì´ë¯¸ì§€ë¡œ ë³€í™˜")
    print(f"âœ… ì „ì²´ ë°ì´í„°ì…‹ì˜ íŒ¨í„´ì„ í•˜ë‚˜ì˜ CNNìœ¼ë¡œ í•™ìŠµ ì™„ë£Œ")
    print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: MAE={mae:.4f}, ìƒê´€ê´€ê³„={correlation:.4f}")
    
    return {
        'matrix_image': matrix_image,
        'vectors': vectors,
        'texts': texts,
        'mae': mae,
        'correlation': correlation,
        'learning_status': learning_status
    }

if __name__ == "__main__":
    # ì „ì²´ ë°ì´í„°ì…‹ í–‰ë ¬ ì ‘ê·¼ë²• í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = test_dataset_matrix_approach()
