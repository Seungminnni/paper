# pylint: disable=all
# type: ignore
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import (Input, Dense, Flatten, Reshape, 
                                     Conv2D, Conv2DTranspose, Embedding)
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# GPU ì„¤ì •
print("=== GPU ì„¤ì • í™•ì¸ ===")

# tensorflow-metal GPU ì„¤ì •
try:
    # GPU ì¥ì¹˜ í™•ì¸
    gpus = tf.config.list_physical_devices('GPU')
    print(f"ê°ì§€ëœ GPU: {gpus}")
    
    if gpus:
        try:
            # GPU ë©”ëª¨ë¦¬ ì ì§„ì  í• ë‹¹ ì„¤ì •
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… GPU ì„¤ì • ì™„ë£Œ: {len(gpus)}ê°œ GPU ì‚¬ìš© ê°€ëŠ¥")
            for i, gpu in enumerate(gpus):
                print(f"  GPU {i}: {gpu}")
            gpu_available = True
        except RuntimeError as e:
            print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            gpu_available = False
    else:
        print("âŒ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        gpu_available = False
        
except Exception as e:
    print(f"âŒ GPU ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    gpus = []
    gpu_available = False

# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤ í™•ì¸
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¼ë¦¬ì  ë””ë°”ì´ìŠ¤: {tf.config.list_physical_devices()}")

if gpu_available:
    print("ğŸš€ GPU ê°€ì†ì„ ì‚¬ìš©í•©ë‹ˆë‹¤!")
else:
    print("ğŸ”§ CPU ìµœì í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# MNIST ë°ì´í„°ë¥¼ ì½ê³  ì‹ ê²½ë§ì— ì…ë ¥í•  ì¤€ë¹„
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))

# í…ìŠ¤íŠ¸ ë ˆì´ë¸”ì„ ì›-í•« ì¸ì½”ë”©
y_train_onehot = to_categorical(y_train, 10)
y_test_onehot = to_categorical(y_test, 10)

zdim = 32  # ì ì¬ ê³µê°„ì˜ ì°¨ì›
text_dim = 64  # í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›

print("=== ê°„ë‹¨í•œ Cross-Modality Autoencoder ===")

# ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ê²°ì •
device_name = '/GPU:0' if gpu_available else '/CPU:0'
print(f"ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device_name}")

with tf.device(device_name):
    # ===================== ëª¨ë¸ 1: Text-to-Image =====================
    print("\n--- Text-to-Image ëª¨ë¸ ---")
    
    # Text Encoder (ê°„ë‹¨í•œ Dense ë ˆì´ì–´)
    text_input = Input(shape=(10,), name='text_input')  # ì›-í•« ì¸ì½”ë”©ëœ ë ˆì´ë¸”
    x = Dense(128, activation='relu', name='text_dense1')(text_input)
    x = Dense(256, activation='relu', name='text_dense2')(x)
    text_encoded = Dense(zdim, activation='relu', name='text_latent')(x)
    
    text_encoder = Model(text_input, text_encoded, name='text_encoder')
    
    # Image Decoder (CNN Transpose)
    latent_input = Input(shape=(zdim,), name='latent_input')
    x = Dense(7 * 7 * 32, activation='relu', name='decode_dense')(latent_input)
    x = Reshape((7, 7, 32), name='decode_reshape')(x)
    x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=(2, 2), name='deconv1')(x)
    x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same', strides=(2, 2), name='deconv2')(x)
    image_output = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same', name='image_output')(x)
    
    image_decoder = Model(latent_input, image_output, name='image_decoder')
    
    # Text-to-Image ëª¨ë¸ ê²°í•©
    text_to_image = Model(text_input, image_decoder(text_encoder(text_input)), name='text_to_image')
    text_to_image.compile(optimizer='adam', loss='mse', metrics=['mae'])
    
    print("Text Encoder êµ¬ì¡°:")
    text_encoder.summary()
    print("\nImage Decoder êµ¬ì¡°:")
    image_decoder.summary()
    
    # ===================== ëª¨ë¸ 2: Image-to-Text =====================
    print("\n--- Image-to-Text ëª¨ë¸ ---")
    
    # Image Encoder (CNN)
    image_input = Input(shape=(28, 28, 1), name='image_input')
    x = Conv2D(16, (3, 3), activation='relu', padding='same', strides=(2, 2), name='conv1')(image_input)
    x = Conv2D(32, (3, 3), activation='relu', padding='same', strides=(2, 2), name='conv2')(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv3')(x)
    x = Flatten(name='flatten')(x)
    x = Dense(128, activation='relu', name='img_dense1')(x)
    image_encoded = Dense(zdim, activation='relu', name='img_latent')(x)
    
    image_encoder = Model(image_input, image_encoded, name='image_encoder')
    
    # Text Decoder (Dense ë ˆì´ì–´)
    latent_to_text_input = Input(shape=(zdim,), name='latent_to_text_input')
    x = Dense(128, activation='relu', name='text_decode_dense1')(latent_to_text_input)  
    x = Dense(64, activation='relu', name='text_decode_dense2')(x)
    text_output = Dense(10, activation='softmax', name='text_classification')(x)
    
    text_decoder = Model(latent_to_text_input, text_output, name='text_decoder')
    
    # Image-to-Text ëª¨ë¸ ê²°í•©
    image_to_text = Model(image_input, text_decoder(image_encoder(image_input)), name='image_to_text')
    image_to_text.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    
    print("Image Encoder êµ¬ì¡°:")
    image_encoder.summary()
    print("\nText Decoder êµ¬ì¡°:")
    text_decoder.summary()

print("\n=== ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")

with tf.device(device_name):
    # 1. Text-to-Image ëª¨ë¸ í•™ìŠµ
    print("\nğŸš€ 1. Text â†’ Image ëª¨ë¸ í•™ìŠµ ì¤‘...")
    history1 = text_to_image.fit(
        y_train_onehot, x_train,
        epochs=5, batch_size=128,
        validation_data=(y_test_onehot, x_test),
        verbose=1
    )
    
    # 2. Image-to-Text ëª¨ë¸ í•™ìŠµ  
    print("\nğŸš€ 2. Image â†’ Text ëª¨ë¸ í•™ìŠµ ì¤‘...")
    history2 = image_to_text.fit(
        x_train, y_train_onehot,
        epochs=5, batch_size=128,
        validation_data=(x_test, y_test_onehot), 
        verbose=1
    )

print("\n=== ì‹¤í—˜ ë° ê²°ê³¼ ì‹œê°í™” ===")

# ì‹¤í—˜ 1: Text â†’ Image ìƒì„±
print("\nì‹¤í—˜ 1: í…ìŠ¤íŠ¸ ë ˆì´ë¸”ë¡œë¶€í„° ì´ë¯¸ì§€ ìƒì„±")
test_labels = np.eye(10)  # 0~9 ê° ìˆ«ìì˜ ì›-í•« ë²¡í„°
generated_images = text_to_image.predict(test_labels)

plt.figure(figsize=(15, 3))
for i in range(10):
    plt.subplot(1, 10, i+1)
    plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')
    plt.title(f'Label: {i}')
    plt.xticks([])
    plt.yticks([])
plt.suptitle('Text â†’ Image: ê° ìˆ«ì ë ˆì´ë¸”ë¡œë¶€í„° ìƒì„±ëœ ì´ë¯¸ì§€')
plt.tight_layout()
plt.show()

# ì‹¤í—˜ 2: Image â†’ Text ë¶„ë¥˜
print("\nì‹¤í—˜ 2: ì´ë¯¸ì§€ë¡œë¶€í„° í…ìŠ¤íŠ¸ ë ˆì´ë¸” ì˜ˆì¸¡")
predicted_labels = image_to_text.predict(x_test[:10])
predicted_classes = np.argmax(predicted_labels, axis=1)
actual_classes = y_test[:10]

plt.figure(figsize=(15, 6))
for i in range(10):
    plt.subplot(2, 10, i+1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    plt.title(f'True: {actual_classes[i]}')
    plt.xticks([])
    plt.yticks([])
    
    plt.subplot(2, 10, i+11)
    plt.bar(range(10), predicted_labels[i])
    plt.title(f'Pred: {predicted_classes[i]}')
    plt.xticks(range(10))
    plt.ylim(0, 1)
plt.suptitle('Image â†’ Text: ì´ë¯¸ì§€ë¡œë¶€í„° ë ˆì´ë¸” ì˜ˆì¸¡')
plt.tight_layout()
plt.show()

# ì‹¤í—˜ 3: Cross-Modality ë³€í™˜ ì‹¤í—˜
print("\nì‹¤í—˜ 3: Cross-Modality ë³€í™˜ (Text â†’ Image â†’ Text)")

# í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ìƒì„±
generated_images_full = text_to_image.predict(test_labels)

# ìƒì„±ëœ ì´ë¯¸ì§€ì—ì„œ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì˜ˆì¸¡
reconstructed_labels = image_to_text.predict(generated_images_full)
reconstructed_classes = np.argmax(reconstructed_labels, axis=1)

plt.figure(figsize=(15, 9))
for i in range(10):
    # ì›ë³¸ ë ˆì´ë¸”
    plt.subplot(3, 10, i+1)
    plt.text(0.5, 0.5, f'{i}', fontsize=20, ha='center', va='center')
    plt.title(f'Input: {i}')
    plt.xticks([])
    plt.yticks([])
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    
    # ìƒì„±ëœ ì´ë¯¸ì§€
    plt.subplot(3, 10, i+11)
    plt.imshow(generated_images_full[i].reshape(28, 28), cmap='gray')
    plt.title('Generated Image')
    plt.xticks([])
    plt.yticks([])
    
    # ì¬êµ¬ì„±ëœ ë ˆì´ë¸”
    plt.subplot(3, 10, i+21)
    plt.bar(range(10), reconstructed_labels[i])
    plt.title(f'Output: {reconstructed_classes[i]}')
    plt.xticks(range(10))
    plt.ylim(0, 1)

plt.suptitle('Cross-Modality: Text â†’ Image â†’ Text')
plt.tight_layout()
plt.show()

# ì„±ëŠ¥ í‰ê°€
print("\n=== ìµœì¢… ì„±ëŠ¥ í‰ê°€ ===")
text_to_image_loss = text_to_image.evaluate(y_test_onehot, x_test, verbose=0)
image_to_text_metrics = image_to_text.evaluate(x_test, y_test_onehot, verbose=0)

print(f"ğŸ“Š Text â†’ Image MSE Loss: {text_to_image_loss[0]:.4f}")
print(f"ğŸ“Š Text â†’ Image MAE: {text_to_image_loss[1]:.4f}")
print(f"ğŸ“Š Image â†’ Text Loss: {image_to_text_metrics[0]:.4f}")
print(f"ğŸ“Š Image â†’ Text Accuracy: {image_to_text_metrics[1]:.4f}")

# Cross-modality ì •í™•ë„ ê³„ì‚°
cross_modality_accuracy = np.mean(np.arange(10) == reconstructed_classes)
print(f"ğŸ“Š Cross-Modality Accuracy (Textâ†’Imageâ†’Text): {cross_modality_accuracy:.4f}")

print(f"\nğŸ‰ GPU ê°€ì† Cross-Modality Autoencoder ì™„ë£Œ!")
print(f"ğŸ”§ ì‚¬ìš©ëœ ë””ë°”ì´ìŠ¤: {device_name}")
