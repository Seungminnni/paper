# ìœ ê¶Œì ë°ì´í„° smashed data ìœ ì‚¬ë„ ê³„ì‚°
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances

def calculate_voter_similarity(client_file, dictionary_file, original_client_data, original_dictionary_data, n=5):
    """
    ìœ ê¶Œì ë°ì´í„°ì˜ smashed data ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜

    Parameters:
    - client_file: í´ë¼ì´ì–¸íŠ¸ ì¸¡ smashed data íŒŒì¼ ê²½ë¡œ
    - dictionary_file: ì„œë²„ ì¸¡ smashed data íŒŒì¼ ê²½ë¡œ
    - original_client_data: ì›ë³¸ í´ë¼ì´ì–¸íŠ¸ ë°ì´í„° (DataFrame)
    - original_dictionary_data: ì›ë³¸ ì„œë²„ ë°ì´í„° (DataFrame)
    - n: Top-N ìœ ì‚¬ë„ ê³„ì‚° ê°œìˆ˜
    """
    # ë³€í™˜ëœ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.
    client_data = pd.read_csv(client_file)
    dictionary_data = pd.read_csv(dictionary_file)

    # ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    distances = euclidean_distances(client_data.values, dictionary_data.values)

    # Top@n ìœ ì‚¬ë„ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    topn_similarities = np.argsort(distances, axis=1)[:, :n]
    topn_values = np.sort(distances, axis=1)[:, :n]

    # ëª¨ë“  ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê³  ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    successful_distances = []
    unsuccessful_distances = []
    successes = 0
    success_indices = []  # ì„±ê³µí•œ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    success_ranks_count = {rank: 0 for rank in range(1, n+1)}  # ê° ì„±ê³µí•œ ì„œë²„ ì¸¡ ë­í¬ì˜ ìˆ˜ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬

    for i, (indices, scores) in enumerate(zip(topn_similarities, topn_values)):
        for rank, (idx, score) in enumerate(zip(indices, scores), 1):
            # ì›ë³¸ ë°ì´í„°ì˜ voter_idë¡œ ë¹„êµ
            client_voter_id = original_client_data.iloc[i]["voter_id"]
            dictionary_voter_id = original_dictionary_data.iloc[idx]["voter_id"]

            if client_voter_id == dictionary_voter_id:
                successes += 1
                successful_distances.append(score)
                success_indices.append((i + 1, rank))  # ì„±ê³µí•œ ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€
                success_ranks_count[rank] += 1  # í•´ë‹¹ ë­í¬ì˜ ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚´
                break  # ê°™ì€ voter_idë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë” ì´ìƒ í™•ì¸í•˜ì§€ ì•ŠìŒ
            else:
                unsuccessful_distances.append(score)

    # ì •í™•ë„ ê³„ì‚°
    accuracy = successes / len(client_data)

    # ì„±ê³µì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ í‰ê·  ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    successful_mean_distance = np.mean(successful_distances) if successful_distances else 0
    unsuccessful_mean_distance = np.mean(unsuccessful_distances) if unsuccessful_distances else 0

    # í‰ê·  ê±°ë¦¬ì˜ ë¶„ì‚° ê³„ì‚°
    successful_distance_variance = np.var(successful_distances) if successful_distances else 0
    unsuccessful_distance_variance = np.var(unsuccessful_distances) if unsuccessful_distances else 0

    return accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    print("Loading voter data for similarity calculation...")

    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
    full_data = pd.read_csv("ncvoterb.csv", encoding='latin-1')

    # ë°ì´í„° í¬ê¸° ì œí•œ: ì‹¤í—˜ì„ ìœ„í•´ 20,000ê°œë¡œ ì œí•œ
    # ì „ì²´ ë°ì´í„°: ì•½ 224,061ê°œ â†’ ì‹¤í—˜ìš©: 20,000ê°œ (ì•½ 8.9% ì‚¬ìš©)
    SAMPLE_SIZE = 20000
    if len(full_data) > SAMPLE_SIZE:
        print(f"ğŸ“Š Reducing data size from {len(full_data):,} to {SAMPLE_SIZE:,} for faster experimentation")
        full_data = full_data.sample(n=SAMPLE_SIZE, random_state=42)
        print(f"âœ… Data reduced successfully! Working with {len(full_data):,} records")

    print(f"âœ… Data loaded successfully! Total records: {len(full_data)}")

    # ì„œë²„ì¸¡ê³¼ í´ë¼ì´ì–¸íŠ¸ ì¸¡ ë°ì´í„° ë¶„ë¦¬ (ì´ì „ê³¼ ë™ì¼í•œ ë°©ì‹)
    # ì‹¤í—˜ìš© ë°ì´í„°ì—ì„œ 70% ì„œë²„, 30% í´ë¼ì´ì–¸íŠ¸ë¡œ ë¶„ë¦¬
    server_sample_size = int(len(full_data) * 0.7)
    server_data = full_data.sample(n=server_sample_size, random_state=42)
    client_data = full_data.drop(server_data.index).sample(frac=1.0, random_state=123)  # ë‚˜ë¨¸ì§€ ë°ì´í„° ì‚¬ìš©

    print(f"ğŸ“Š Server data size: {len(server_data)} (70% of {len(full_data)} = {len(server_data):,})")
    print(f"ğŸ“Š Client data size: {len(client_data)} (30% of {len(full_data)} = {len(client_data):,})")

    # ë³€í™˜ëœ íŒŒì¼ ê²½ë¡œ
    dictionary_file = "Dictionary_smashed_data.csv"
    client_file = "Client_smashed_data.csv"

    # Top n ì„¤ì •
    n = 5

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    import os
    if not os.path.exists(client_file) or not os.path.exists(dictionary_file):
        print(f"Error: Required files not found!")
        print(f"Client file: {client_file} - {'Found' if os.path.exists(client_file) else 'Not found'}")
        print(f"Dictionary file: {dictionary_file} - {'Found' if os.path.exists(dictionary_file) else 'Not found'}")
        exit(1)

    # ìœ ì‚¬ë„ ê³„ì‚°
    print("Calculating similarity between smashed data...")
    accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count = calculate_voter_similarity(
        client_file, dictionary_file, client_data, server_data, n
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("VOTER DATA SMASHED DATA SIMILARITY RESULTS")
    print("="*50)
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Successful Mean Distance: {successful_mean_distance:.4f}")
    print(f"Unsuccessful Mean Distance: {unsuccessful_mean_distance:.4f}")
    print(f"Successful Distance Variance: {successful_distance_variance:.4f}")
    print(f"Unsuccessful Distance Variance: {unsuccessful_distance_variance:.4f}")

    print(f"\nSuccess Indices (Top {n}): {len(success_indices)} matches found")
    for idx, rank in success_indices[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
        print(f"  Client {idx} matched at rank {rank}")

    print(f"\nSuccess Ranks Count:")
    for rank, count in success_ranks_count.items():
        print(f"  Rank {rank}: {count} successes")

    print("\nSimilarity calculation completed!")
