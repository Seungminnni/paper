{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081c6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at lyeonii/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\MCC\\anaconda3\\envs\\ieie\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCC\\anaconda3\\envs\\ieie\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-train용\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[-8]  # n번째 레이어의 hidden states를 반환합니다.\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "    \n",
    "\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data_A = pd.read_csv(\"output1.csv\")  # data set A 파일명에 맞게 수정\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B 파일명에 맞게 수정\n",
    "# 모델 저장 경로\n",
    "model_path = \"Pre-trained.pt\"\n",
    "\n",
    "# X_train, Y_train 생성\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():  # 중복 제거를 하지 않고 원본 데이터 사용\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "#print(\"X_train\\n\", X_train[:10])\n",
    "#print(\"Y_train\\n\", Y_train[:10])\n",
    "        \n",
    "# BERT 토크나이저 및 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('lyeonii/bert-medium')\n",
    "model = CustomBertForSequenceClassification.from_pretrained('lyeonii/bert-medium', num_labels=2)\n",
    "\n",
    "# 입력 데이터를 BERT의 입력 형식으로 변환\n",
    "max_len = 128  # 입력 시퀀스의 최대 길이\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # 환자 정보 및 증상\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] 토큰 추가\n",
    "                        max_length = max_len,         # 최대 길이 지정\n",
    "                        pad_to_max_length = True,     # 패딩을 추가하여 최대 길이로 맞춤\n",
    "                        return_attention_mask = True, # 어텐션 마스크 생성\n",
    "                        return_tensors = 'pt',        # PyTorch 텐서로 반환\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = 0.8\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=1-train_size, random_state=42)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저 및 학습률 설정\n",
    "# 기본 학습률 : 2e-6\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 에폭 설정\n",
    "epochs = 10\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[1]  # loss가 outputs의 두 번째 값입니다.\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # 모델 저장 및 평가\n",
    "    model_save_path = f\"Pre_train_epoch{epoch + 1}_BERT_Medium.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved for epoch {epoch + 1} at {model_save_path}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs[0]  # logits가 outputs의 첫 번째 값입니다.\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "\n",
    "    print(f'Validation Accuracy for epoch {epoch + 1}: {val_accuracy / len(val_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b3cf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at lyeonii/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 41374210\n",
      "Layer: bert.embeddings.word_embeddings.weight, Size: 15627264\n",
      "Layer: bert.embeddings.position_embeddings.weight, Size: 262144\n",
      "Layer: bert.embeddings.token_type_embeddings.weight, Size: 1024\n",
      "Layer: bert.embeddings.LayerNorm.weight, Size: 512\n",
      "Layer: bert.embeddings.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.0.attention.self.query.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.0.attention.self.query.bias, Size: 512\n",
      "Layer: bert.encoder.layer.0.attention.self.key.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.0.attention.self.key.bias, Size: 512\n",
      "Layer: bert.encoder.layer.0.attention.self.value.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.0.attention.self.value.bias, Size: 512\n",
      "Layer: bert.encoder.layer.0.attention.output.dense.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.0.attention.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.0.attention.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.0.attention.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.0.intermediate.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.0.intermediate.dense.bias, Size: 2048\n",
      "Layer: bert.encoder.layer.0.output.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.0.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.0.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.0.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.1.attention.self.query.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.1.attention.self.query.bias, Size: 512\n",
      "Layer: bert.encoder.layer.1.attention.self.key.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.1.attention.self.key.bias, Size: 512\n",
      "Layer: bert.encoder.layer.1.attention.self.value.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.1.attention.self.value.bias, Size: 512\n",
      "Layer: bert.encoder.layer.1.attention.output.dense.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.1.attention.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.1.attention.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.1.attention.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.1.intermediate.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.1.intermediate.dense.bias, Size: 2048\n",
      "Layer: bert.encoder.layer.1.output.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.1.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.1.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.1.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.2.attention.self.query.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.2.attention.self.query.bias, Size: 512\n",
      "Layer: bert.encoder.layer.2.attention.self.key.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.2.attention.self.key.bias, Size: 512\n",
      "Layer: bert.encoder.layer.2.attention.self.value.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.2.attention.self.value.bias, Size: 512\n",
      "Layer: bert.encoder.layer.2.attention.output.dense.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.2.attention.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.2.attention.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.2.attention.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.2.intermediate.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.2.intermediate.dense.bias, Size: 2048\n",
      "Layer: bert.encoder.layer.2.output.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.2.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.2.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.2.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.3.attention.self.query.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.3.attention.self.query.bias, Size: 512\n",
      "Layer: bert.encoder.layer.3.attention.self.key.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.3.attention.self.key.bias, Size: 512\n",
      "Layer: bert.encoder.layer.3.attention.self.value.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.3.attention.self.value.bias, Size: 512\n",
      "Layer: bert.encoder.layer.3.attention.output.dense.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.3.attention.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.3.attention.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.3.attention.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.3.intermediate.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.3.intermediate.dense.bias, Size: 2048\n",
      "Layer: bert.encoder.layer.3.output.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.3.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.3.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.3.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.4.attention.self.query.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.4.attention.self.query.bias, Size: 512\n",
      "Layer: bert.encoder.layer.4.attention.self.key.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.4.attention.self.key.bias, Size: 512\n",
      "Layer: bert.encoder.layer.4.attention.self.value.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.4.attention.self.value.bias, Size: 512\n",
      "Layer: bert.encoder.layer.4.attention.output.dense.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.4.attention.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.4.attention.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.4.attention.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.4.intermediate.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.4.intermediate.dense.bias, Size: 2048\n",
      "Layer: bert.encoder.layer.4.output.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.4.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.4.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.4.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.5.attention.self.query.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.5.attention.self.query.bias, Size: 512\n",
      "Layer: bert.encoder.layer.5.attention.self.key.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.5.attention.self.key.bias, Size: 512\n",
      "Layer: bert.encoder.layer.5.attention.self.value.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.5.attention.self.value.bias, Size: 512\n",
      "Layer: bert.encoder.layer.5.attention.output.dense.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.5.attention.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.5.attention.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.5.attention.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.5.intermediate.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.5.intermediate.dense.bias, Size: 2048\n",
      "Layer: bert.encoder.layer.5.output.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.5.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.5.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.5.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.6.attention.self.query.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.6.attention.self.query.bias, Size: 512\n",
      "Layer: bert.encoder.layer.6.attention.self.key.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.6.attention.self.key.bias, Size: 512\n",
      "Layer: bert.encoder.layer.6.attention.self.value.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.6.attention.self.value.bias, Size: 512\n",
      "Layer: bert.encoder.layer.6.attention.output.dense.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.6.attention.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.6.attention.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.6.attention.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.6.intermediate.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.6.intermediate.dense.bias, Size: 2048\n",
      "Layer: bert.encoder.layer.6.output.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.6.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.6.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.6.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.7.attention.self.query.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.7.attention.self.query.bias, Size: 512\n",
      "Layer: bert.encoder.layer.7.attention.self.key.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.7.attention.self.key.bias, Size: 512\n",
      "Layer: bert.encoder.layer.7.attention.self.value.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.7.attention.self.value.bias, Size: 512\n",
      "Layer: bert.encoder.layer.7.attention.output.dense.weight, Size: 262144\n",
      "Layer: bert.encoder.layer.7.attention.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.7.attention.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.7.attention.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.encoder.layer.7.intermediate.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.7.intermediate.dense.bias, Size: 2048\n",
      "Layer: bert.encoder.layer.7.output.dense.weight, Size: 1048576\n",
      "Layer: bert.encoder.layer.7.output.dense.bias, Size: 512\n",
      "Layer: bert.encoder.layer.7.output.LayerNorm.weight, Size: 512\n",
      "Layer: bert.encoder.layer.7.output.LayerNorm.bias, Size: 512\n",
      "Layer: bert.pooler.dense.weight, Size: 262144\n",
      "Layer: bert.pooler.dense.bias, Size: 512\n",
      "Layer: classifier.weight, Size: 1024\n",
      "Layer: classifier.bias, Size: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def print_parameter_sizes(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Layer: {name}, Size: {param.numel()}\")\n",
    "\n",
    "\n",
    "model = CustomBertForSequenceClassification.from_pretrained('lyeonii/bert-medium', num_labels=2)\n",
    "print(\"Total number of parameters:\", count_parameters(model))\n",
    "print_parameter_sizes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d715e452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at lyeonii/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\MCC\\anaconda3\\envs\\biotf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train model loaded.\n",
      "True\n",
      "Epoch 1/20, Average Training Loss: 0.32414423365418504\n",
      "Model saved for epoch 1 at Fine_tuned_epoch1_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 1: 0.8900089605734768\n",
      "Epoch 2/20, Average Training Loss: 0.3154187362368514\n",
      "Model saved for epoch 2 at Fine_tuned_epoch2_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 2: 0.8895609318996416\n",
      "Epoch 3/20, Average Training Loss: 0.3119955459019033\n",
      "Model saved for epoch 3 at Fine_tuned_epoch3_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 3: 0.8875448028673836\n",
      "Epoch 4/20, Average Training Loss: 0.3044623065118988\n",
      "Model saved for epoch 4 at Fine_tuned_epoch4_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 4: 0.8859767025089605\n",
      "Epoch 5/20, Average Training Loss: 0.30504115519484853\n",
      "Model saved for epoch 5 at Fine_tuned_epoch5_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 5: 0.8895609318996416\n",
      "Epoch 6/20, Average Training Loss: 0.30758850936724885\n",
      "Model saved for epoch 6 at Fine_tuned_epoch6_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 6: 0.8895609318996416\n",
      "Epoch 7/20, Average Training Loss: 0.3030547809188928\n",
      "Model saved for epoch 7 at Fine_tuned_epoch7_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 7: 0.8879928315412187\n",
      "Epoch 8/20, Average Training Loss: 0.30474921552146356\n",
      "Model saved for epoch 8 at Fine_tuned_epoch8_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 8: 0.8879928315412187\n",
      "Epoch 9/20, Average Training Loss: 0.303454743985965\n",
      "Model saved for epoch 9 at Fine_tuned_epoch9_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 9: 0.8895609318996416\n",
      "Epoch 10/20, Average Training Loss: 0.2972988921815787\n",
      "Model saved for epoch 10 at Fine_tuned_epoch10_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 10: 0.8819444444444444\n",
      "Epoch 11/20, Average Training Loss: 0.29527058458425165\n",
      "Model saved for epoch 11 at Fine_tuned_epoch11_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 11: 0.8891129032258065\n",
      "Epoch 12/20, Average Training Loss: 0.29533952329216934\n",
      "Model saved for epoch 12 at Fine_tuned_epoch12_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 12: 0.8844086021505376\n",
      "Epoch 13/20, Average Training Loss: 0.2949205866250081\n",
      "Model saved for epoch 13 at Fine_tuned_epoch13_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 13: 0.8839605734767025\n",
      "Epoch 14/20, Average Training Loss: 0.29580108179309506\n",
      "Model saved for epoch 14 at Fine_tuned_epoch14_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 14: 0.8835125448028674\n",
      "Epoch 15/20, Average Training Loss: 0.29033466095362254\n",
      "Model saved for epoch 15 at Fine_tuned_epoch15_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 15: 0.8810483870967742\n",
      "Epoch 16/20, Average Training Loss: 0.2903110614818771\n",
      "Model saved for epoch 16 at Fine_tuned_epoch16_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 16: 0.8794802867383512\n",
      "Epoch 17/20, Average Training Loss: 0.2910443521369763\n",
      "Model saved for epoch 17 at Fine_tuned_epoch17_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 17: 0.8870967741935484\n",
      "Epoch 18/20, Average Training Loss: 0.2952683362655523\n",
      "Model saved for epoch 18 at Fine_tuned_epoch18_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 18: 0.8810483870967742\n",
      "Epoch 19/20, Average Training Loss: 0.28746420699285297\n",
      "Model saved for epoch 19 at Fine_tuned_epoch19_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 19: 0.8870967741935484\n",
      "Epoch 20/20, Average Training Loss: 0.28692914551593424\n",
      "Model saved for epoch 20 at Fine_tuned_epoch20_BERT_Medium.pt\n",
      "Validation Accuracy for epoch 20: 0.8734318996415771\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune용\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[-8]  # n번째 레이어의 hidden states를 반환합니다.\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data_A = pd.read_csv(\"output3.csv\")  # data set A 파일명에 맞게 수정\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B 파일명에 맞게 수정\n",
    "# 모델 불러오는 경로\n",
    "model_path = \"Pre_train_epoch10_BERT_Medium.pt\"\n",
    "# 모델 저장경로\n",
    "model_path2 = \"Fine-tuned.pt\"\n",
    "\n",
    "# X_train, Y_train 생성\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():  # 중복 제거를 하지 않고 원본 데이터 사용\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "#print(\"X_train\\n\", X_train[:10])\n",
    "#print(\"Y_train\\n\", Y_train[:10])\n",
    "        \n",
    "# BERT 토크나이저 및 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('lyeonii/bert-medium')\n",
    "# 모델이 이미 저장되어 있는지 확인하고, 저장된 모델이 있으면 불러오고 없으면 새로운 모델 생성\n",
    "if os.path.exists(model_path):\n",
    "    # 저장된 모델이 있을 경우 불러오기\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('lyeonii/bert-medium', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Pre-train model loaded.\")\n",
    "else:\n",
    "    # 저장된 모델이 없을 경우 새로운 모델 생성\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('lyeonii/bert-medium', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "# 입력 데이터를 BERT의 입력 형식으로 변환\n",
    "max_len = 128  # 입력 시퀀스의 최대 길이\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # 환자 정보 및 증상\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] 토큰 추가\n",
    "                        max_length = max_len,         # 최대 길이 지정\n",
    "                        pad_to_max_length = True,     # 패딩을 추가하여 최대 길이로 맞춤\n",
    "                        return_attention_mask = True, # 어텐션 마스크 생성\n",
    "                        return_tensors = 'pt',        # PyTorch 텐서로 반환\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = 0.8\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=1-train_size, random_state=42)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저 및 학습률 설정\n",
    "# 기본 학습률 : 2e-6\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)\n",
    "\n",
    "# 에폭 설정\n",
    "epochs = 20\n",
    "\n",
    "# 학습 루프\n",
    "hidden_states_list = []  # 모든 에폭에 대한 hidden state를 저장할 리스트\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[1]  # loss가 outputs의 두 번째 값입니다.\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # 모델 저장 및 평가\n",
    "    model_save_path = f\"Fine_tuned_epoch{epoch + 1}_BERT_Medium.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved for epoch {epoch + 1} at {model_save_path}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs[0]  # logits가 outputs의 첫 번째 값입니다.\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "\n",
    "    print(f'Validation Accuracy for epoch {epoch + 1}: {val_accuracy / len(val_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3936463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at lyeonii/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train model loaded.\n",
      "True\n",
      "Validation Accuracy: 0.8799603174603174\n"
     ]
    }
   ],
   "source": [
    "# smashed data 생성 (500/server side)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[7]  # n번째 레이어의 hidden states를 반환합니다.\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data_A = pd.read_csv(\"random_500.csv\")  # data set A 파일명에 맞게 수정\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B 파일명에 맞게 수정\n",
    "# 모델 저장 경로\n",
    "model_path = \"Pre_train_epoch10_BERT_Medium.pt\"\n",
    "\n",
    "# X_train, Y_train 생성\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():  # 중복 제거를 하지 않고 원본 데이터 사용\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "#print(\"X_train\\n\", X_train[:10])\n",
    "#print(\"Y_train\\n\", Y_train[:10])\n",
    "        \n",
    "# BERT 토크나이저 및 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('lyeonii/bert-medium')\n",
    "# 모델이 이미 저장되어 있는지 확인하고, 저장된 모델이 있으면 불러오고 없으면 새로운 모델 생성\n",
    "if os.path.exists(model_path):\n",
    "    # 저장된 모델이 있을 경우 불러오기\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('lyeonii/bert-medium', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Pre-train model loaded.\")\n",
    "else:\n",
    "    # 저장된 모델이 없을 경우 새로운 모델 생성\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('lyeonii/bert-medium', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "# 입력 데이터를 BERT의 입력 형식으로 변환\n",
    "max_len = 128  # 입력 시퀀스의 최대 길이\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # 환자 정보 및 증상\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] 토큰 추가\n",
    "                        max_length = max_len,         # 최대 길이 지정\n",
    "                        pad_to_max_length = True,     # 패딩을 추가하여 최대 길이로 맞춤\n",
    "                        return_attention_mask = True, # 어텐션 마스크 생성\n",
    "                        return_tensors = 'pt',        # PyTorch 텐서로 반환\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 데이터로더 생성\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "val_accuracy = 0\n",
    "hidden_states_list = []  # 평가할 때 hidden state를 저장할 리스트\n",
    "for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0],\n",
    "              'attention_mask': batch[1],\n",
    "              'labels': batch[2]}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs[0]  # logits가 outputs의 첫 번째 값입니다.\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = inputs['labels'].cpu().numpy()\n",
    "    val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "    # hidden state를 저장합니다.\n",
    "    hidden_states = outputs[2]\n",
    "    hidden_states_list.append(hidden_states)\n",
    "hidden_states_concat = torch.cat(hidden_states_list, dim=0)\n",
    "hidden_states_concat = hidden_states_concat[:, 0, :].cpu().detach().numpy()\n",
    "hidden_states_df = pd.DataFrame(hidden_states_concat)\n",
    "hidden_states_df.to_csv(\"Dictionary_smashed_data_layer1.csv\", index=False)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy / len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0b701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at lyeonii/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCC\\anaconda3\\envs\\ieie\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Validation Accuracy: 0.890625\n"
     ]
    }
   ],
   "source": [
    "# smashed data 생성 (100/client side) #라이브러리 개변\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[7]  # n번째 레이어의 hidden states를 반환합니다.\n",
    "        #for j in range(len(hidden_states)):  # hidden_states는 768차원으로 구성되어있음\n",
    "        #    noise = np.random.normal(0, 10.0)  # 표준 정규 분포에서 적절한 분산값을 사용하여 랜덤한 노이즈 생성\n",
    "        #    hidden_states[j] += noise  # hidden_states의 값에 노이즈 추가\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data_A = pd.read_csv(\"random_300.csv\")  # data set A 파일명에 맞게 수정\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B 파일명에 맞게 수정\n",
    "# 모델 저장 경로\n",
    "model_path = \"Fine_tuned_epoch20_BERT_Medium.pt\"\n",
    "\n",
    "# X_train, Y_train 생성\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():  # 중복 제거를 하지 않고 원본 데이터 사용\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "#print(\"X_train\\n\", X_train[:10])\n",
    "#print(\"Y_train\\n\", Y_train[:10])\n",
    "        \n",
    "# BERT 토크나이저 및 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('lyeonii/bert-medium')\n",
    "# 모델이 이미 저장되어 있는지 확인하고, 저장된 모델이 있으면 불러오고 없으면 새로운 모델 생성\n",
    "if os.path.exists(model_path):\n",
    "    # 저장된 모델이 있을 경우 불러오기\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('lyeonii/bert-medium', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path), strict=False)\n",
    "    print(\"Pre-train model loaded.\")\n",
    "else:\n",
    "    # 저장된 모델이 없을 경우 새로운 모델 생성\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('lyeonii/bert-medium', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "# 입력 데이터를 BERT의 입력 형식으로 변환\n",
    "max_len = 128  # 입력 시퀀스의 최대 길이\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # 환자 정보 및 증상\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] 토큰 추가\n",
    "                        max_length = max_len,         # 최대 길이 지정\n",
    "                        pad_to_max_length = True,     # 패딩을 추가하여 최대 길이로 맞춤\n",
    "                        return_attention_mask = True, # 어텐션 마스크 생성\n",
    "                        return_tensors = 'pt',        # PyTorch 텐서로 반환\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 데이터로더 생성\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "val_accuracy = 0\n",
    "hidden_states_list = []  # 평가할 때 hidden state를 저장할 리스트\n",
    "for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0],\n",
    "              'attention_mask': batch[1],\n",
    "              'labels': batch[2]}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs[0]  # logits가 outputs의 첫 번째 값입니다.\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = inputs['labels'].cpu().numpy()\n",
    "    val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "    # hidden state를 저장합니다.\n",
    "    hidden_states = outputs[2]\n",
    "    hidden_states_list.append(hidden_states)\n",
    "hidden_states_concat = torch.cat(hidden_states_list, dim=0)\n",
    "hidden_states_concat = hidden_states_concat[:, 0, :].cpu().detach().numpy()\n",
    "hidden_states_df = pd.DataFrame(hidden_states_concat)\n",
    "hidden_states_df.to_csv(\"Client_smashed_data_layer1.csv\", index=False)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy / len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f678a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 유클리드 거리 유사도\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def calculate_accuracy_and_distance(client_file, dictionary_file, original_file_client, original_file_dictionary, n=5):\n",
    "    # 변환된 파일을 읽어옵니다.\n",
    "    client_data = pd.read_csv(client_file)\n",
    "    dictionary_data = pd.read_csv(dictionary_file)\n",
    "    \n",
    "    # 원본 파일을 읽어옵니다.\n",
    "    original_client_data = pd.read_csv(original_file_client)\n",
    "    original_dictionary_data = pd.read_csv(original_file_dictionary)\n",
    "    \n",
    "    # 데이터 포인트 간의 유클리드 거리를 계산합니다.\n",
    "    distances = euclidean_distances(client_data.values, dictionary_data.values)\n",
    "    \n",
    "    # Top@n 유사도를 찾습니다.\n",
    "    topn_similarities = np.argsort(distances, axis=1)[:, :n]\n",
    "    topn_values = np.sort(distances, axis=1)[:, :n]\n",
    "    \n",
    "    # 모든 결과를 출력하고 정확도를 계산합니다.\n",
    "    successful_distances = []\n",
    "    unsuccessful_distances = []\n",
    "    successes = 0\n",
    "    success_indices = []  # 성공한 인덱스를 저장할 리스트\n",
    "    success_ranks_count = {rank: 0 for rank in range(1, n+1)}  # 각 성공한 서버 측 랭크의 수를 저장할 딕셔너리\n",
    "    for i, (indices, scores) in enumerate(zip(topn_similarities, topn_values)):\n",
    "        \"\"\"print(f\"\\nTop {n} inferences for client {i + 1}:\")\"\"\"\n",
    "        for rank, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "            \"\"\"print(f\"Server {idx + 1} with distance {score}\")\"\"\"\n",
    "            if original_client_data.iloc[i].equals(original_dictionary_data.iloc[idx]):\n",
    "                successes += 1\n",
    "                successful_distances.append(score)\n",
    "                success_indices.append((i + 1, rank))  # 성공한 인덱스를 추가\n",
    "                success_ranks_count[rank] += 1  # 해당 랭크의 수를 증가시킴\n",
    "            else:\n",
    "                unsuccessful_distances.append(score)\n",
    "        if successes == 0:\n",
    "            print(\"No successful match found.\")\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = successes / len(client_data)\n",
    "    \n",
    "    # 성공적으로 일치하는 데이터 포인트와 클라이언트 데이터 포인트, 그리고 일치하지 않는 데이터 포인트와 클라이언트 데이터 포인트 간의 평균 거리를 계산합니다.\n",
    "    successful_mean_distance = np.mean(successful_distances)\n",
    "    unsuccessful_mean_distance = np.mean(unsuccessful_distances)\n",
    "    \n",
    "    # 평균 거리의 분산 계산\n",
    "    successful_distance_variance = np.var(successful_distances)\n",
    "    unsuccessful_distance_variance = np.var(unsuccessful_distances)\n",
    "    \n",
    "    return accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count\n",
    "\n",
    "# 변환된 파일 경로\n",
    "dictionary_file = \"Dictionary_smashed_data_layer1.csv\"\n",
    "\n",
    "# 원본 파일 경로\n",
    "original_file_client = \"random_300.csv\"\n",
    "original_file_dictionary = \"random_500.csv\"\n",
    "\n",
    "# Top n 설정\n",
    "n = 5\n",
    "\n",
    "# 정확도 계산 및 평균 거리 계산\n",
    "\n",
    "client_file = f'Client_smashed_data_layer1.csv'\n",
    "accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count = calculate_accuracy_and_distance(client_file, dictionary_file, original_file_client, original_file_dictionary, n)\n",
    "\n",
    "print(\"\\nFor file:\", client_file)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Successful Mean Distance:\", successful_mean_distance)\n",
    "print(\"Unsuccessful Mean Distance:\", unsuccessful_mean_distance)\n",
    "\n",
    "# 분산 출력\n",
    "print(\"Successful Distance Variance:\", successful_distance_variance)\n",
    "print(\"Unsuccessful Distance Variance:\", unsuccessful_distance_variance)\n",
    "\n",
    "# 성공한 인덱스들을 출력합니다.\n",
    "print(\"Success Indices:\", success_indices)\n",
    "\n",
    "# 각 성공한 서버 측 랭크의 수를 출력합니다.\n",
    "print(\"Success Ranks Count:\")\n",
    "for rank, count in success_ranks_count.items():\n",
    "    print(f\"Rank {rank}: {count} successes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcbde76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
