{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081c6c04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      " ['7/29/1966, nan, 999-91-3709, S99988287, X5601074X, Mrs., Celia938, Roberts511, nan, Mayert710, M, white, hispanic, F, Agawam  Massachusetts  US, 362 Pacocha Gateway Apt 1, Northborough, Massachusetts, Worcester County, 1532.0, 42.27341123, -71.63243239, 1166971.45, 13416.2, 4/20/1989, nan, 5cfda74f-b462-4c73-aa96-d90da4002f8a, 40055000.0, Chronic sinusitis (disorder), Body mass index 30+ - obesity (finding), Miscarriage in first trimester, Prediabetes, Hyperlipidemia, Nasal congestion (finding), Cough (finding), Sore throat symptom (finding), Sputum finding (finding), Muscle pain (finding), Joint pain (finding), Fever (finding)', '12/19/1965, 3/1/2020, 999-70-4989, S99948277, X2560575X, Mrs., Kala987, Prohaska837, nan, Gleason633, M, white, nonhispanic, F, Boston  Massachusetts  US, 310 Effertz Promenade, Gloucester, Massachusetts, Essex County, 1930.0, 42.63072986, -70.6443488, 1229943.52, 20003.74, 2/12/1984, nan, 3fc7077f-903c-4601-8078-a016e9b5a630, 59621000.0, Hypertension, Chronic sinusitis (disorder), Body mass index 30+ - obesity (finding), Hyperlipidemia, Prediabetes, Dyspnea (finding), Wheezing (finding), Diarrhea symptom (finding), Fever (finding), Pneumonia (disorder), Hypoxemia (disorder), Respiratory distress (finding), Acute respiratory failure (disorder), Sepsis caused by virus (disorder), Acute pulmonary embolism (disorder), Injury of heart (disorder), Heart failure (disorder), Acute respiratory distress syndrome (disorder)', '6/10/1967, nan, 999-91-1294, S99999431, X11888547X, Mr., Deon400, Mante251, nan, nan, M, white, hispanic, M, Reading  Massachusetts  US, 479 Turcotte Byway Apt 47, Beverly, Massachusetts, Essex County, nan, 42.61134382, -70.87263385, 1216308.17, 5884.4, 7/5/1980, nan, e947863a-d0d9-4943-ac51-61b92f928cfa, 162864005.0, Body mass index 30+ - obesity (finding), Prediabetes, Anemia (disorder), Drug overdose, Opioid abuse (disorder), First degree burn, Cough (finding), Sputum finding (finding), Fever (finding), Loss of taste (finding)', '12/26/1995, nan, 999-11-8772, S99919263, X76708464X, Ms., Mar√≠a Cristina383, Laureano185, nan, nan, nan, white, hispanic, F, Santo Domingo  National District  DO, 506 Daugherty Neck Unit 41, Hampden, Massachusetts, Hampden County, nan, 42.11948085, -72.41484443, 515342.88, 6450.27, 2/18/2014, nan, 02d68d39-2195-4b7b-8a80-0412a32a88fe, 59621000.0, Hypertension, Nasal congestion (finding)', '5/3/1996, nan, 999-17-4976, S99924959, X58550607X, Ms., Shane235, Wunsch504, nan, nan, nan, white, nonhispanic, F, Nizhny Novgorod  Nizhny Novgorod Oblast  RU, 155 Spinka Key, Boston, Massachusetts, Suffolk County, 2109.0, 42.24382202, -71.19304971, 501015.88, 2846.4, 3/10/2020, 4/1/2020, 6ec793c5-14fd-408f-8982-05e1266fa67b, 49727002.0, Cough (finding), Sore throat symptom (finding), Fever (finding), Loss of taste (finding)', '1/10/1916, 8/31/1984, 999-90-2276, S99956796, X30521847X, Mr., Cristobal567, Osorio731, nan, nan, M, white, hispanic, M, Panama City  Panama  PA, 762 Schmidt Gateway, Stow, Massachusetts, Middlesex County, nan, 42.44215829, -71.49530302, 1680489.4, 25890.01, 3/14/1938, nan, d9fc9523-5754-4ffa-97ef-9fabaf65fd55, 44054006.0, Diabetes, Chronic kidney disease stage 1 (disorder), Diabetic renal disease (disorder), Hypertension, Hypertriglyceridemia (disorder), Metabolic syndrome X (disorder), Osteoarthritis of knee, Neoplasm of prostate, Metastasis from malignant tumor of prostate (disorder)', '3/9/1930, nan, 999-20-7431, S99921262, X68783451X, Mr., Mary779, Legros616, nan, nan, M, white, nonhispanic, M, Boston  Massachusetts  US, 710 Bailey Parade, Amesbury, Massachusetts, Essex County, nan, 42.8768494, -70.96107574, 1552037.01, 539700.96, 12/28/1948, nan, 83a219ef-6ee3-42a0-99bc-d22c0643e7ec, 40055000.0, Chronic sinusitis (disorder), Opioid abuse (disorder), Smokes tobacco daily, Prediabetes, Anemia (disorder), Hyperlipidemia, Diabetes, Neuropathy due to type 2 diabetes mellitus (disorder), Hypertriglyceridemia (disorder), Metabolic syndrome X (disorder), Chronic kidney disease stage 1 (disorder), Diabetic renal disease (disorder), Neoplasm of prostate, Carcinoma in situ of prostate (disorder), Chronic intractable migraine without aura, Sore throat symptom (finding), Sputum finding (finding), Fatigue (finding), Hemoptysis (finding), Dyspnea (finding), Wheezing (finding), Fever (finding), Loss of taste (finding)', '9/7/1992, nan, 999-56-7732, S99978179, X79244170X, Mrs., Libby988, Oberbrunner298, nan, Strosin214, M, white, nonhispanic, F, Quincy  Massachusetts  US, 880 Effertz Fort, New Bedford, Massachusetts, Bristol County, 2748.0, 41.62843647, -70.92306453, 482959.37, 3095.45, 12/10/2006, nan, 8fb4077f-4fa2-4c6d-bcb4-965e980852ba, 124000000000000.0, Chronic intractable migraine without aura, Impacted molars, Chronic pain, Drug overdose, Normal pregnancy, Cough (finding), Sputum finding (finding), Dyspnea (finding), Wheezing (finding), Fever (finding)', '1/25/2015, nan, 999-52-6460, nan, nan, nan, Lionel365, Dickinson688, nan, nan, nan, white, nonhispanic, M, Leominster  Massachusetts  US, 1074 Kulas Promenade Unit 28, Shirley, Massachusetts, Middlesex County, 1464.0, 42.60861738, -71.62818225, 134740.72, 2015.72, 8/3/2019, 8/11/2019, a789710b-e8c5-4655-b66d-820c644f5120, 43878008.0, Streptococcal sore throat (disorder), Cough (finding), Sputum finding (finding), Chill (finding), Fever (finding)', '3/11/2004, nan, 999-70-4190, S99919185, nan, nan, Pablo44, Klein929, nan, nan, nan, white, nonhispanic, M, Franklin  Massachusetts  US, 188 Hane Center, Chicopee, Massachusetts, Hampden County, nan, 42.2051048, -72.56331605, 352326.01, 3116.44, 3/7/2020, 3/26/2020, ab61e2ff-8315-4a69-b92f-84add51ae886, 49727002.0, Cough (finding), Fatigue (finding), Fever (finding)']\n",
      "Y_train\n",
      " [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\MCC\\anaconda3\\envs\\biotf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1/10, Batch Loss: 0.5948078632354736\n",
      "Epoch 1/10, Batch Loss: 0.4141175150871277\n",
      "Epoch 1/10, Batch Loss: 0.9622668027877808\n",
      "Epoch 1/10, Batch Loss: 0.48511311411857605\n",
      "Epoch 1/10, Batch Loss: 0.46169981360435486\n",
      "Epoch 1/10, Batch Loss: 0.6177780628204346\n",
      "Epoch 1/10, Batch Loss: 0.6891560554504395\n",
      "Epoch 1/10, Batch Loss: 0.4923348128795624\n",
      "Epoch 1/10, Batch Loss: 0.5581233501434326\n",
      "Epoch 1/10, Batch Loss: 0.7338570952415466\n",
      "Epoch 1/10, Batch Loss: 0.5559920072555542\n",
      "Epoch 1/10, Batch Loss: 0.5882829427719116\n",
      "Epoch 1/10, Batch Loss: 0.612876296043396\n",
      "Epoch 1/10, Batch Loss: 0.5901021957397461\n",
      "Epoch 1/10, Batch Loss: 0.5141141414642334\n",
      "Epoch 1/10, Batch Loss: 0.6080482006072998\n",
      "Epoch 1/10, Batch Loss: 0.552339494228363\n",
      "Epoch 1/10, Batch Loss: 0.29683631658554077\n",
      "Epoch 1/10, Batch Loss: 0.4292985200881958\n",
      "Epoch 1/10, Batch Loss: 0.44860002398490906\n",
      "Epoch 1/10, Batch Loss: 0.5967109799385071\n",
      "Epoch 1/10, Batch Loss: 0.7726467847824097\n",
      "Epoch 1/10, Batch Loss: 0.379714697599411\n",
      "Epoch 1/10, Batch Loss: 0.5429425239562988\n",
      "Epoch 1/10, Batch Loss: 0.6715739369392395\n",
      "Epoch 1/10, Batch Loss: 0.3890535235404968\n",
      "Epoch 1/10, Batch Loss: 0.3950265645980835\n",
      "Epoch 1/10, Batch Loss: 0.8282049894332886\n",
      "Epoch 1/10, Batch Loss: 0.32750678062438965\n",
      "Epoch 1/10, Batch Loss: 0.7243058085441589\n",
      "Epoch 1/10, Batch Loss: 0.598419189453125\n",
      "Epoch 1/10, Batch Loss: 0.4048624038696289\n",
      "Epoch 1/10, Batch Loss: 0.42487645149230957\n",
      "Epoch 1/10, Batch Loss: 0.23288527131080627\n",
      "Epoch 1/10, Batch Loss: 0.5047391653060913\n",
      "Epoch 1/10, Batch Loss: 0.15518993139266968\n",
      "Epoch 1/10, Batch Loss: 0.1673734039068222\n",
      "Epoch 1/10, Batch Loss: 0.11678335815668106\n",
      "Epoch 1/10, Batch Loss: 0.679195761680603\n",
      "Epoch 1/10, Batch Loss: 0.41173824667930603\n",
      "Epoch 1/10, Batch Loss: 0.3806235194206238\n",
      "Epoch 1/10, Batch Loss: 0.25635963678359985\n",
      "Epoch 1/10, Batch Loss: 0.39512941241264343\n",
      "Epoch 1/10, Batch Loss: 0.4318370819091797\n",
      "Epoch 1/10, Batch Loss: 0.4558584988117218\n",
      "Epoch 1/10, Batch Loss: 0.25426700711250305\n",
      "Epoch 1/10, Batch Loss: 0.410178542137146\n",
      "Epoch 1/10, Batch Loss: 0.5827755331993103\n",
      "Epoch 1/10, Batch Loss: 0.49546879529953003\n",
      "Epoch 1/10, Batch Loss: 0.2616187036037445\n",
      "Epoch 1/10, Batch Loss: 0.6998185515403748\n",
      "Epoch 1/10, Batch Loss: 0.5366404056549072\n",
      "Epoch 1/10, Batch Loss: 0.2674388289451599\n",
      "Epoch 1/10, Batch Loss: 0.287578821182251\n",
      "Epoch 1/10, Batch Loss: 0.6138930320739746\n",
      "Epoch 1/10, Batch Loss: 0.37769654393196106\n",
      "Epoch 1/10, Batch Loss: 0.4558987021446228\n",
      "Epoch 1/10, Batch Loss: 0.4108674228191376\n",
      "Epoch 1/10, Batch Loss: 0.1934627890586853\n",
      "Epoch 1/10, Batch Loss: 0.4927535355091095\n",
      "Epoch 1/10, Batch Loss: 0.3718118667602539\n",
      "Epoch 1/10, Batch Loss: 0.22333359718322754\n",
      "Epoch 1/10, Batch Loss: 0.15712818503379822\n",
      "Epoch 1/10, Batch Loss: 0.3799072802066803\n",
      "Epoch 1/10, Batch Loss: 0.2598489224910736\n",
      "Epoch 1/10, Batch Loss: 0.2804003059864044\n",
      "Epoch 1/10, Batch Loss: 0.353500634431839\n",
      "Epoch 1/10, Batch Loss: 0.24771957099437714\n",
      "Epoch 1/10, Batch Loss: 0.3762781620025635\n",
      "Epoch 1/10, Batch Loss: 0.13532672822475433\n",
      "Epoch 1/10, Batch Loss: 0.13620445132255554\n",
      "Epoch 1/10, Batch Loss: 0.11697696149349213\n",
      "Epoch 1/10, Batch Loss: 0.38487696647644043\n",
      "Epoch 1/10, Batch Loss: 0.4085041880607605\n",
      "Epoch 1/10, Batch Loss: 0.9428379535675049\n",
      "Epoch 1/10, Batch Loss: 0.24031300842761993\n",
      "Epoch 1/10, Batch Loss: 0.7425999045372009\n",
      "Epoch 1/10, Batch Loss: 0.24799759685993195\n",
      "Epoch 1/10, Batch Loss: 0.35989195108413696\n",
      "Epoch 1/10, Batch Loss: 0.2680366337299347\n",
      "Epoch 1/10, Batch Loss: 0.1399034708738327\n",
      "Epoch 1/10, Batch Loss: 0.26704880595207214\n",
      "Epoch 1/10, Batch Loss: 0.5390244722366333\n",
      "Epoch 1/10, Batch Loss: 0.8229112029075623\n",
      "Epoch 1/10, Batch Loss: 0.24826188385486603\n",
      "Epoch 1/10, Batch Loss: 0.10139884054660797\n",
      "Epoch 1/10, Batch Loss: 0.6324051022529602\n",
      "Epoch 1/10, Batch Loss: 0.6378332376480103\n",
      "Epoch 1/10, Batch Loss: 0.11260446161031723\n",
      "Epoch 1/10, Batch Loss: 0.364747017621994\n",
      "Epoch 1/10, Batch Loss: 0.30160245299339294\n",
      "Epoch 1/10, Batch Loss: 0.32979443669319153\n",
      "Epoch 1/10, Batch Loss: 0.5341494679450989\n",
      "Epoch 1/10, Batch Loss: 0.24277639389038086\n",
      "Epoch 1/10, Batch Loss: 0.15682685375213623\n",
      "Epoch 1/10, Batch Loss: 0.6033877730369568\n",
      "Epoch 1/10, Batch Loss: 0.48205724358558655\n",
      "Epoch 1/10, Batch Loss: 0.11286181956529617\n",
      "Epoch 1/10, Batch Loss: 0.2378837764263153\n",
      "Epoch 1/10, Batch Loss: 0.5458022356033325\n",
      "Epoch 1/10, Batch Loss: 0.2128695547580719\n",
      "Epoch 1/10, Batch Loss: 0.35842958092689514\n",
      "Epoch 1/10, Batch Loss: 0.42453303933143616\n",
      "Epoch 1/10, Batch Loss: 0.46160823106765747\n",
      "Epoch 1/10, Batch Loss: 0.39282023906707764\n",
      "Epoch 1/10, Batch Loss: 0.3919662833213806\n",
      "Epoch 1/10, Batch Loss: 0.40610402822494507\n",
      "Epoch 1/10, Batch Loss: 0.22441589832305908\n",
      "Epoch 1/10, Batch Loss: 0.5964813828468323\n",
      "Epoch 1/10, Batch Loss: 0.36438000202178955\n",
      "Epoch 1/10, Batch Loss: 0.24661776423454285\n",
      "Epoch 1/10, Batch Loss: 0.2725585997104645\n",
      "Epoch 1/10, Batch Loss: 0.6085999608039856\n",
      "Epoch 1/10, Batch Loss: 0.5146235227584839\n",
      "Epoch 1/10, Batch Loss: 0.5793464779853821\n",
      "Epoch 1/10, Batch Loss: 0.21861331164836884\n",
      "Epoch 1/10, Batch Loss: 0.3692748546600342\n",
      "Epoch 1/10, Batch Loss: 0.10906419903039932\n",
      "Epoch 1/10, Batch Loss: 0.11943499743938446\n",
      "Epoch 1/10, Batch Loss: 0.2599347233772278\n",
      "Epoch 1/10, Batch Loss: 0.34746888279914856\n",
      "Epoch 1/10, Batch Loss: 0.11345293372869492\n",
      "Epoch 1/10, Batch Loss: 0.5337036848068237\n",
      "Epoch 1/10, Batch Loss: 0.26490721106529236\n",
      "Epoch 1/10, Batch Loss: 0.24275220930576324\n",
      "Epoch 1/10, Batch Loss: 0.3419426679611206\n",
      "Epoch 1/10, Batch Loss: 0.48145756125450134\n",
      "Epoch 1/10, Batch Loss: 0.2551487386226654\n",
      "Epoch 1/10, Batch Loss: 0.3562072217464447\n",
      "Epoch 1/10, Batch Loss: 0.22048218548297882\n",
      "Epoch 1/10, Batch Loss: 0.34380921721458435\n",
      "Epoch 1/10, Batch Loss: 0.10023194551467896\n",
      "Epoch 1/10, Batch Loss: 0.3721052408218384\n",
      "Epoch 1/10, Batch Loss: 0.25499674677848816\n",
      "Epoch 1/10, Batch Loss: 0.21829114854335785\n",
      "Epoch 1/10, Batch Loss: 0.25830233097076416\n",
      "Epoch 1/10, Batch Loss: 0.5935778021812439\n",
      "Epoch 1/10, Batch Loss: 0.37005501985549927\n",
      "Epoch 1/10, Batch Loss: 0.21061865985393524\n",
      "Epoch 1/10, Batch Loss: 0.3547680974006653\n",
      "Epoch 1/10, Batch Loss: 0.5017073750495911\n",
      "Epoch 1/10, Batch Loss: 0.41019952297210693\n",
      "Epoch 1/10, Batch Loss: 0.2693718373775482\n",
      "Epoch 1/10, Batch Loss: 0.3579472303390503\n",
      "Epoch 1/10, Batch Loss: 0.40808260440826416\n",
      "Epoch 1/10, Batch Loss: 0.5230674147605896\n",
      "Epoch 1/10, Batch Loss: 0.36350739002227783\n",
      "Epoch 1/10, Batch Loss: 0.6307494640350342\n",
      "Epoch 1/10, Batch Loss: 0.33065325021743774\n",
      "Epoch 1/10, Batch Loss: 0.3660432696342468\n",
      "Epoch 1/10, Batch Loss: 0.47080284357070923\n",
      "Epoch 1/10, Batch Loss: 0.3717733919620514\n",
      "Epoch 1/10, Batch Loss: 0.12534332275390625\n",
      "Epoch 1/10, Batch Loss: 0.3907940983772278\n",
      "Epoch 1/10, Batch Loss: 0.6614501476287842\n",
      "Epoch 1/10, Batch Loss: 0.2503407597541809\n",
      "Epoch 1/10, Batch Loss: 0.3456721007823944\n",
      "Epoch 1/10, Batch Loss: 0.4614783227443695\n",
      "Epoch 1/10, Batch Loss: 0.401231974363327\n",
      "Epoch 1/10, Batch Loss: 0.2751207649707794\n",
      "Epoch 1/10, Batch Loss: 0.34105268120765686\n",
      "Epoch 1/10, Batch Loss: 0.45587822794914246\n",
      "Epoch 1/10, Batch Loss: 0.3517877757549286\n",
      "Epoch 1/10, Batch Loss: 0.32554686069488525\n",
      "Epoch 1/10, Batch Loss: 0.5750484466552734\n",
      "Epoch 1/10, Batch Loss: 0.47815728187561035\n",
      "Epoch 1/10, Batch Loss: 0.3764975666999817\n",
      "Epoch 1/10, Batch Loss: 0.3986106514930725\n",
      "Epoch 1/10, Batch Loss: 0.3675260841846466\n",
      "Epoch 1/10, Batch Loss: 0.15833443403244019\n",
      "Epoch 1/10, Batch Loss: 0.22279956936836243\n",
      "Epoch 1/10, Batch Loss: 0.20791472494602203\n",
      "Epoch 1/10, Batch Loss: 0.27821099758148193\n",
      "Epoch 1/10, Batch Loss: 0.2925754189491272\n",
      "Epoch 1/10, Batch Loss: 0.44842639565467834\n",
      "Epoch 1/10, Batch Loss: 0.36203134059906006\n",
      "Epoch 1/10, Batch Loss: 0.5645478963851929\n",
      "Epoch 1/10, Batch Loss: 0.36644670367240906\n",
      "Epoch 1/10, Batch Loss: 0.3268105089664459\n",
      "Epoch 1/10, Batch Loss: 0.3743632733821869\n",
      "Epoch 1/10, Batch Loss: 0.354669988155365\n",
      "Epoch 1/10, Batch Loss: 0.11602441221475601\n",
      "Epoch 1/10, Batch Loss: 0.5649960041046143\n",
      "Epoch 1/10, Batch Loss: 0.46072593331336975\n",
      "Epoch 1/10, Batch Loss: 0.421162873506546\n",
      "Epoch 1/10, Batch Loss: 0.33474594354629517\n",
      "Epoch 1/10, Batch Loss: 0.2879697382450104\n",
      "Epoch 1/10, Batch Loss: 0.3033852279186249\n",
      "Epoch 1/10, Batch Loss: 0.38105344772338867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch Loss: 0.25408414006233215\n",
      "Epoch 1/10, Batch Loss: 0.47630366683006287\n",
      "Epoch 1/10, Batch Loss: 0.4940844476222992\n",
      "Epoch 1/10, Batch Loss: 0.6591581702232361\n",
      "Epoch 1/10, Batch Loss: 0.241426482796669\n",
      "Epoch 1/10, Batch Loss: 0.39139968156814575\n",
      "Epoch 1/10, Batch Loss: 0.26176148653030396\n",
      "Epoch 1/10, Batch Loss: 0.6208778619766235\n",
      "Epoch 1/10, Batch Loss: 0.4253423810005188\n",
      "Epoch 1/10, Batch Loss: 0.4583967328071594\n",
      "Epoch 1/10, Batch Loss: 0.20162606239318848\n",
      "Epoch 1/10, Batch Loss: 0.4518060088157654\n",
      "Epoch 1/10, Batch Loss: 0.2670995891094208\n",
      "Epoch 1/10, Batch Loss: 0.39663153886795044\n",
      "Epoch 1/10, Batch Loss: 0.1207134798169136\n",
      "Epoch 1/10, Batch Loss: 0.2647384703159332\n",
      "Epoch 1/10, Batch Loss: 0.2026481330394745\n",
      "Epoch 1/10, Batch Loss: 0.730182945728302\n",
      "Epoch 1/10, Batch Loss: 0.4835037887096405\n",
      "Epoch 1/10, Batch Loss: 0.4554523825645447\n",
      "Epoch 1/10, Batch Loss: 0.31033214926719666\n",
      "Epoch 1/10, Batch Loss: 0.17102262377738953\n",
      "Epoch 1/10, Batch Loss: 0.43716344237327576\n",
      "Epoch 1/10, Batch Loss: 0.4829329550266266\n",
      "Epoch 1/10, Batch Loss: 0.1371437907218933\n",
      "Epoch 1/10, Batch Loss: 0.3511475920677185\n",
      "Epoch 1/10, Batch Loss: 0.5867054462432861\n",
      "Epoch 1/10, Batch Loss: 0.4794517159461975\n",
      "Epoch 1/10, Batch Loss: 0.34069886803627014\n",
      "Epoch 1/10, Batch Loss: 0.2710501253604889\n",
      "Epoch 1/10, Batch Loss: 0.5884445905685425\n",
      "Epoch 1/10, Batch Loss: 0.3442457318305969\n",
      "Epoch 1/10, Batch Loss: 0.29132771492004395\n",
      "Epoch 1/10, Batch Loss: 0.11851754039525986\n",
      "Epoch 1/10, Batch Loss: 0.3377709984779358\n",
      "Epoch 1/10, Batch Loss: 0.19868597388267517\n",
      "Epoch 1/10, Batch Loss: 0.12673041224479675\n",
      "Epoch 1/10, Batch Loss: 0.16370771825313568\n",
      "Epoch 1/10, Batch Loss: 0.47242146730422974\n",
      "Epoch 1/10, Batch Loss: 0.4132540822029114\n",
      "Epoch 1/10, Batch Loss: 0.516208827495575\n",
      "Epoch 1/10, Batch Loss: 0.39518287777900696\n",
      "Epoch 1/10, Batch Loss: 0.40404945611953735\n",
      "Epoch 1/10, Batch Loss: 0.6389626264572144\n",
      "Epoch 1/10, Batch Loss: 0.5473910570144653\n",
      "Epoch 1/10, Batch Loss: 0.4395512044429779\n",
      "Epoch 1/10, Batch Loss: 0.6922145485877991\n",
      "Epoch 1/10, Batch Loss: 0.3616282641887665\n",
      "Epoch 1/10, Batch Loss: 0.6283718347549438\n",
      "Epoch 1/10, Batch Loss: 0.3525744080543518\n",
      "Epoch 1/10, Batch Loss: 0.36994123458862305\n",
      "Epoch 1/10, Batch Loss: 0.2585793733596802\n",
      "Epoch 1/10, Batch Loss: 0.20305395126342773\n",
      "Epoch 1/10, Batch Loss: 0.38808661699295044\n",
      "Epoch 1/10, Batch Loss: 0.34163329005241394\n",
      "Epoch 1/10, Batch Loss: 0.12412970513105392\n",
      "Epoch 1/10, Batch Loss: 0.09568342566490173\n",
      "Epoch 1/10, Batch Loss: 0.0791192501783371\n",
      "Epoch 1/10, Batch Loss: 0.06250511854887009\n",
      "Epoch 1/10, Batch Loss: 0.23942038416862488\n",
      "Epoch 1/10, Batch Loss: 0.5361089110374451\n",
      "Epoch 1/10, Batch Loss: 0.38396844267845154\n",
      "Epoch 1/10, Batch Loss: 0.20223671197891235\n",
      "Epoch 1/10, Batch Loss: 0.5324379205703735\n",
      "Epoch 1/10, Batch Loss: 0.423141747713089\n",
      "Epoch 1/10, Batch Loss: 0.7242903113365173\n",
      "Epoch 1/10, Batch Loss: 0.7095022201538086\n",
      "Epoch 1/10, Batch Loss: 0.22086037695407867\n",
      "Epoch 1/10, Batch Loss: 0.23030415177345276\n",
      "Epoch 1/10, Batch Loss: 0.3042259216308594\n",
      "Epoch 1/10, Batch Loss: 0.3328237235546112\n",
      "Epoch 1/10, Batch Loss: 0.44161728024482727\n",
      "Epoch 1/10, Batch Loss: 0.3082570433616638\n",
      "Epoch 1/10, Batch Loss: 0.3218710124492645\n",
      "Epoch 1/10, Batch Loss: 0.3466615676879883\n",
      "Epoch 1/10, Batch Loss: 0.31115856766700745\n",
      "Epoch 1/10, Batch Loss: 0.33462053537368774\n",
      "Epoch 1/10, Batch Loss: 0.46928924322128296\n",
      "Epoch 1/10, Batch Loss: 0.1191815733909607\n",
      "Epoch 1/10, Batch Loss: 0.3617337942123413\n",
      "Epoch 1/10, Batch Loss: 0.5982342958450317\n",
      "Epoch 1/10, Batch Loss: 0.14181943237781525\n",
      "Epoch 1/10, Batch Loss: 0.5768846273422241\n",
      "Epoch 1/10, Batch Loss: 0.36430835723876953\n",
      "Epoch 1/10, Batch Loss: 0.26556140184402466\n",
      "Epoch 1/10, Batch Loss: 0.24855633080005646\n",
      "Epoch 1/10, Batch Loss: 0.3929872214794159\n",
      "Epoch 1/10, Batch Loss: 0.38374102115631104\n",
      "Epoch 1/10, Batch Loss: 0.22496239840984344\n",
      "Epoch 1/10, Batch Loss: 0.8275870680809021\n",
      "Epoch 1/10, Batch Loss: 0.20513440668582916\n",
      "Epoch 1/10, Batch Loss: 0.3556838929653168\n",
      "Epoch 1/10, Batch Loss: 0.35292595624923706\n",
      "Epoch 1/10, Batch Loss: 0.34963467717170715\n",
      "Epoch 1/10, Batch Loss: 0.3241909444332123\n",
      "Epoch 1/10, Batch Loss: 0.464605450630188\n",
      "Epoch 1/10, Batch Loss: 0.1653582751750946\n",
      "Epoch 1/10, Batch Loss: 0.35599154233932495\n",
      "Epoch 1/10, Batch Loss: 0.4284936487674713\n",
      "Epoch 1/10, Batch Loss: 0.2335204780101776\n",
      "Epoch 1/10, Batch Loss: 0.34274035692214966\n",
      "Epoch 1/10, Batch Loss: 0.35909464955329895\n",
      "Epoch 1/10, Batch Loss: 0.35048580169677734\n",
      "Epoch 1/10, Batch Loss: 0.21277354657649994\n",
      "Epoch 1/10, Batch Loss: 0.4650769829750061\n",
      "Epoch 1/10, Batch Loss: 0.09989911317825317\n",
      "Epoch 1/10, Batch Loss: 0.485942006111145\n",
      "Epoch 1/10, Batch Loss: 0.2368726134300232\n",
      "Epoch 1/10, Batch Loss: 0.5747111439704895\n",
      "Epoch 1/10, Batch Loss: 0.4985625445842743\n",
      "Epoch 1/10, Batch Loss: 0.18601015210151672\n",
      "Epoch 1/10, Batch Loss: 0.268849641084671\n",
      "Epoch 1/10, Batch Loss: 0.39458614587783813\n",
      "Epoch 1/10, Batch Loss: 0.6274772882461548\n",
      "Epoch 1/10, Batch Loss: 0.2445439249277115\n",
      "Epoch 1/10, Batch Loss: 0.1413523107767105\n",
      "Epoch 1/10, Batch Loss: 0.35642170906066895\n",
      "Epoch 1/10, Batch Loss: 0.3593018054962158\n",
      "Epoch 1/10, Batch Loss: 0.2581087052822113\n",
      "Epoch 1/10, Batch Loss: 0.21839730441570282\n",
      "Epoch 1/10, Batch Loss: 0.34384387731552124\n",
      "Epoch 1/10, Batch Loss: 0.4066493809223175\n",
      "Epoch 1/10, Batch Loss: 0.641109049320221\n",
      "Epoch 1/10, Batch Loss: 0.11136047542095184\n",
      "Epoch 1/10, Batch Loss: 0.26017916202545166\n",
      "Epoch 1/10, Batch Loss: 0.3575390577316284\n",
      "Epoch 1/10, Batch Loss: 0.1162833571434021\n",
      "Epoch 1/10, Batch Loss: 0.1105249673128128\n",
      "Epoch 1/10, Batch Loss: 0.36099544167518616\n",
      "Epoch 1/10, Batch Loss: 0.43991708755493164\n",
      "Epoch 1/10, Batch Loss: 0.5012636184692383\n",
      "Epoch 1/10, Batch Loss: 0.3399847745895386\n",
      "Epoch 1/10, Batch Loss: 0.3641906976699829\n",
      "Epoch 1/10, Batch Loss: 0.21896736323833466\n",
      "Epoch 1/10, Batch Loss: 0.10599233955144882\n",
      "Epoch 1/10, Batch Loss: 0.33559858798980713\n",
      "Epoch 1/10, Batch Loss: 0.22898399829864502\n",
      "Epoch 1/10, Batch Loss: 0.09731795638799667\n",
      "Epoch 1/10, Batch Loss: 0.36544445157051086\n",
      "Epoch 1/10, Batch Loss: 0.6571219563484192\n",
      "Epoch 1/10, Batch Loss: 0.25271883606910706\n",
      "Epoch 1/10, Batch Loss: 0.46244093775749207\n",
      "Epoch 1/10, Batch Loss: 0.21580366790294647\n",
      "Epoch 1/10, Batch Loss: 0.23519392311573029\n",
      "Epoch 1/10, Batch Loss: 0.3318277597427368\n",
      "Epoch 1/10, Batch Loss: 0.21519622206687927\n",
      "Epoch 1/10, Batch Loss: 0.3618597686290741\n",
      "Epoch 1/10, Batch Loss: 0.3927233815193176\n",
      "Epoch 1/10, Batch Loss: 0.06414121389389038\n",
      "Epoch 1/10, Batch Loss: 0.3422043025493622\n",
      "Epoch 1/10, Batch Loss: 0.24157901108264923\n",
      "Epoch 1/10, Batch Loss: 0.23899096250534058\n",
      "Epoch 1/10, Batch Loss: 0.49497702717781067\n",
      "Epoch 1/10, Batch Loss: 0.6360167860984802\n",
      "Epoch 1/10, Batch Loss: 0.629438579082489\n",
      "Epoch 1/10, Batch Loss: 0.24445392191410065\n",
      "Epoch 1/10, Batch Loss: 0.37031713128089905\n",
      "Epoch 1/10, Batch Loss: 0.11246193200349808\n",
      "Epoch 1/10, Batch Loss: 0.23979495465755463\n",
      "Epoch 1/10, Batch Loss: 0.5795654058456421\n",
      "Epoch 1/10, Batch Loss: 0.21382537484169006\n",
      "Epoch 1/10, Batch Loss: 0.5023889541625977\n",
      "Epoch 1/10, Batch Loss: 0.23864315450191498\n",
      "Epoch 1/10, Batch Loss: 0.2612670063972473\n",
      "Epoch 1/10, Batch Loss: 0.2535845637321472\n",
      "Epoch 1/10, Batch Loss: 0.6628175973892212\n",
      "Epoch 1/10, Batch Loss: 0.24533601105213165\n",
      "Epoch 1/10, Batch Loss: 0.24694521725177765\n",
      "Epoch 1/10, Batch Loss: 0.41927027702331543\n",
      "Epoch 1/10, Batch Loss: 0.23754338920116425\n",
      "Epoch 1/10, Batch Loss: 0.33773374557495117\n",
      "Epoch 1/10, Batch Loss: 0.1325462907552719\n",
      "Epoch 1/10, Batch Loss: 0.2569158375263214\n",
      "Epoch 1/10, Batch Loss: 0.2467106282711029\n",
      "Epoch 1/10, Batch Loss: 0.48986855149269104\n",
      "Epoch 1/10, Batch Loss: 0.36028599739074707\n",
      "Epoch 1/10, Batch Loss: 0.09783217310905457\n",
      "Epoch 1/10, Batch Loss: 0.43917426466941833\n",
      "Epoch 1/10, Batch Loss: 0.14796358346939087\n",
      "Epoch 1/10, Batch Loss: 0.21885421872138977\n",
      "Epoch 1/10, Batch Loss: 0.1973344385623932\n",
      "Epoch 1/10, Batch Loss: 0.6075682044029236\n",
      "Epoch 1/10, Batch Loss: 0.09043429791927338\n",
      "Epoch 1/10, Batch Loss: 0.8028015494346619\n",
      "Epoch 1/10, Batch Loss: 0.3638853430747986\n",
      "Epoch 1/10, Batch Loss: 0.4900078773498535\n",
      "Epoch 1/10, Batch Loss: 0.3611117899417877\n",
      "Epoch 1/10, Batch Loss: 0.31472036242485046\n",
      "Epoch 1/10, Batch Loss: 0.353300005197525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch Loss: 0.3216312825679779\n",
      "Epoch 1/10, Batch Loss: 0.12798450887203217\n",
      "Epoch 1/10, Batch Loss: 0.3631243109703064\n",
      "Epoch 1/10, Batch Loss: 0.12048667669296265\n",
      "Epoch 1/10, Batch Loss: 0.36682096123695374\n",
      "Epoch 1/10, Batch Loss: 0.2732905149459839\n",
      "Epoch 1/10, Batch Loss: 0.12411116808652878\n",
      "Epoch 1/10, Batch Loss: 0.3483610153198242\n",
      "Epoch 1/10, Batch Loss: 0.11404358595609665\n",
      "Epoch 1/10, Batch Loss: 0.37447044253349304\n",
      "Epoch 1/10, Batch Loss: 0.3435271978378296\n",
      "Epoch 1/10, Batch Loss: 0.3668426275253296\n",
      "Epoch 1/10, Batch Loss: 0.22380492091178894\n",
      "Epoch 1/10, Batch Loss: 0.22250209748744965\n",
      "Epoch 1/10, Batch Loss: 0.5060283541679382\n",
      "Epoch 1/10, Batch Loss: 0.07139237225055695\n",
      "Epoch 1/10, Batch Loss: 0.23438046872615814\n",
      "Epoch 1/10, Batch Loss: 0.1977069079875946\n",
      "Epoch 1/10, Batch Loss: 0.5009791851043701\n",
      "Epoch 1/10, Batch Loss: 0.23570337891578674\n",
      "Epoch 1/10, Batch Loss: 0.19854247570037842\n",
      "Epoch 1/10, Batch Loss: 0.24289563298225403\n",
      "Epoch 1/10, Batch Loss: 0.2146228700876236\n",
      "Epoch 1/10, Batch Loss: 0.3357573449611664\n",
      "Epoch 1/10, Batch Loss: 0.22162742912769318\n",
      "Epoch 1/10, Batch Loss: 0.07950625568628311\n",
      "Epoch 1/10, Batch Loss: 0.22787241637706757\n",
      "Epoch 1/10, Batch Loss: 0.3662593960762024\n",
      "Epoch 1/10, Batch Loss: 0.5160136222839355\n",
      "Epoch 1/10, Batch Loss: 0.08959348499774933\n",
      "Epoch 1/10, Batch Loss: 0.1965586543083191\n",
      "Epoch 1/10, Batch Loss: 0.5339117050170898\n",
      "Epoch 1/10, Batch Loss: 0.34792324900627136\n",
      "Epoch 1/10, Batch Loss: 0.08456359803676605\n",
      "Epoch 1/10, Batch Loss: 0.2250850647687912\n",
      "Epoch 1/10, Batch Loss: 0.09142006933689117\n",
      "Epoch 1/10, Batch Loss: 0.36129486560821533\n",
      "Epoch 1/10, Batch Loss: 0.474942147731781\n",
      "Epoch 1/10, Batch Loss: 0.3535715341567993\n",
      "Epoch 1/10, Batch Loss: 0.7962237596511841\n",
      "Epoch 1/10, Batch Loss: 0.41767552495002747\n",
      "Epoch 1/10, Batch Loss: 0.22543048858642578\n",
      "Epoch 1/10, Batch Loss: 0.09535492211580276\n",
      "Epoch 1/10, Batch Loss: 0.2345936894416809\n",
      "Epoch 1/10, Batch Loss: 0.5936766862869263\n",
      "Epoch 1/10, Batch Loss: 0.3326801061630249\n",
      "Epoch 1/10, Batch Loss: 0.44025498628616333\n",
      "Epoch 1/10, Batch Loss: 0.17763501405715942\n",
      "Epoch 1/10, Average Training Loss: 0.36224721198980236\n",
      "Model saved for epoch 1 at Pre_train_epoch1.pt\n",
      "Validation Accuracy for epoch 1: 0.8841789052069426\n",
      "Epoch 2/10, Batch Loss: 0.24855847656726837\n",
      "Epoch 2/10, Batch Loss: 0.4760635197162628\n",
      "Epoch 2/10, Batch Loss: 0.18677540123462677\n",
      "Epoch 2/10, Batch Loss: 0.36352822184562683\n",
      "Epoch 2/10, Batch Loss: 0.3429190218448639\n",
      "Epoch 2/10, Batch Loss: 0.15208198130130768\n",
      "Epoch 2/10, Batch Loss: 0.4989055395126343\n",
      "Epoch 2/10, Batch Loss: 0.19574417173862457\n",
      "Epoch 2/10, Batch Loss: 0.5062595009803772\n",
      "Epoch 2/10, Batch Loss: 0.09323901683092117\n",
      "Epoch 2/10, Batch Loss: 0.622012734413147\n",
      "Epoch 2/10, Batch Loss: 0.5065792202949524\n",
      "Epoch 2/10, Batch Loss: 0.20305219292640686\n",
      "Epoch 2/10, Batch Loss: 0.11470582336187363\n",
      "Epoch 2/10, Batch Loss: 0.3636578619480133\n",
      "Epoch 2/10, Batch Loss: 0.20663627982139587\n",
      "Epoch 2/10, Batch Loss: 0.3852660357952118\n",
      "Epoch 2/10, Batch Loss: 0.3486018180847168\n",
      "Epoch 2/10, Batch Loss: 0.48731255531311035\n",
      "Epoch 2/10, Batch Loss: 0.33624210953712463\n",
      "Epoch 2/10, Batch Loss: 0.0747375413775444\n",
      "Epoch 2/10, Batch Loss: 0.46374884247779846\n",
      "Epoch 2/10, Batch Loss: 0.221400186419487\n",
      "Epoch 2/10, Batch Loss: 0.44141799211502075\n",
      "Epoch 2/10, Batch Loss: 0.3679479956626892\n",
      "Epoch 2/10, Batch Loss: 0.11907671391963959\n",
      "Epoch 2/10, Batch Loss: 0.4699211120605469\n",
      "Epoch 2/10, Batch Loss: 0.22491584718227386\n",
      "Epoch 2/10, Batch Loss: 0.47861212491989136\n",
      "Epoch 2/10, Batch Loss: 0.10981369018554688\n",
      "Epoch 2/10, Batch Loss: 0.640953779220581\n",
      "Epoch 2/10, Batch Loss: 0.49233585596084595\n",
      "Epoch 2/10, Batch Loss: 0.10751178860664368\n",
      "Epoch 2/10, Batch Loss: 0.4501999616622925\n",
      "Epoch 2/10, Batch Loss: 0.23598630726337433\n",
      "Epoch 2/10, Batch Loss: 0.3678668141365051\n",
      "Epoch 2/10, Batch Loss: 0.22323834896087646\n",
      "Epoch 2/10, Batch Loss: 0.11554620414972305\n",
      "Epoch 2/10, Batch Loss: 0.4709084630012512\n",
      "Epoch 2/10, Batch Loss: 0.12733881175518036\n",
      "Epoch 2/10, Batch Loss: 0.33420130610466003\n",
      "Epoch 2/10, Batch Loss: 0.24512556195259094\n",
      "Epoch 2/10, Batch Loss: 0.21306556463241577\n",
      "Epoch 2/10, Batch Loss: 0.2592492401599884\n",
      "Epoch 2/10, Batch Loss: 0.35837215185165405\n",
      "Epoch 2/10, Batch Loss: 0.08167798817157745\n",
      "Epoch 2/10, Batch Loss: 0.3542044162750244\n",
      "Epoch 2/10, Batch Loss: 0.23105575144290924\n",
      "Epoch 2/10, Batch Loss: 0.5216056108474731\n",
      "Epoch 2/10, Batch Loss: 0.21002455055713654\n",
      "Epoch 2/10, Batch Loss: 0.3547346293926239\n",
      "Epoch 2/10, Batch Loss: 0.2292104810476303\n",
      "Epoch 2/10, Batch Loss: 0.19045966863632202\n",
      "Epoch 2/10, Batch Loss: 0.21119742095470428\n",
      "Epoch 2/10, Batch Loss: 0.5105458498001099\n",
      "Epoch 2/10, Batch Loss: 0.33423399925231934\n",
      "Epoch 2/10, Batch Loss: 0.40353187918663025\n",
      "Epoch 2/10, Batch Loss: 0.2100578099489212\n",
      "Epoch 2/10, Batch Loss: 0.33792227506637573\n",
      "Epoch 2/10, Batch Loss: 0.42732128500938416\n",
      "Epoch 2/10, Batch Loss: 0.4519938826560974\n",
      "Epoch 2/10, Batch Loss: 0.12903112173080444\n",
      "Epoch 2/10, Batch Loss: 0.25541171431541443\n",
      "Epoch 2/10, Batch Loss: 0.25169843435287476\n",
      "Epoch 2/10, Batch Loss: 0.5840396285057068\n",
      "Epoch 2/10, Batch Loss: 0.2398366928100586\n",
      "Epoch 2/10, Batch Loss: 0.46286913752555847\n",
      "Epoch 2/10, Batch Loss: 0.3304831385612488\n",
      "Epoch 2/10, Batch Loss: 0.11017277091741562\n",
      "Epoch 2/10, Batch Loss: 0.2397446185350418\n",
      "Epoch 2/10, Batch Loss: 0.08092809468507767\n",
      "Epoch 2/10, Batch Loss: 0.24204276502132416\n",
      "Epoch 2/10, Batch Loss: 0.35744476318359375\n",
      "Epoch 2/10, Batch Loss: 0.3485545217990875\n",
      "Epoch 2/10, Batch Loss: 0.2551193833351135\n",
      "Epoch 2/10, Batch Loss: 0.3753570318222046\n",
      "Epoch 2/10, Batch Loss: 0.2255014032125473\n",
      "Epoch 2/10, Batch Loss: 0.2284853607416153\n",
      "Epoch 2/10, Batch Loss: 0.21001800894737244\n",
      "Epoch 2/10, Batch Loss: 0.23624131083488464\n",
      "Epoch 2/10, Batch Loss: 0.20435215532779694\n",
      "Epoch 2/10, Batch Loss: 0.3326398730278015\n",
      "Epoch 2/10, Batch Loss: 0.33189961314201355\n",
      "Epoch 2/10, Batch Loss: 0.2688327729701996\n",
      "Epoch 2/10, Batch Loss: 0.2082470953464508\n",
      "Epoch 2/10, Batch Loss: 0.6724390387535095\n",
      "Epoch 2/10, Batch Loss: 0.21529681980609894\n",
      "Epoch 2/10, Batch Loss: 0.06623563915491104\n",
      "Epoch 2/10, Batch Loss: 0.6052150130271912\n",
      "Epoch 2/10, Batch Loss: 0.07016026228666306\n",
      "Epoch 2/10, Batch Loss: 0.5728643536567688\n",
      "Epoch 2/10, Batch Loss: 0.2099904716014862\n",
      "Epoch 2/10, Batch Loss: 0.38931339979171753\n",
      "Epoch 2/10, Batch Loss: 0.306399941444397\n",
      "Epoch 2/10, Batch Loss: 0.3502107262611389\n",
      "Epoch 2/10, Batch Loss: 0.5964072942733765\n",
      "Epoch 2/10, Batch Loss: 0.10357378423213959\n",
      "Epoch 2/10, Batch Loss: 0.5174372792243958\n",
      "Epoch 2/10, Batch Loss: 0.21365152299404144\n",
      "Epoch 2/10, Batch Loss: 0.36435699462890625\n",
      "Epoch 2/10, Batch Loss: 0.3009810447692871\n",
      "Epoch 2/10, Batch Loss: 0.3341156244277954\n",
      "Epoch 2/10, Batch Loss: 0.1617020219564438\n",
      "Epoch 2/10, Batch Loss: 0.2489364594221115\n",
      "Epoch 2/10, Batch Loss: 0.33721548318862915\n",
      "Epoch 2/10, Batch Loss: 0.3443472385406494\n",
      "Epoch 2/10, Batch Loss: 0.14147669076919556\n",
      "Epoch 2/10, Batch Loss: 0.34850189089775085\n",
      "Epoch 2/10, Batch Loss: 0.3541657030582428\n",
      "Epoch 2/10, Batch Loss: 0.3517446517944336\n",
      "Epoch 2/10, Batch Loss: 0.31774353981018066\n",
      "Epoch 2/10, Batch Loss: 0.4703114628791809\n",
      "Epoch 2/10, Batch Loss: 0.22325432300567627\n",
      "Epoch 2/10, Batch Loss: 0.24161571264266968\n",
      "Epoch 2/10, Batch Loss: 0.26058343052864075\n",
      "Epoch 2/10, Batch Loss: 0.24828007817268372\n",
      "Epoch 2/10, Batch Loss: 0.44755417108535767\n",
      "Epoch 2/10, Batch Loss: 0.828526496887207\n",
      "Epoch 2/10, Batch Loss: 0.32949069142341614\n",
      "Epoch 2/10, Batch Loss: 0.24069713056087494\n",
      "Epoch 2/10, Batch Loss: 0.22029787302017212\n",
      "Epoch 2/10, Batch Loss: 0.13948975503444672\n",
      "Epoch 2/10, Batch Loss: 0.1672513484954834\n",
      "Epoch 2/10, Batch Loss: 0.3422617018222809\n",
      "Epoch 2/10, Batch Loss: 0.1312716007232666\n",
      "Epoch 2/10, Batch Loss: 0.3402920961380005\n",
      "Epoch 2/10, Batch Loss: 0.3767355680465698\n",
      "Epoch 2/10, Batch Loss: 0.21953612565994263\n",
      "Epoch 2/10, Batch Loss: 0.23658910393714905\n",
      "Epoch 2/10, Batch Loss: 0.33759933710098267\n",
      "Epoch 2/10, Batch Loss: 0.07865120470523834\n",
      "Epoch 2/10, Batch Loss: 0.20895104110240936\n",
      "Epoch 2/10, Batch Loss: 0.4673023223876953\n",
      "Epoch 2/10, Batch Loss: 0.7560538649559021\n",
      "Epoch 2/10, Batch Loss: 0.37710267305374146\n",
      "Epoch 2/10, Batch Loss: 0.35727986693382263\n",
      "Epoch 2/10, Batch Loss: 0.6699422001838684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Batch Loss: 0.36476776003837585\n",
      "Epoch 2/10, Batch Loss: 0.11052891612052917\n",
      "Epoch 2/10, Batch Loss: 0.31898367404937744\n",
      "Epoch 2/10, Batch Loss: 0.1192537322640419\n",
      "Epoch 2/10, Batch Loss: 0.2137306183576584\n",
      "Epoch 2/10, Batch Loss: 0.4418972134590149\n",
      "Epoch 2/10, Batch Loss: 0.22884273529052734\n",
      "Epoch 2/10, Batch Loss: 0.5592710971832275\n",
      "Epoch 2/10, Batch Loss: 0.23405513167381287\n",
      "Epoch 2/10, Batch Loss: 0.36403146386146545\n",
      "Epoch 2/10, Batch Loss: 0.26686733961105347\n",
      "Epoch 2/10, Batch Loss: 0.10722211748361588\n",
      "Epoch 2/10, Batch Loss: 0.11102891713380814\n",
      "Epoch 2/10, Batch Loss: 0.33663076162338257\n",
      "Epoch 2/10, Batch Loss: 0.24196112155914307\n",
      "Epoch 2/10, Batch Loss: 0.245708167552948\n",
      "Epoch 2/10, Batch Loss: 0.5717613697052002\n",
      "Epoch 2/10, Batch Loss: 0.3553922772407532\n",
      "Epoch 2/10, Batch Loss: 0.20737874507904053\n",
      "Epoch 2/10, Batch Loss: 0.7981641888618469\n",
      "Epoch 2/10, Batch Loss: 0.43138930201530457\n",
      "Epoch 2/10, Batch Loss: 0.20272313058376312\n",
      "Epoch 2/10, Batch Loss: 0.21220193803310394\n",
      "Epoch 2/10, Batch Loss: 0.23048357665538788\n",
      "Epoch 2/10, Batch Loss: 0.331816703081131\n",
      "Epoch 2/10, Batch Loss: 0.1006939560174942\n",
      "Epoch 2/10, Batch Loss: 0.22608324885368347\n",
      "Epoch 2/10, Batch Loss: 0.22929494082927704\n",
      "Epoch 2/10, Batch Loss: 0.3500053584575653\n",
      "Epoch 2/10, Batch Loss: 0.41014549136161804\n",
      "Epoch 2/10, Batch Loss: 0.3729536235332489\n",
      "Epoch 2/10, Batch Loss: 0.32304316759109497\n",
      "Epoch 2/10, Batch Loss: 0.5678154826164246\n",
      "Epoch 2/10, Batch Loss: 0.3851841390132904\n",
      "Epoch 2/10, Batch Loss: 0.3359481394290924\n",
      "Epoch 2/10, Batch Loss: 0.13399344682693481\n",
      "Epoch 2/10, Batch Loss: 0.4511721432209015\n",
      "Epoch 2/10, Batch Loss: 0.337647944688797\n",
      "Epoch 2/10, Batch Loss: 0.33485209941864014\n",
      "Epoch 2/10, Batch Loss: 0.3328358232975006\n",
      "Epoch 2/10, Batch Loss: 0.21795715391635895\n",
      "Epoch 2/10, Batch Loss: 0.2181275188922882\n",
      "Epoch 2/10, Batch Loss: 0.22570115327835083\n",
      "Epoch 2/10, Batch Loss: 0.24187184870243073\n",
      "Epoch 2/10, Batch Loss: 0.24443678557872772\n",
      "Epoch 2/10, Batch Loss: 0.5710828900337219\n",
      "Epoch 2/10, Batch Loss: 0.31110504269599915\n",
      "Epoch 2/10, Batch Loss: 0.23481354117393494\n",
      "Epoch 2/10, Batch Loss: 0.4933781623840332\n",
      "Epoch 2/10, Batch Loss: 0.24974381923675537\n",
      "Epoch 2/10, Batch Loss: 0.2641817629337311\n",
      "Epoch 2/10, Batch Loss: 0.1371477246284485\n",
      "Epoch 2/10, Batch Loss: 0.1972300112247467\n",
      "Epoch 2/10, Batch Loss: 0.2424500286579132\n",
      "Epoch 2/10, Batch Loss: 0.22750301659107208\n",
      "Epoch 2/10, Batch Loss: 0.7041780352592468\n",
      "Epoch 2/10, Batch Loss: 0.3237459361553192\n",
      "Epoch 2/10, Batch Loss: 0.2526938319206238\n",
      "Epoch 2/10, Batch Loss: 0.5960180163383484\n",
      "Epoch 2/10, Batch Loss: 0.33615052700042725\n",
      "Epoch 2/10, Batch Loss: 0.09908606112003326\n",
      "Epoch 2/10, Batch Loss: 0.4873177111148834\n",
      "Epoch 2/10, Batch Loss: 0.45216506719589233\n",
      "Epoch 2/10, Batch Loss: 0.2498190701007843\n",
      "Epoch 2/10, Batch Loss: 0.5298170447349548\n",
      "Epoch 2/10, Batch Loss: 0.3222830593585968\n",
      "Epoch 2/10, Batch Loss: 0.3434719741344452\n",
      "Epoch 2/10, Batch Loss: 0.33975231647491455\n",
      "Epoch 2/10, Batch Loss: 0.11201325058937073\n",
      "Epoch 2/10, Batch Loss: 0.41810503602027893\n",
      "Epoch 2/10, Batch Loss: 0.2530128061771393\n",
      "Epoch 2/10, Batch Loss: 0.1443648785352707\n",
      "Epoch 2/10, Batch Loss: 0.3186233937740326\n",
      "Epoch 2/10, Batch Loss: 0.24835987389087677\n",
      "Epoch 2/10, Batch Loss: 0.5589029788970947\n",
      "Epoch 2/10, Batch Loss: 0.2368437647819519\n",
      "Epoch 2/10, Batch Loss: 0.3179624080657959\n",
      "Epoch 2/10, Batch Loss: 0.2200227826833725\n",
      "Epoch 2/10, Batch Loss: 0.0688583105802536\n",
      "Epoch 2/10, Batch Loss: 0.3019322454929352\n",
      "Epoch 2/10, Batch Loss: 0.5937851071357727\n",
      "Epoch 2/10, Batch Loss: 0.3426710367202759\n",
      "Epoch 2/10, Batch Loss: 0.31236883997917175\n",
      "Epoch 2/10, Batch Loss: 0.32858559489250183\n",
      "Epoch 2/10, Batch Loss: 0.46118542551994324\n",
      "Epoch 2/10, Batch Loss: 0.4543256163597107\n",
      "Epoch 2/10, Batch Loss: 0.4669983685016632\n",
      "Epoch 2/10, Batch Loss: 0.22116144001483917\n",
      "Epoch 2/10, Batch Loss: 0.2036968320608139\n",
      "Epoch 2/10, Batch Loss: 0.13484592735767365\n",
      "Epoch 2/10, Batch Loss: 0.15131883323192596\n",
      "Epoch 2/10, Batch Loss: 0.25274112820625305\n",
      "Epoch 2/10, Batch Loss: 0.36228427290916443\n",
      "Epoch 2/10, Batch Loss: 0.24618571996688843\n",
      "Epoch 2/10, Batch Loss: 0.32538822293281555\n",
      "Epoch 2/10, Batch Loss: 0.3162337839603424\n",
      "Epoch 2/10, Batch Loss: 0.21110302209854126\n",
      "Epoch 2/10, Batch Loss: 0.34233787655830383\n",
      "Epoch 2/10, Batch Loss: 0.5202637314796448\n",
      "Epoch 2/10, Batch Loss: 0.09094557166099548\n",
      "Epoch 2/10, Batch Loss: 0.21607784926891327\n",
      "Epoch 2/10, Batch Loss: 0.47421565651893616\n",
      "Epoch 2/10, Batch Loss: 0.09071885794401169\n",
      "Epoch 2/10, Batch Loss: 0.18722349405288696\n",
      "Epoch 2/10, Batch Loss: 0.4635215401649475\n",
      "Epoch 2/10, Batch Loss: 0.20872488617897034\n",
      "Epoch 2/10, Batch Loss: 0.21089477837085724\n",
      "Epoch 2/10, Batch Loss: 0.3566516637802124\n",
      "Epoch 2/10, Batch Loss: 0.2157064974308014\n",
      "Epoch 2/10, Batch Loss: 0.5301244258880615\n",
      "Epoch 2/10, Batch Loss: 0.36181411147117615\n",
      "Epoch 2/10, Batch Loss: 0.5122557878494263\n",
      "Epoch 2/10, Batch Loss: 0.3280903100967407\n",
      "Epoch 2/10, Batch Loss: 0.19735024869441986\n",
      "Epoch 2/10, Batch Loss: 0.10613462328910828\n",
      "Epoch 2/10, Batch Loss: 0.41085052490234375\n",
      "Epoch 2/10, Batch Loss: 0.2126564234495163\n",
      "Epoch 2/10, Batch Loss: 0.12132236361503601\n",
      "Epoch 2/10, Batch Loss: 0.23878081142902374\n",
      "Epoch 2/10, Batch Loss: 0.4321484863758087\n",
      "Epoch 2/10, Batch Loss: 0.12801535427570343\n",
      "Epoch 2/10, Batch Loss: 0.1539456695318222\n",
      "Epoch 2/10, Batch Loss: 0.1837209016084671\n",
      "Epoch 2/10, Batch Loss: 0.44525524973869324\n",
      "Epoch 2/10, Batch Loss: 0.12672379612922668\n",
      "Epoch 2/10, Batch Loss: 0.3650882840156555\n",
      "Epoch 2/10, Batch Loss: 0.45761263370513916\n",
      "Epoch 2/10, Batch Loss: 0.3438510000705719\n",
      "Epoch 2/10, Batch Loss: 0.09424625337123871\n",
      "Epoch 2/10, Batch Loss: 0.4869344234466553\n",
      "Epoch 2/10, Batch Loss: 0.19401191174983978\n",
      "Epoch 2/10, Batch Loss: 0.3826790452003479\n",
      "Epoch 2/10, Batch Loss: 0.573816180229187\n",
      "Epoch 2/10, Batch Loss: 0.22928202152252197\n",
      "Epoch 2/10, Batch Loss: 0.32165220379829407\n",
      "Epoch 2/10, Batch Loss: 0.19498716294765472\n",
      "Epoch 2/10, Batch Loss: 0.5672204494476318\n",
      "Epoch 2/10, Batch Loss: 0.45234474539756775\n",
      "Epoch 2/10, Batch Loss: 0.33757951855659485\n",
      "Epoch 2/10, Batch Loss: 0.2326759397983551\n",
      "Epoch 2/10, Batch Loss: 0.4606941044330597\n",
      "Epoch 2/10, Batch Loss: 0.3551918566226959\n",
      "Epoch 2/10, Batch Loss: 0.3203815817832947\n",
      "Epoch 2/10, Batch Loss: 0.5038506984710693\n",
      "Epoch 2/10, Batch Loss: 0.1479991376399994\n",
      "Epoch 2/10, Batch Loss: 0.14181020855903625\n",
      "Epoch 2/10, Batch Loss: 0.31540459394454956\n",
      "Epoch 2/10, Batch Loss: 0.20602014660835266\n",
      "Epoch 2/10, Batch Loss: 0.31437698006629944\n",
      "Epoch 2/10, Batch Loss: 0.27670761942863464\n",
      "Epoch 2/10, Batch Loss: 0.3500620424747467\n",
      "Epoch 2/10, Batch Loss: 0.42332276701927185\n",
      "Epoch 2/10, Batch Loss: 0.35731640458106995\n",
      "Epoch 2/10, Batch Loss: 0.28869250416755676\n",
      "Epoch 2/10, Batch Loss: 0.671978235244751\n",
      "Epoch 2/10, Batch Loss: 0.22126297652721405\n",
      "Epoch 2/10, Batch Loss: 0.31760549545288086\n",
      "Epoch 2/10, Batch Loss: 0.3288206160068512\n",
      "Epoch 2/10, Batch Loss: 0.19987158477306366\n",
      "Epoch 2/10, Batch Loss: 0.1536789834499359\n",
      "Epoch 2/10, Batch Loss: 0.16471675038337708\n",
      "Epoch 2/10, Batch Loss: 0.23063935339450836\n",
      "Epoch 2/10, Batch Loss: 0.3363354504108429\n",
      "Epoch 2/10, Batch Loss: 0.44379934668540955\n",
      "Epoch 2/10, Batch Loss: 0.34366846084594727\n",
      "Epoch 2/10, Batch Loss: 0.3414251506328583\n",
      "Epoch 2/10, Batch Loss: 0.10369373857975006\n",
      "Epoch 2/10, Batch Loss: 0.4592435359954834\n",
      "Epoch 2/10, Batch Loss: 0.3543562889099121\n",
      "Epoch 2/10, Batch Loss: 0.3414222300052643\n",
      "Epoch 2/10, Batch Loss: 0.21767757833003998\n",
      "Epoch 2/10, Batch Loss: 0.5929639339447021\n",
      "Epoch 2/10, Batch Loss: 0.6301674246788025\n",
      "Epoch 2/10, Batch Loss: 0.2054210901260376\n",
      "Epoch 2/10, Batch Loss: 0.2015119045972824\n",
      "Epoch 2/10, Batch Loss: 0.34395962953567505\n",
      "Epoch 2/10, Batch Loss: 0.5562976002693176\n",
      "Epoch 2/10, Batch Loss: 0.10239820182323456\n",
      "Epoch 2/10, Batch Loss: 0.24323700368404388\n",
      "Epoch 2/10, Batch Loss: 0.4052351415157318\n",
      "Epoch 2/10, Batch Loss: 0.09329434484243393\n",
      "Epoch 2/10, Batch Loss: 0.44447481632232666\n",
      "Epoch 2/10, Batch Loss: 0.2780131697654724\n",
      "Epoch 2/10, Batch Loss: 0.2648346424102783\n",
      "Epoch 2/10, Batch Loss: 0.4721197485923767\n",
      "Epoch 2/10, Batch Loss: 0.5001651048660278\n",
      "Epoch 2/10, Batch Loss: 0.2763907313346863\n",
      "Epoch 2/10, Batch Loss: 0.3493381440639496\n",
      "Epoch 2/10, Batch Loss: 0.2431420534849167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Batch Loss: 0.11468043923377991\n",
      "Epoch 2/10, Batch Loss: 0.11218635737895966\n",
      "Epoch 2/10, Batch Loss: 0.207257479429245\n",
      "Epoch 2/10, Batch Loss: 0.5816338062286377\n",
      "Epoch 2/10, Batch Loss: 0.548229455947876\n",
      "Epoch 2/10, Batch Loss: 0.24304263293743134\n",
      "Epoch 2/10, Batch Loss: 0.34932634234428406\n",
      "Epoch 2/10, Batch Loss: 0.2301732897758484\n",
      "Epoch 2/10, Batch Loss: 0.4059648811817169\n",
      "Epoch 2/10, Batch Loss: 0.2898111343383789\n",
      "Epoch 2/10, Batch Loss: 0.24344737827777863\n",
      "Epoch 2/10, Batch Loss: 0.18425241112709045\n",
      "Epoch 2/10, Batch Loss: 0.440733402967453\n",
      "Epoch 2/10, Batch Loss: 0.24318745732307434\n",
      "Epoch 2/10, Batch Loss: 0.48604199290275574\n",
      "Epoch 2/10, Batch Loss: 0.21875350177288055\n",
      "Epoch 2/10, Batch Loss: 0.6664556264877319\n",
      "Epoch 2/10, Batch Loss: 0.19494062662124634\n",
      "Epoch 2/10, Batch Loss: 0.14270660281181335\n",
      "Epoch 2/10, Batch Loss: 0.24428825080394745\n",
      "Epoch 2/10, Batch Loss: 0.22593754529953003\n",
      "Epoch 2/10, Batch Loss: 0.7167632579803467\n",
      "Epoch 2/10, Batch Loss: 0.5565463900566101\n",
      "Epoch 2/10, Batch Loss: 0.33934929966926575\n",
      "Epoch 2/10, Batch Loss: 0.22324389219284058\n",
      "Epoch 2/10, Batch Loss: 0.3605595529079437\n",
      "Epoch 2/10, Batch Loss: 0.2459232211112976\n",
      "Epoch 2/10, Batch Loss: 0.294036865234375\n",
      "Epoch 2/10, Batch Loss: 0.2116449624300003\n",
      "Epoch 2/10, Batch Loss: 0.3518030047416687\n",
      "Epoch 2/10, Batch Loss: 0.27014949917793274\n",
      "Epoch 2/10, Batch Loss: 0.23895391821861267\n",
      "Epoch 2/10, Batch Loss: 0.4024328589439392\n",
      "Epoch 2/10, Batch Loss: 0.2511000633239746\n",
      "Epoch 2/10, Batch Loss: 0.3244650065898895\n",
      "Epoch 2/10, Batch Loss: 0.26590287685394287\n",
      "Epoch 2/10, Batch Loss: 0.3234160542488098\n",
      "Epoch 2/10, Batch Loss: 0.23471496999263763\n",
      "Epoch 2/10, Batch Loss: 0.12941348552703857\n",
      "Epoch 2/10, Batch Loss: 0.33596906065940857\n",
      "Epoch 2/10, Batch Loss: 0.36689886450767517\n",
      "Epoch 2/10, Batch Loss: 0.31206369400024414\n",
      "Epoch 2/10, Batch Loss: 0.4469023048877716\n",
      "Epoch 2/10, Batch Loss: 0.22703048586845398\n",
      "Epoch 2/10, Batch Loss: 0.11395355314016342\n",
      "Epoch 2/10, Batch Loss: 0.49570104479789734\n",
      "Epoch 2/10, Batch Loss: 0.4874742925167084\n",
      "Epoch 2/10, Batch Loss: 0.4438961148262024\n",
      "Epoch 2/10, Batch Loss: 0.09870804101228714\n",
      "Epoch 2/10, Batch Loss: 0.4016043245792389\n",
      "Epoch 2/10, Batch Loss: 0.2132662683725357\n",
      "Epoch 2/10, Batch Loss: 0.4268815219402313\n",
      "Epoch 2/10, Batch Loss: 0.2469855695962906\n",
      "Epoch 2/10, Batch Loss: 0.5457319617271423\n",
      "Epoch 2/10, Batch Loss: 0.24792444705963135\n",
      "Epoch 2/10, Batch Loss: 0.17552411556243896\n",
      "Epoch 2/10, Batch Loss: 0.20975176990032196\n",
      "Epoch 2/10, Batch Loss: 0.20637661218643188\n",
      "Epoch 2/10, Batch Loss: 0.23557841777801514\n",
      "Epoch 2/10, Batch Loss: 0.21334613859653473\n",
      "Epoch 2/10, Batch Loss: 0.2263212352991104\n",
      "Epoch 2/10, Batch Loss: 0.3514406681060791\n",
      "Epoch 2/10, Batch Loss: 0.7134358286857605\n",
      "Epoch 2/10, Batch Loss: 0.314914345741272\n",
      "Epoch 2/10, Batch Loss: 0.0923871397972107\n",
      "Epoch 2/10, Batch Loss: 0.5580043792724609\n",
      "Epoch 2/10, Batch Loss: 0.4420457184314728\n",
      "Epoch 2/10, Batch Loss: 0.4887668490409851\n",
      "Epoch 2/10, Batch Loss: 0.31194359064102173\n",
      "Epoch 2/10, Batch Loss: 0.36022573709487915\n",
      "Epoch 2/10, Batch Loss: 0.1002810150384903\n",
      "Epoch 2/10, Batch Loss: 0.34063971042633057\n",
      "Epoch 2/10, Batch Loss: 0.361014187335968\n",
      "Epoch 2/10, Batch Loss: 0.45679590106010437\n",
      "Epoch 2/10, Batch Loss: 0.32561782002449036\n",
      "Epoch 2/10, Batch Loss: 0.36321189999580383\n",
      "Epoch 2/10, Batch Loss: 0.3803929090499878\n",
      "Epoch 2/10, Batch Loss: 0.35445794463157654\n",
      "Epoch 2/10, Batch Loss: 0.42371103167533875\n",
      "Epoch 2/10, Batch Loss: 0.35531049966812134\n",
      "Epoch 2/10, Batch Loss: 0.34887591004371643\n",
      "Epoch 2/10, Batch Loss: 0.4360165297985077\n",
      "Epoch 2/10, Batch Loss: 0.34446507692337036\n",
      "Epoch 2/10, Batch Loss: 0.33465322852134705\n",
      "Epoch 2/10, Batch Loss: 0.42929038405418396\n",
      "Epoch 2/10, Batch Loss: 0.5507075190544128\n",
      "Epoch 2/10, Batch Loss: 0.17190545797348022\n",
      "Epoch 2/10, Batch Loss: 0.37225157022476196\n",
      "Epoch 2/10, Batch Loss: 0.5711004734039307\n",
      "Epoch 2/10, Batch Loss: 0.3956543803215027\n",
      "Epoch 2/10, Batch Loss: 0.410333514213562\n",
      "Epoch 2/10, Batch Loss: 0.5405043363571167\n",
      "Epoch 2/10, Batch Loss: 0.21363431215286255\n",
      "Epoch 2/10, Batch Loss: 0.36255258321762085\n",
      "Epoch 2/10, Batch Loss: 0.36898261308670044\n",
      "Epoch 2/10, Batch Loss: 0.5652958750724792\n",
      "Epoch 2/10, Batch Loss: 0.23495087027549744\n",
      "Epoch 2/10, Batch Loss: 0.1807389110326767\n",
      "Epoch 2/10, Batch Loss: 0.5284692645072937\n",
      "Epoch 2/10, Batch Loss: 0.43197521567344666\n",
      "Epoch 2/10, Average Training Loss: 0.31901501617591144\n",
      "Model saved for epoch 2 at Pre_train_epoch2.pt\n",
      "Validation Accuracy for epoch 2: 0.884929906542056\n",
      "Epoch 3/10, Batch Loss: 0.3882719874382019\n",
      "Epoch 3/10, Batch Loss: 0.24389471113681793\n",
      "Epoch 3/10, Batch Loss: 0.27649134397506714\n",
      "Epoch 3/10, Batch Loss: 0.23463404178619385\n",
      "Epoch 3/10, Batch Loss: 0.31438273191452026\n",
      "Epoch 3/10, Batch Loss: 0.25808292627334595\n",
      "Epoch 3/10, Batch Loss: 0.1831853687763214\n",
      "Epoch 3/10, Batch Loss: 0.5146315097808838\n",
      "Epoch 3/10, Batch Loss: 0.23180057108402252\n",
      "Epoch 3/10, Batch Loss: 0.43162912130355835\n",
      "Epoch 3/10, Batch Loss: 0.2444465458393097\n",
      "Epoch 3/10, Batch Loss: 0.32840514183044434\n",
      "Epoch 3/10, Batch Loss: 0.36847177147865295\n",
      "Epoch 3/10, Batch Loss: 0.5425029397010803\n",
      "Epoch 3/10, Batch Loss: 0.3444463908672333\n",
      "Epoch 3/10, Batch Loss: 0.09812670201063156\n",
      "Epoch 3/10, Batch Loss: 0.22214967012405396\n",
      "Epoch 3/10, Batch Loss: 0.34834912419319153\n",
      "Epoch 3/10, Batch Loss: 0.2491278201341629\n",
      "Epoch 3/10, Batch Loss: 0.3392087519168854\n",
      "Epoch 3/10, Batch Loss: 0.24272483587265015\n",
      "Epoch 3/10, Batch Loss: 0.26310399174690247\n",
      "Epoch 3/10, Batch Loss: 0.09699974954128265\n",
      "Epoch 3/10, Batch Loss: 0.4356694519519806\n",
      "Epoch 3/10, Batch Loss: 0.24294544756412506\n",
      "Epoch 3/10, Batch Loss: 0.35900336503982544\n",
      "Epoch 3/10, Batch Loss: 0.2267710417509079\n",
      "Epoch 3/10, Batch Loss: 0.09028619527816772\n",
      "Epoch 3/10, Batch Loss: 0.09253311902284622\n",
      "Epoch 3/10, Batch Loss: 0.350117027759552\n",
      "Epoch 3/10, Batch Loss: 0.3560948371887207\n",
      "Epoch 3/10, Batch Loss: 0.20582307875156403\n",
      "Epoch 3/10, Batch Loss: 0.3983718156814575\n",
      "Epoch 3/10, Batch Loss: 0.17770764231681824\n",
      "Epoch 3/10, Batch Loss: 0.2000342756509781\n",
      "Epoch 3/10, Batch Loss: 0.1901283860206604\n",
      "Epoch 3/10, Batch Loss: 0.21772995591163635\n",
      "Epoch 3/10, Batch Loss: 0.45620644092559814\n",
      "Epoch 3/10, Batch Loss: 0.3600221574306488\n",
      "Epoch 3/10, Batch Loss: 0.4696469306945801\n",
      "Epoch 3/10, Batch Loss: 0.24124234914779663\n",
      "Epoch 3/10, Batch Loss: 0.24900342524051666\n",
      "Epoch 3/10, Batch Loss: 0.20398569107055664\n",
      "Epoch 3/10, Batch Loss: 0.3143797218799591\n",
      "Epoch 3/10, Batch Loss: 0.20655210316181183\n",
      "Epoch 3/10, Batch Loss: 0.6707355976104736\n",
      "Epoch 3/10, Batch Loss: 0.09620440006256104\n",
      "Epoch 3/10, Batch Loss: 0.10002673417329788\n",
      "Epoch 3/10, Batch Loss: 0.45433786511421204\n",
      "Epoch 3/10, Batch Loss: 0.35332104563713074\n",
      "Epoch 3/10, Batch Loss: 0.3373270630836487\n",
      "Epoch 3/10, Batch Loss: 0.11497355252504349\n",
      "Epoch 3/10, Batch Loss: 0.2083841860294342\n",
      "Epoch 3/10, Batch Loss: 0.21579042077064514\n",
      "Epoch 3/10, Batch Loss: 0.4653683602809906\n",
      "Epoch 3/10, Batch Loss: 0.19177138805389404\n",
      "Epoch 3/10, Batch Loss: 0.4224145710468292\n",
      "Epoch 3/10, Batch Loss: 0.4547595679759979\n",
      "Epoch 3/10, Batch Loss: 0.21528568863868713\n",
      "Epoch 3/10, Batch Loss: 0.34811216592788696\n",
      "Epoch 3/10, Batch Loss: 0.32335570454597473\n",
      "Epoch 3/10, Batch Loss: 0.22766725718975067\n",
      "Epoch 3/10, Batch Loss: 0.3308621048927307\n",
      "Epoch 3/10, Batch Loss: 0.3364979922771454\n",
      "Epoch 3/10, Batch Loss: 0.4482095241546631\n",
      "Epoch 3/10, Batch Loss: 0.5877805352210999\n",
      "Epoch 3/10, Batch Loss: 0.3683043122291565\n",
      "Epoch 3/10, Batch Loss: 0.44283267855644226\n",
      "Epoch 3/10, Batch Loss: 0.34834036231040955\n",
      "Epoch 3/10, Batch Loss: 0.2975325286388397\n",
      "Epoch 3/10, Batch Loss: 0.08572336286306381\n",
      "Epoch 3/10, Batch Loss: 0.4631475508213043\n",
      "Epoch 3/10, Batch Loss: 0.34142544865608215\n",
      "Epoch 3/10, Batch Loss: 0.5565062761306763\n",
      "Epoch 3/10, Batch Loss: 0.22751882672309875\n",
      "Epoch 3/10, Batch Loss: 0.5359389185905457\n",
      "Epoch 3/10, Batch Loss: 0.2220185548067093\n",
      "Epoch 3/10, Batch Loss: 0.4162614047527313\n",
      "Epoch 3/10, Batch Loss: 0.5417801737785339\n",
      "Epoch 3/10, Batch Loss: 0.2309425324201584\n",
      "Epoch 3/10, Batch Loss: 0.21088235080242157\n",
      "Epoch 3/10, Batch Loss: 0.5353257656097412\n",
      "Epoch 3/10, Batch Loss: 0.27682653069496155\n",
      "Epoch 3/10, Batch Loss: 0.3341308832168579\n",
      "Epoch 3/10, Batch Loss: 0.2056075781583786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Batch Loss: 0.434532105922699\n",
      "Epoch 3/10, Batch Loss: 0.5644709467887878\n",
      "Epoch 3/10, Batch Loss: 0.3751179873943329\n",
      "Epoch 3/10, Batch Loss: 0.32393878698349\n",
      "Epoch 3/10, Batch Loss: 0.39530590176582336\n",
      "Epoch 3/10, Batch Loss: 0.1480053961277008\n",
      "Epoch 3/10, Batch Loss: 0.45451971888542175\n",
      "Epoch 3/10, Batch Loss: 0.2704889178276062\n",
      "Epoch 3/10, Batch Loss: 0.3163966238498688\n",
      "Epoch 3/10, Batch Loss: 0.5258164405822754\n",
      "Epoch 3/10, Batch Loss: 0.32988664507865906\n",
      "Epoch 3/10, Batch Loss: 0.3478582799434662\n",
      "Epoch 3/10, Batch Loss: 0.15758681297302246\n",
      "Epoch 3/10, Batch Loss: 0.46237069368362427\n",
      "Epoch 3/10, Batch Loss: 0.11538927257061005\n",
      "Epoch 3/10, Batch Loss: 0.12498921155929565\n",
      "Epoch 3/10, Batch Loss: 0.4225130081176758\n",
      "Epoch 3/10, Batch Loss: 0.5386319756507874\n",
      "Epoch 3/10, Batch Loss: 0.12910886108875275\n",
      "Epoch 3/10, Batch Loss: 0.33519524335861206\n",
      "Epoch 3/10, Batch Loss: 0.4388473629951477\n",
      "Epoch 3/10, Batch Loss: 0.38394618034362793\n",
      "Epoch 3/10, Batch Loss: 0.31126776337623596\n",
      "Epoch 3/10, Batch Loss: 0.08943883329629898\n",
      "Epoch 3/10, Batch Loss: 0.2333087921142578\n",
      "Epoch 3/10, Batch Loss: 0.10114296525716782\n",
      "Epoch 3/10, Batch Loss: 0.10390263795852661\n",
      "Epoch 3/10, Batch Loss: 0.3663429319858551\n",
      "Epoch 3/10, Batch Loss: 0.1014886349439621\n",
      "Epoch 3/10, Batch Loss: 0.05701567977666855\n",
      "Epoch 3/10, Batch Loss: 0.4947202503681183\n",
      "Epoch 3/10, Batch Loss: 0.3868970572948456\n",
      "Epoch 3/10, Batch Loss: 0.20354945957660675\n",
      "Epoch 3/10, Batch Loss: 0.3719135522842407\n",
      "Epoch 3/10, Batch Loss: 0.06509269773960114\n",
      "Epoch 3/10, Batch Loss: 0.23020456731319427\n",
      "Epoch 3/10, Batch Loss: 0.23077969253063202\n",
      "Epoch 3/10, Batch Loss: 0.3743395507335663\n",
      "Epoch 3/10, Batch Loss: 0.06299039721488953\n",
      "Epoch 3/10, Batch Loss: 0.33684128522872925\n",
      "Epoch 3/10, Batch Loss: 0.5234917402267456\n",
      "Epoch 3/10, Batch Loss: 0.0639200359582901\n",
      "Epoch 3/10, Batch Loss: 0.3744027018547058\n",
      "Epoch 3/10, Batch Loss: 0.3539775311946869\n",
      "Epoch 3/10, Batch Loss: 0.20926693081855774\n",
      "Epoch 3/10, Batch Loss: 0.21446122229099274\n",
      "Epoch 3/10, Batch Loss: 0.32873085141181946\n",
      "Epoch 3/10, Batch Loss: 0.22086843848228455\n",
      "Epoch 3/10, Batch Loss: 0.5115005970001221\n",
      "Epoch 3/10, Batch Loss: 0.0695357397198677\n",
      "Epoch 3/10, Batch Loss: 0.3603499233722687\n",
      "Epoch 3/10, Batch Loss: 0.6018730401992798\n",
      "Epoch 3/10, Batch Loss: 0.08727049082517624\n",
      "Epoch 3/10, Batch Loss: 0.10290011018514633\n",
      "Epoch 3/10, Batch Loss: 0.34925249218940735\n",
      "Epoch 3/10, Batch Loss: 0.5975227952003479\n",
      "Epoch 3/10, Batch Loss: 0.2528986632823944\n",
      "Epoch 3/10, Batch Loss: 0.34850552678108215\n",
      "Epoch 3/10, Batch Loss: 0.7406474351882935\n",
      "Epoch 3/10, Batch Loss: 0.21710187196731567\n",
      "Epoch 3/10, Batch Loss: 0.47123050689697266\n",
      "Epoch 3/10, Batch Loss: 0.21816261112689972\n",
      "Epoch 3/10, Batch Loss: 0.4371568560600281\n",
      "Epoch 3/10, Batch Loss: 0.3177641034126282\n",
      "Epoch 3/10, Batch Loss: 0.11653850972652435\n",
      "Epoch 3/10, Batch Loss: 0.11680919677019119\n",
      "Epoch 3/10, Batch Loss: 0.46140360832214355\n",
      "Epoch 3/10, Batch Loss: 0.23170137405395508\n",
      "Epoch 3/10, Batch Loss: 0.15231142938137054\n",
      "Epoch 3/10, Batch Loss: 0.2980060577392578\n",
      "Epoch 3/10, Batch Loss: 0.48434773087501526\n",
      "Epoch 3/10, Batch Loss: 0.4467255473136902\n",
      "Epoch 3/10, Batch Loss: 0.5045963525772095\n",
      "Epoch 3/10, Batch Loss: 0.10737748444080353\n",
      "Epoch 3/10, Batch Loss: 0.4286830425262451\n",
      "Epoch 3/10, Batch Loss: 0.28892260789871216\n",
      "Epoch 3/10, Batch Loss: 0.2408614456653595\n",
      "Epoch 3/10, Batch Loss: 0.14610405266284943\n",
      "Epoch 3/10, Batch Loss: 0.22995717823505402\n",
      "Epoch 3/10, Batch Loss: 0.2663564085960388\n",
      "Epoch 3/10, Batch Loss: 0.4811984896659851\n",
      "Epoch 3/10, Batch Loss: 0.24623508751392365\n",
      "Epoch 3/10, Batch Loss: 0.35462862253189087\n",
      "Epoch 3/10, Batch Loss: 0.341608464717865\n",
      "Epoch 3/10, Batch Loss: 0.23019003868103027\n",
      "Epoch 3/10, Batch Loss: 0.43026381731033325\n",
      "Epoch 3/10, Batch Loss: 0.3986705541610718\n",
      "Epoch 3/10, Batch Loss: 0.3106920123100281\n",
      "Epoch 3/10, Batch Loss: 0.4571492671966553\n",
      "Epoch 3/10, Batch Loss: 0.4712591767311096\n",
      "Epoch 3/10, Batch Loss: 0.35359933972358704\n",
      "Epoch 3/10, Batch Loss: 0.3420301675796509\n",
      "Epoch 3/10, Batch Loss: 0.4531528651714325\n",
      "Epoch 3/10, Batch Loss: 0.49772101640701294\n",
      "Epoch 3/10, Batch Loss: 0.42337608337402344\n",
      "Epoch 3/10, Batch Loss: 0.23634934425354004\n",
      "Epoch 3/10, Batch Loss: 0.2148839235305786\n",
      "Epoch 3/10, Batch Loss: 0.3324163556098938\n",
      "Epoch 3/10, Batch Loss: 0.18660114705562592\n",
      "Epoch 3/10, Batch Loss: 0.4693157374858856\n",
      "Epoch 3/10, Batch Loss: 0.20977690815925598\n",
      "Epoch 3/10, Batch Loss: 0.47027134895324707\n",
      "Epoch 3/10, Batch Loss: 0.18224215507507324\n",
      "Epoch 3/10, Batch Loss: 0.3082202076911926\n",
      "Epoch 3/10, Batch Loss: 0.32022109627723694\n",
      "Epoch 3/10, Batch Loss: 0.44665881991386414\n",
      "Epoch 3/10, Batch Loss: 0.1562644988298416\n",
      "Epoch 3/10, Batch Loss: 0.36180397868156433\n",
      "Epoch 3/10, Batch Loss: 0.6310702562332153\n",
      "Epoch 3/10, Batch Loss: 0.29624059796333313\n",
      "Epoch 3/10, Batch Loss: 0.3116162419319153\n",
      "Epoch 3/10, Batch Loss: 0.22893714904785156\n",
      "Epoch 3/10, Batch Loss: 0.34526610374450684\n",
      "Epoch 3/10, Batch Loss: 0.4821834862232208\n",
      "Epoch 3/10, Batch Loss: 0.12103129923343658\n",
      "Epoch 3/10, Batch Loss: 0.6408685445785522\n",
      "Epoch 3/10, Batch Loss: 0.3410128653049469\n",
      "Epoch 3/10, Batch Loss: 0.3488129675388336\n",
      "Epoch 3/10, Batch Loss: 0.09610515832901001\n",
      "Epoch 3/10, Batch Loss: 0.3353811800479889\n",
      "Epoch 3/10, Batch Loss: 0.37928593158721924\n",
      "Epoch 3/10, Batch Loss: 0.47517678141593933\n",
      "Epoch 3/10, Batch Loss: 0.4054768681526184\n",
      "Epoch 3/10, Batch Loss: 0.3008720874786377\n",
      "Epoch 3/10, Batch Loss: 0.2491343915462494\n",
      "Epoch 3/10, Batch Loss: 0.4704490602016449\n",
      "Epoch 3/10, Batch Loss: 0.4312441349029541\n",
      "Epoch 3/10, Batch Loss: 0.41652911901474\n",
      "Epoch 3/10, Batch Loss: 0.35863009095191956\n",
      "Epoch 3/10, Batch Loss: 0.3648527264595032\n",
      "Epoch 3/10, Batch Loss: 0.32850000262260437\n",
      "Epoch 3/10, Batch Loss: 0.15373170375823975\n",
      "Epoch 3/10, Batch Loss: 0.6774678230285645\n",
      "Epoch 3/10, Batch Loss: 0.41612815856933594\n",
      "Epoch 3/10, Batch Loss: 0.13692200183868408\n",
      "Epoch 3/10, Batch Loss: 0.44761425256729126\n",
      "Epoch 3/10, Batch Loss: 0.3071615993976593\n",
      "Epoch 3/10, Batch Loss: 0.1867290437221527\n",
      "Epoch 3/10, Batch Loss: 0.2699359655380249\n",
      "Epoch 3/10, Batch Loss: 0.47602203488349915\n",
      "Epoch 3/10, Batch Loss: 0.23072749376296997\n",
      "Epoch 3/10, Batch Loss: 0.5821459889411926\n",
      "Epoch 3/10, Batch Loss: 0.33983442187309265\n",
      "Epoch 3/10, Batch Loss: 0.14603665471076965\n",
      "Epoch 3/10, Batch Loss: 0.5622658133506775\n",
      "Epoch 3/10, Batch Loss: 0.36285722255706787\n",
      "Epoch 3/10, Batch Loss: 0.36359694600105286\n",
      "Epoch 3/10, Batch Loss: 1.3995180130004883\n",
      "Epoch 3/10, Batch Loss: 2.0140507221221924\n",
      "Epoch 3/10, Batch Loss: 0.3282206654548645\n",
      "Epoch 3/10, Batch Loss: 0.6563321948051453\n",
      "Epoch 3/10, Batch Loss: 0.6563877463340759\n",
      "Epoch 3/10, Batch Loss: 0.7619362473487854\n",
      "Epoch 3/10, Batch Loss: 0.8916301131248474\n",
      "Epoch 3/10, Batch Loss: 0.5701018571853638\n",
      "Epoch 3/10, Batch Loss: 0.698758602142334\n",
      "Epoch 3/10, Batch Loss: 0.6094843149185181\n",
      "Epoch 3/10, Batch Loss: 0.5772136449813843\n",
      "Epoch 3/10, Batch Loss: 0.7278164029121399\n",
      "Epoch 3/10, Batch Loss: 0.6321778893470764\n",
      "Epoch 3/10, Batch Loss: 0.42988526821136475\n",
      "Epoch 3/10, Batch Loss: 0.9258573651313782\n",
      "Epoch 3/10, Batch Loss: 0.7808229923248291\n",
      "Epoch 3/10, Batch Loss: 0.5981142520904541\n",
      "Epoch 3/10, Batch Loss: 0.6526616811752319\n",
      "Epoch 3/10, Batch Loss: 0.6727855205535889\n",
      "Epoch 3/10, Batch Loss: 0.5620771646499634\n",
      "Epoch 3/10, Batch Loss: 0.6487436890602112\n",
      "Epoch 3/10, Batch Loss: 0.5733139514923096\n",
      "Epoch 3/10, Batch Loss: 0.6232805252075195\n",
      "Epoch 3/10, Batch Loss: 0.6913560628890991\n",
      "Epoch 3/10, Batch Loss: 0.5558386445045471\n",
      "Epoch 3/10, Batch Loss: 0.5301042795181274\n",
      "Epoch 3/10, Batch Loss: 0.6598488092422485\n",
      "Epoch 3/10, Batch Loss: 0.4888182282447815\n",
      "Epoch 3/10, Batch Loss: 0.6219377517700195\n",
      "Epoch 3/10, Batch Loss: 0.6633371710777283\n",
      "Epoch 3/10, Batch Loss: 0.629524290561676\n",
      "Epoch 3/10, Batch Loss: 0.5588611364364624\n",
      "Epoch 3/10, Batch Loss: 0.41213417053222656\n",
      "Epoch 3/10, Batch Loss: 0.7544053196907043\n",
      "Epoch 3/10, Batch Loss: 0.5018008947372437\n",
      "Epoch 3/10, Batch Loss: 0.757734477519989\n",
      "Epoch 3/10, Batch Loss: 0.6984938383102417\n",
      "Epoch 3/10, Batch Loss: 0.5417692065238953\n",
      "Epoch 3/10, Batch Loss: 0.6647367477416992\n",
      "Epoch 3/10, Batch Loss: 0.6171935796737671\n",
      "Epoch 3/10, Batch Loss: 0.6794649958610535\n",
      "Epoch 3/10, Batch Loss: 0.4876354932785034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Batch Loss: 0.8637329936027527\n",
      "Epoch 3/10, Batch Loss: 0.5929728150367737\n",
      "Epoch 3/10, Batch Loss: 0.5313339233398438\n",
      "Epoch 3/10, Batch Loss: 0.6762797236442566\n",
      "Epoch 3/10, Batch Loss: 0.7434151768684387\n",
      "Epoch 3/10, Batch Loss: 0.504677951335907\n",
      "Epoch 3/10, Batch Loss: 0.6736072301864624\n",
      "Epoch 3/10, Batch Loss: 0.6721469163894653\n",
      "Epoch 3/10, Batch Loss: 0.7232152223587036\n",
      "Epoch 3/10, Batch Loss: 0.44177496433258057\n",
      "Epoch 3/10, Batch Loss: 0.7215025424957275\n",
      "Epoch 3/10, Batch Loss: 0.5318666100502014\n",
      "Epoch 3/10, Batch Loss: 0.6336197853088379\n",
      "Epoch 3/10, Batch Loss: 0.6861889362335205\n",
      "Epoch 3/10, Batch Loss: 0.6570265293121338\n",
      "Epoch 3/10, Batch Loss: 0.6002522706985474\n",
      "Epoch 3/10, Batch Loss: 0.6455762386322021\n",
      "Epoch 3/10, Batch Loss: 0.5318251252174377\n",
      "Epoch 3/10, Batch Loss: 0.5945788025856018\n",
      "Epoch 3/10, Batch Loss: 0.5895785689353943\n",
      "Epoch 3/10, Batch Loss: 0.6540151238441467\n",
      "Epoch 3/10, Batch Loss: 0.38634923100471497\n",
      "Epoch 3/10, Batch Loss: 0.7011598348617554\n",
      "Epoch 3/10, Batch Loss: 0.3798520565032959\n",
      "Epoch 3/10, Batch Loss: 0.5728355646133423\n",
      "Epoch 3/10, Batch Loss: 0.4300658702850342\n",
      "Epoch 3/10, Batch Loss: 0.46959394216537476\n",
      "Epoch 3/10, Batch Loss: 0.634110689163208\n",
      "Epoch 3/10, Batch Loss: 0.4039640724658966\n",
      "Epoch 3/10, Batch Loss: 0.6553061008453369\n",
      "Epoch 3/10, Batch Loss: 0.500007688999176\n",
      "Epoch 3/10, Batch Loss: 0.5023035407066345\n",
      "Epoch 3/10, Batch Loss: 0.9369284510612488\n",
      "Epoch 3/10, Batch Loss: 0.6163032650947571\n",
      "Epoch 3/10, Batch Loss: 0.6970306634902954\n",
      "Epoch 3/10, Batch Loss: 0.6065786480903625\n",
      "Epoch 3/10, Batch Loss: 0.4561576545238495\n",
      "Epoch 3/10, Batch Loss: 0.549318253993988\n",
      "Epoch 3/10, Batch Loss: 0.6274251937866211\n",
      "Epoch 3/10, Batch Loss: 0.5921087861061096\n",
      "Epoch 3/10, Batch Loss: 0.505151093006134\n",
      "Epoch 3/10, Batch Loss: 0.3410669267177582\n",
      "Epoch 3/10, Batch Loss: 0.4556411802768707\n",
      "Epoch 3/10, Batch Loss: 0.4743775725364685\n",
      "Epoch 3/10, Batch Loss: 0.9297422766685486\n",
      "Epoch 3/10, Batch Loss: 0.48869797587394714\n",
      "Epoch 3/10, Batch Loss: 0.5677149295806885\n",
      "Epoch 3/10, Batch Loss: 0.46073105931282043\n",
      "Epoch 3/10, Batch Loss: 0.6256052255630493\n",
      "Epoch 3/10, Batch Loss: 0.5034044981002808\n",
      "Epoch 3/10, Batch Loss: 0.6743636727333069\n",
      "Epoch 3/10, Batch Loss: 0.4898700416088104\n",
      "Epoch 3/10, Batch Loss: 0.689750611782074\n",
      "Epoch 3/10, Batch Loss: 0.4753967821598053\n",
      "Epoch 3/10, Batch Loss: 0.6962448358535767\n",
      "Epoch 3/10, Batch Loss: 0.5930496454238892\n",
      "Epoch 3/10, Batch Loss: 0.4666372239589691\n",
      "Epoch 3/10, Batch Loss: 0.7653488516807556\n",
      "Epoch 3/10, Batch Loss: 0.662229061126709\n",
      "Epoch 3/10, Batch Loss: 0.7224799394607544\n",
      "Epoch 3/10, Batch Loss: 0.47609901428222656\n",
      "Epoch 3/10, Batch Loss: 0.5654644966125488\n",
      "Epoch 3/10, Batch Loss: 0.5174049735069275\n",
      "Epoch 3/10, Batch Loss: 0.6824138760566711\n",
      "Epoch 3/10, Batch Loss: 0.5151563286781311\n",
      "Epoch 3/10, Batch Loss: 0.716217041015625\n",
      "Epoch 3/10, Batch Loss: 0.6802177429199219\n",
      "Epoch 3/10, Batch Loss: 0.4381091594696045\n",
      "Epoch 3/10, Batch Loss: 0.49141809344291687\n",
      "Epoch 3/10, Batch Loss: 0.5555950999259949\n",
      "Epoch 3/10, Batch Loss: 0.6359270215034485\n",
      "Epoch 3/10, Batch Loss: 0.8394004106521606\n",
      "Epoch 3/10, Batch Loss: 0.39872002601623535\n",
      "Epoch 3/10, Batch Loss: 0.7314194440841675\n",
      "Epoch 3/10, Batch Loss: 0.5121080875396729\n",
      "Epoch 3/10, Batch Loss: 0.5944072604179382\n",
      "Epoch 3/10, Batch Loss: 0.6277512907981873\n",
      "Epoch 3/10, Batch Loss: 0.6954435110092163\n",
      "Epoch 3/10, Batch Loss: 0.6139869689941406\n",
      "Epoch 3/10, Batch Loss: 0.47506022453308105\n",
      "Epoch 3/10, Batch Loss: 0.668217122554779\n",
      "Epoch 3/10, Batch Loss: 0.4996097683906555\n",
      "Epoch 3/10, Batch Loss: 0.5256317257881165\n",
      "Epoch 3/10, Batch Loss: 0.7333724498748779\n",
      "Epoch 3/10, Batch Loss: 0.5842701196670532\n",
      "Epoch 3/10, Batch Loss: 0.6083064675331116\n",
      "Epoch 3/10, Batch Loss: 0.7087165713310242\n",
      "Epoch 3/10, Batch Loss: 0.6201105117797852\n",
      "Epoch 3/10, Batch Loss: 0.33464494347572327\n",
      "Epoch 3/10, Batch Loss: 0.7331495881080627\n",
      "Epoch 3/10, Batch Loss: 0.4353678524494171\n",
      "Epoch 3/10, Batch Loss: 0.5743751525878906\n",
      "Epoch 3/10, Batch Loss: 0.6502537727355957\n",
      "Epoch 3/10, Batch Loss: 0.44670525193214417\n",
      "Epoch 3/10, Batch Loss: 0.811325192451477\n",
      "Epoch 3/10, Batch Loss: 0.5044559240341187\n",
      "Epoch 3/10, Batch Loss: 0.546332597732544\n",
      "Epoch 3/10, Batch Loss: 0.42098894715309143\n",
      "Epoch 3/10, Batch Loss: 0.47896361351013184\n",
      "Epoch 3/10, Batch Loss: 0.6541641354560852\n",
      "Epoch 3/10, Batch Loss: 0.5399668216705322\n",
      "Epoch 3/10, Batch Loss: 0.6883171200752258\n",
      "Epoch 3/10, Batch Loss: 0.7577451467514038\n",
      "Epoch 3/10, Batch Loss: 0.6118190884590149\n",
      "Epoch 3/10, Batch Loss: 0.8335931301116943\n",
      "Epoch 3/10, Batch Loss: 0.7326664924621582\n",
      "Epoch 3/10, Batch Loss: 0.7840560674667358\n",
      "Epoch 3/10, Batch Loss: 0.7403094172477722\n",
      "Epoch 3/10, Batch Loss: 0.5375207662582397\n",
      "Epoch 3/10, Batch Loss: 0.5337845087051392\n",
      "Epoch 3/10, Batch Loss: 0.5453032851219177\n",
      "Epoch 3/10, Batch Loss: 0.6219579577445984\n",
      "Epoch 3/10, Batch Loss: 0.5313688516616821\n",
      "Epoch 3/10, Batch Loss: 0.3897901773452759\n",
      "Epoch 3/10, Batch Loss: 0.6024557948112488\n",
      "Epoch 3/10, Batch Loss: 0.7735880613327026\n",
      "Epoch 3/10, Batch Loss: 0.5112821459770203\n",
      "Epoch 3/10, Batch Loss: 0.7465074062347412\n",
      "Epoch 3/10, Batch Loss: 0.6264464855194092\n",
      "Epoch 3/10, Batch Loss: 0.40309447050094604\n",
      "Epoch 3/10, Batch Loss: 0.5427227020263672\n",
      "Epoch 3/10, Batch Loss: 0.47707971930503845\n",
      "Epoch 3/10, Batch Loss: 0.3873962461948395\n",
      "Epoch 3/10, Batch Loss: 0.6451263427734375\n",
      "Epoch 3/10, Batch Loss: 0.4776288866996765\n",
      "Epoch 3/10, Batch Loss: 0.6334783434867859\n",
      "Epoch 3/10, Batch Loss: 0.8639298677444458\n",
      "Epoch 3/10, Batch Loss: 0.539948046207428\n",
      "Epoch 3/10, Batch Loss: 0.5613083839416504\n",
      "Epoch 3/10, Batch Loss: 0.43582528829574585\n",
      "Epoch 3/10, Batch Loss: 0.6574605703353882\n",
      "Epoch 3/10, Batch Loss: 0.5434395670890808\n",
      "Epoch 3/10, Batch Loss: 0.424467533826828\n",
      "Epoch 3/10, Batch Loss: 0.7118531465530396\n",
      "Epoch 3/10, Batch Loss: 0.7607913017272949\n",
      "Epoch 3/10, Batch Loss: 0.608119547367096\n",
      "Epoch 3/10, Batch Loss: 0.6178726553916931\n",
      "Epoch 3/10, Batch Loss: 1.0044478178024292\n",
      "Epoch 3/10, Batch Loss: 0.41523656249046326\n",
      "Epoch 3/10, Batch Loss: 0.47164487838745117\n",
      "Epoch 3/10, Batch Loss: 0.8117037415504456\n",
      "Epoch 3/10, Batch Loss: 0.4318240284919739\n",
      "Epoch 3/10, Batch Loss: 0.7557942271232605\n",
      "Epoch 3/10, Batch Loss: 0.5376181602478027\n",
      "Epoch 3/10, Batch Loss: 0.6473641395568848\n",
      "Epoch 3/10, Batch Loss: 0.5275884866714478\n",
      "Epoch 3/10, Batch Loss: 0.4148241877555847\n",
      "Epoch 3/10, Batch Loss: 0.3882167637348175\n",
      "Epoch 3/10, Batch Loss: 0.5595122575759888\n",
      "Epoch 3/10, Batch Loss: 0.5716680884361267\n",
      "Epoch 3/10, Batch Loss: 0.5160180330276489\n",
      "Epoch 3/10, Batch Loss: 0.5208316445350647\n",
      "Epoch 3/10, Average Training Loss: 0.45216832971069176\n",
      "Model saved for epoch 3 at Pre_train_epoch3.pt\n",
      "Validation Accuracy for epoch 3: 0.7261348464619493\n",
      "Epoch 4/10, Batch Loss: 0.5119425058364868\n",
      "Epoch 4/10, Batch Loss: 0.7529656887054443\n",
      "Epoch 4/10, Batch Loss: 0.4953349530696869\n",
      "Epoch 4/10, Batch Loss: 0.4988080859184265\n",
      "Epoch 4/10, Batch Loss: 0.4667702913284302\n",
      "Epoch 4/10, Batch Loss: 0.6890249848365784\n",
      "Epoch 4/10, Batch Loss: 0.7108410596847534\n",
      "Epoch 4/10, Batch Loss: 0.6608098149299622\n",
      "Epoch 4/10, Batch Loss: 0.670126736164093\n",
      "Epoch 4/10, Batch Loss: 0.6875242590904236\n",
      "Epoch 4/10, Batch Loss: 0.7101040482521057\n",
      "Epoch 4/10, Batch Loss: 0.6406315565109253\n",
      "Epoch 4/10, Batch Loss: 0.4320940375328064\n",
      "Epoch 4/10, Batch Loss: 0.7226337790489197\n",
      "Epoch 4/10, Batch Loss: 0.5198595523834229\n",
      "Epoch 4/10, Batch Loss: 0.5147193074226379\n",
      "Epoch 4/10, Batch Loss: 0.7456785440444946\n",
      "Epoch 4/10, Batch Loss: 0.687991201877594\n",
      "Epoch 4/10, Batch Loss: 0.5968706607818604\n",
      "Epoch 4/10, Batch Loss: 0.6032981276512146\n",
      "Epoch 4/10, Batch Loss: 0.5397610664367676\n",
      "Epoch 4/10, Batch Loss: 0.7658328413963318\n",
      "Epoch 4/10, Batch Loss: 0.6097289323806763\n",
      "Epoch 4/10, Batch Loss: 0.5319251418113708\n",
      "Epoch 4/10, Batch Loss: 0.5088697075843811\n",
      "Epoch 4/10, Batch Loss: 0.645319938659668\n",
      "Epoch 4/10, Batch Loss: 0.5393510460853577\n",
      "Epoch 4/10, Batch Loss: 0.5794876217842102\n",
      "Epoch 4/10, Batch Loss: 0.6102713942527771\n",
      "Epoch 4/10, Batch Loss: 0.641825795173645\n",
      "Epoch 4/10, Batch Loss: 0.6340801119804382\n",
      "Epoch 4/10, Batch Loss: 0.7150478363037109\n",
      "Epoch 4/10, Batch Loss: 0.8260253071784973\n",
      "Epoch 4/10, Batch Loss: 0.7751386165618896\n",
      "Epoch 4/10, Batch Loss: 0.5822485089302063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Batch Loss: 0.5250555276870728\n",
      "Epoch 4/10, Batch Loss: 0.8559890389442444\n",
      "Epoch 4/10, Batch Loss: 0.5248561501502991\n",
      "Epoch 4/10, Batch Loss: 0.8604393601417542\n",
      "Epoch 4/10, Batch Loss: 0.9000163674354553\n",
      "Epoch 4/10, Batch Loss: 0.6066642999649048\n",
      "Epoch 4/10, Batch Loss: 0.5529175400733948\n",
      "Epoch 4/10, Batch Loss: 0.6180764436721802\n",
      "Epoch 4/10, Batch Loss: 0.47370845079421997\n",
      "Epoch 4/10, Batch Loss: 0.5727359652519226\n",
      "Epoch 4/10, Batch Loss: 0.5468273758888245\n",
      "Epoch 4/10, Batch Loss: 0.5739675164222717\n",
      "Epoch 4/10, Batch Loss: 0.684780478477478\n",
      "Epoch 4/10, Batch Loss: 0.6933670043945312\n",
      "Epoch 4/10, Batch Loss: 0.5067875385284424\n",
      "Epoch 4/10, Batch Loss: 0.8093979358673096\n",
      "Epoch 4/10, Batch Loss: 0.6730056405067444\n",
      "Epoch 4/10, Batch Loss: 0.6830881237983704\n",
      "Epoch 4/10, Batch Loss: 0.7040009498596191\n",
      "Epoch 4/10, Batch Loss: 0.6427336931228638\n",
      "Epoch 4/10, Batch Loss: 0.5078014135360718\n",
      "Epoch 4/10, Batch Loss: 0.7310107350349426\n",
      "Epoch 4/10, Batch Loss: 0.6280449628829956\n",
      "Epoch 4/10, Batch Loss: 0.5313422679901123\n",
      "Epoch 4/10, Batch Loss: 0.587289571762085\n",
      "Epoch 4/10, Batch Loss: 0.7177467346191406\n",
      "Epoch 4/10, Batch Loss: 0.5926176905632019\n",
      "Epoch 4/10, Batch Loss: 0.5494040846824646\n",
      "Epoch 4/10, Batch Loss: 0.8017924427986145\n",
      "Epoch 4/10, Batch Loss: 0.4194319546222687\n",
      "Epoch 4/10, Batch Loss: 0.7982932925224304\n",
      "Epoch 4/10, Batch Loss: 0.4843917787075043\n",
      "Epoch 4/10, Batch Loss: 0.5575557351112366\n",
      "Epoch 4/10, Batch Loss: 0.3766525089740753\n",
      "Epoch 4/10, Batch Loss: 0.6698966026306152\n",
      "Epoch 4/10, Batch Loss: 0.6638137698173523\n",
      "Epoch 4/10, Batch Loss: 0.5357887744903564\n",
      "Epoch 4/10, Batch Loss: 0.615914523601532\n",
      "Epoch 4/10, Batch Loss: 0.5932586193084717\n",
      "Epoch 4/10, Batch Loss: 0.5578204393386841\n",
      "Epoch 4/10, Batch Loss: 0.6788032054901123\n",
      "Epoch 4/10, Batch Loss: 0.637910783290863\n",
      "Epoch 4/10, Batch Loss: 0.5399178862571716\n",
      "Epoch 4/10, Batch Loss: 0.6269291043281555\n",
      "Epoch 4/10, Batch Loss: 0.4285513460636139\n",
      "Epoch 4/10, Batch Loss: 0.28204527497291565\n",
      "Epoch 4/10, Batch Loss: 0.41000041365623474\n",
      "Epoch 4/10, Batch Loss: 0.7499132752418518\n",
      "Epoch 4/10, Batch Loss: 0.6348633170127869\n",
      "Epoch 4/10, Batch Loss: 0.5240249037742615\n",
      "Epoch 4/10, Batch Loss: 0.5936786532402039\n",
      "Epoch 4/10, Batch Loss: 0.7392734289169312\n",
      "Epoch 4/10, Batch Loss: 0.5725703239440918\n",
      "Epoch 4/10, Batch Loss: 0.43201664090156555\n",
      "Epoch 4/10, Batch Loss: 0.5677103996276855\n",
      "Epoch 4/10, Batch Loss: 0.3955807089805603\n",
      "Epoch 4/10, Batch Loss: 0.7585231065750122\n",
      "Epoch 4/10, Batch Loss: 0.7536603808403015\n",
      "Epoch 4/10, Batch Loss: 0.6318753361701965\n",
      "Epoch 4/10, Batch Loss: 0.7157170176506042\n",
      "Epoch 4/10, Batch Loss: 0.6010291576385498\n",
      "Epoch 4/10, Batch Loss: 0.6001512408256531\n",
      "Epoch 4/10, Batch Loss: 0.518020749092102\n",
      "Epoch 4/10, Batch Loss: 0.6735402345657349\n",
      "Epoch 4/10, Batch Loss: 0.5971787571907043\n",
      "Epoch 4/10, Batch Loss: 0.6891348361968994\n",
      "Epoch 4/10, Batch Loss: 0.7490648031234741\n",
      "Epoch 4/10, Batch Loss: 0.4689328670501709\n",
      "Epoch 4/10, Batch Loss: 0.4853861927986145\n",
      "Epoch 4/10, Batch Loss: 0.5545555353164673\n",
      "Epoch 4/10, Batch Loss: 0.45876461267471313\n",
      "Epoch 4/10, Batch Loss: 0.5068590641021729\n",
      "Epoch 4/10, Batch Loss: 0.549504280090332\n",
      "Epoch 4/10, Batch Loss: 0.6397040486335754\n",
      "Epoch 4/10, Batch Loss: 0.7072103023529053\n",
      "Epoch 4/10, Batch Loss: 0.6302332878112793\n",
      "Epoch 4/10, Batch Loss: 0.6271231770515442\n",
      "Epoch 4/10, Batch Loss: 0.5696936845779419\n",
      "Epoch 4/10, Batch Loss: 0.7049916386604309\n",
      "Epoch 4/10, Batch Loss: 0.5632274150848389\n",
      "Epoch 4/10, Batch Loss: 0.44262099266052246\n",
      "Epoch 4/10, Batch Loss: 0.6067650318145752\n",
      "Epoch 4/10, Batch Loss: 0.48429185152053833\n",
      "Epoch 4/10, Batch Loss: 0.632736325263977\n",
      "Epoch 4/10, Batch Loss: 0.44883766770362854\n",
      "Epoch 4/10, Batch Loss: 0.79463130235672\n",
      "Epoch 4/10, Batch Loss: 0.514839768409729\n",
      "Epoch 4/10, Batch Loss: 0.47301429510116577\n",
      "Epoch 4/10, Batch Loss: 0.7599161267280579\n",
      "Epoch 4/10, Batch Loss: 0.6459534168243408\n",
      "Epoch 4/10, Batch Loss: 0.4652494490146637\n",
      "Epoch 4/10, Batch Loss: 0.5542401075363159\n",
      "Epoch 4/10, Batch Loss: 0.6114018559455872\n",
      "Epoch 4/10, Batch Loss: 0.4541586637496948\n",
      "Epoch 4/10, Batch Loss: 0.4540155827999115\n",
      "Epoch 4/10, Batch Loss: 0.6715757846832275\n",
      "Epoch 4/10, Batch Loss: 0.562306821346283\n",
      "Epoch 4/10, Batch Loss: 0.6925380825996399\n",
      "Epoch 4/10, Batch Loss: 0.6996243596076965\n",
      "Epoch 4/10, Batch Loss: 0.6514167785644531\n",
      "Epoch 4/10, Batch Loss: 0.543312668800354\n",
      "Epoch 4/10, Batch Loss: 0.3634834587574005\n",
      "Epoch 4/10, Batch Loss: 0.607960045337677\n",
      "Epoch 4/10, Batch Loss: 0.4805397391319275\n",
      "Epoch 4/10, Batch Loss: 0.597547709941864\n",
      "Epoch 4/10, Batch Loss: 0.5749685764312744\n",
      "Epoch 4/10, Batch Loss: 0.7184733152389526\n",
      "Epoch 4/10, Batch Loss: 1.019966959953308\n",
      "Epoch 4/10, Batch Loss: 0.4731318950653076\n",
      "Epoch 4/10, Batch Loss: 0.6442059874534607\n",
      "Epoch 4/10, Batch Loss: 0.5724387168884277\n",
      "Epoch 4/10, Batch Loss: 0.5401403903961182\n",
      "Epoch 4/10, Batch Loss: 0.47317659854888916\n",
      "Epoch 4/10, Batch Loss: 0.799209475517273\n",
      "Epoch 4/10, Batch Loss: 0.3283924162387848\n",
      "Epoch 4/10, Batch Loss: 0.6124873757362366\n",
      "Epoch 4/10, Batch Loss: 0.5582907795906067\n",
      "Epoch 4/10, Batch Loss: 0.43606171011924744\n",
      "Epoch 4/10, Batch Loss: 0.5804835557937622\n",
      "Epoch 4/10, Batch Loss: 0.5609406232833862\n",
      "Epoch 4/10, Batch Loss: 0.6288977861404419\n",
      "Epoch 4/10, Batch Loss: 0.5509199500083923\n",
      "Epoch 4/10, Batch Loss: 0.6868093609809875\n",
      "Epoch 4/10, Batch Loss: 0.5502596497535706\n",
      "Epoch 4/10, Batch Loss: 0.6126275658607483\n",
      "Epoch 4/10, Batch Loss: 0.6484946608543396\n",
      "Epoch 4/10, Batch Loss: 0.5805250406265259\n",
      "Epoch 4/10, Batch Loss: 0.6922025680541992\n",
      "Epoch 4/10, Batch Loss: 0.5051270723342896\n",
      "Epoch 4/10, Batch Loss: 0.4889630377292633\n",
      "Epoch 4/10, Batch Loss: 0.6067630052566528\n",
      "Epoch 4/10, Batch Loss: 0.5803209543228149\n",
      "Epoch 4/10, Batch Loss: 0.7443209290504456\n",
      "Epoch 4/10, Batch Loss: 0.7348710298538208\n",
      "Epoch 4/10, Batch Loss: 0.5772383809089661\n",
      "Epoch 4/10, Batch Loss: 0.8124423027038574\n",
      "Epoch 4/10, Batch Loss: 0.4472818076610565\n",
      "Epoch 4/10, Batch Loss: 0.6831890344619751\n",
      "Epoch 4/10, Batch Loss: 0.6945494413375854\n",
      "Epoch 4/10, Batch Loss: 0.44534242153167725\n",
      "Epoch 4/10, Batch Loss: 0.5318593978881836\n",
      "Epoch 4/10, Batch Loss: 0.712691605091095\n",
      "Epoch 4/10, Batch Loss: 0.36994659900665283\n",
      "Epoch 4/10, Batch Loss: 0.5966977477073669\n",
      "Epoch 4/10, Batch Loss: 0.6300340890884399\n",
      "Epoch 4/10, Batch Loss: 0.5814937949180603\n",
      "Epoch 4/10, Batch Loss: 0.8188483715057373\n",
      "Epoch 4/10, Batch Loss: 0.514162540435791\n",
      "Epoch 4/10, Batch Loss: 0.4591790735721588\n",
      "Epoch 4/10, Batch Loss: 0.7138200402259827\n",
      "Epoch 4/10, Batch Loss: 0.6027376651763916\n",
      "Epoch 4/10, Batch Loss: 0.5310069918632507\n",
      "Epoch 4/10, Batch Loss: 0.4697883725166321\n",
      "Epoch 4/10, Batch Loss: 0.45045459270477295\n",
      "Epoch 4/10, Batch Loss: 0.7715150713920593\n",
      "Epoch 4/10, Batch Loss: 0.5523922443389893\n",
      "Epoch 4/10, Batch Loss: 0.5662464499473572\n",
      "Epoch 4/10, Batch Loss: 0.6713545918464661\n",
      "Epoch 4/10, Batch Loss: 0.6520193219184875\n",
      "Epoch 4/10, Batch Loss: 0.736578643321991\n",
      "Epoch 4/10, Batch Loss: 0.3385584056377411\n",
      "Epoch 4/10, Batch Loss: 0.4990190267562866\n",
      "Epoch 4/10, Batch Loss: 0.7870392203330994\n",
      "Epoch 4/10, Batch Loss: 0.611170768737793\n",
      "Epoch 4/10, Batch Loss: 0.4916897714138031\n",
      "Epoch 4/10, Batch Loss: 0.5693162679672241\n",
      "Epoch 4/10, Batch Loss: 0.9373021125793457\n",
      "Epoch 4/10, Batch Loss: 0.5734056830406189\n",
      "Epoch 4/10, Batch Loss: 0.68220454454422\n",
      "Epoch 4/10, Batch Loss: 0.5439709424972534\n",
      "Epoch 4/10, Batch Loss: 0.5489723682403564\n",
      "Epoch 4/10, Batch Loss: 0.6301639080047607\n",
      "Epoch 4/10, Batch Loss: 0.5873726010322571\n",
      "Epoch 4/10, Batch Loss: 0.7288423776626587\n",
      "Epoch 4/10, Batch Loss: 0.488876074552536\n",
      "Epoch 4/10, Batch Loss: 0.614873468875885\n",
      "Epoch 4/10, Batch Loss: 0.5568081140518188\n",
      "Epoch 4/10, Batch Loss: 0.6350546479225159\n",
      "Epoch 4/10, Batch Loss: 0.5535842776298523\n",
      "Epoch 4/10, Batch Loss: 0.7688738703727722\n",
      "Epoch 4/10, Batch Loss: 0.5425377488136292\n",
      "Epoch 4/10, Batch Loss: 0.4275917112827301\n",
      "Epoch 4/10, Batch Loss: 0.4433309733867645\n",
      "Epoch 4/10, Batch Loss: 0.49017345905303955\n",
      "Epoch 4/10, Batch Loss: 0.6495320796966553\n",
      "Epoch 4/10, Batch Loss: 0.5690370202064514\n",
      "Epoch 4/10, Batch Loss: 0.4585818648338318\n",
      "Epoch 4/10, Batch Loss: 0.5745239853858948\n",
      "Epoch 4/10, Batch Loss: 0.6317868828773499\n",
      "Epoch 4/10, Batch Loss: 0.8130455613136292\n",
      "Epoch 4/10, Batch Loss: 0.5530291795730591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Batch Loss: 0.3202593922615051\n",
      "Epoch 4/10, Batch Loss: 0.641838014125824\n",
      "Epoch 4/10, Batch Loss: 0.5764975547790527\n",
      "Epoch 4/10, Batch Loss: 0.6660592555999756\n",
      "Epoch 4/10, Batch Loss: 0.6024842262268066\n",
      "Epoch 4/10, Batch Loss: 0.7287573218345642\n",
      "Epoch 4/10, Batch Loss: 0.6651892066001892\n",
      "Epoch 4/10, Batch Loss: 0.6049145460128784\n",
      "Epoch 4/10, Batch Loss: 0.5407819151878357\n",
      "Epoch 4/10, Batch Loss: 0.5532221794128418\n",
      "Epoch 4/10, Batch Loss: 0.6237477660179138\n",
      "Epoch 4/10, Batch Loss: 0.5427470803260803\n",
      "Epoch 4/10, Batch Loss: 0.5326001048088074\n",
      "Epoch 4/10, Batch Loss: 0.6019906997680664\n",
      "Epoch 4/10, Batch Loss: 0.5432001948356628\n",
      "Epoch 4/10, Batch Loss: 0.49850064516067505\n",
      "Epoch 4/10, Batch Loss: 0.40291303396224976\n",
      "Epoch 4/10, Batch Loss: 0.517487645149231\n",
      "Epoch 4/10, Batch Loss: 0.6384865045547485\n",
      "Epoch 4/10, Batch Loss: 0.4873877167701721\n",
      "Epoch 4/10, Batch Loss: 0.6392702460289001\n",
      "Epoch 4/10, Batch Loss: 0.6966561675071716\n",
      "Epoch 4/10, Batch Loss: 0.5236292481422424\n",
      "Epoch 4/10, Batch Loss: 0.56470787525177\n",
      "Epoch 4/10, Batch Loss: 0.6947665214538574\n",
      "Epoch 4/10, Batch Loss: 0.602598249912262\n",
      "Epoch 4/10, Batch Loss: 0.7749781012535095\n",
      "Epoch 4/10, Batch Loss: 0.46585071086883545\n",
      "Epoch 4/10, Batch Loss: 0.8365163803100586\n",
      "Epoch 4/10, Batch Loss: 0.45644325017929077\n",
      "Epoch 4/10, Batch Loss: 0.5358660817146301\n",
      "Epoch 4/10, Batch Loss: 0.3718983232975006\n",
      "Epoch 4/10, Batch Loss: 0.5319424271583557\n",
      "Epoch 4/10, Batch Loss: 0.4787105321884155\n",
      "Epoch 4/10, Batch Loss: 0.8294724225997925\n",
      "Epoch 4/10, Batch Loss: 0.46564215421676636\n",
      "Epoch 4/10, Batch Loss: 0.8004505634307861\n",
      "Epoch 4/10, Batch Loss: 0.4598696827888489\n",
      "Epoch 4/10, Batch Loss: 0.5882567763328552\n",
      "Epoch 4/10, Batch Loss: 0.7274954319000244\n",
      "Epoch 4/10, Batch Loss: 0.5978032946586609\n",
      "Epoch 4/10, Batch Loss: 0.6621140837669373\n",
      "Epoch 4/10, Batch Loss: 0.6513075828552246\n",
      "Epoch 4/10, Batch Loss: 0.6295672059059143\n",
      "Epoch 4/10, Batch Loss: 0.5373346209526062\n",
      "Epoch 4/10, Batch Loss: 0.6819449663162231\n",
      "Epoch 4/10, Batch Loss: 0.4166914224624634\n",
      "Epoch 4/10, Batch Loss: 0.678586483001709\n",
      "Epoch 4/10, Batch Loss: 0.644197940826416\n",
      "Epoch 4/10, Batch Loss: 0.6442411541938782\n",
      "Epoch 4/10, Batch Loss: 0.8593884706497192\n",
      "Epoch 4/10, Batch Loss: 0.6119678616523743\n",
      "Epoch 4/10, Batch Loss: 0.5401632785797119\n",
      "Epoch 4/10, Batch Loss: 0.48492833971977234\n",
      "Epoch 4/10, Batch Loss: 0.7978740930557251\n",
      "Epoch 4/10, Batch Loss: 0.594524085521698\n",
      "Epoch 4/10, Batch Loss: 0.4521578848361969\n",
      "Epoch 4/10, Batch Loss: 0.5613650679588318\n",
      "Epoch 4/10, Batch Loss: 0.47681787610054016\n",
      "Epoch 4/10, Batch Loss: 0.6022636294364929\n",
      "Epoch 4/10, Batch Loss: 0.6181852221488953\n",
      "Epoch 4/10, Batch Loss: 0.6652212738990784\n",
      "Epoch 4/10, Batch Loss: 0.5424100160598755\n",
      "Epoch 4/10, Batch Loss: 0.6797069311141968\n",
      "Epoch 4/10, Batch Loss: 0.6670399904251099\n",
      "Epoch 4/10, Batch Loss: 0.654485821723938\n",
      "Epoch 4/10, Batch Loss: 0.6265461444854736\n",
      "Epoch 4/10, Batch Loss: 0.6254976987838745\n",
      "Epoch 4/10, Batch Loss: 0.6852459907531738\n",
      "Epoch 4/10, Batch Loss: 0.8680548667907715\n",
      "Epoch 4/10, Batch Loss: 0.5033748149871826\n",
      "Epoch 4/10, Batch Loss: 0.4867995083332062\n",
      "Epoch 4/10, Batch Loss: 0.47716662287712097\n",
      "Epoch 4/10, Batch Loss: 0.5859910845756531\n",
      "Epoch 4/10, Batch Loss: 0.6862331032752991\n",
      "Epoch 4/10, Batch Loss: 0.5813491940498352\n",
      "Epoch 4/10, Batch Loss: 0.7644026875495911\n",
      "Epoch 4/10, Batch Loss: 0.6383970379829407\n",
      "Epoch 4/10, Batch Loss: 0.46239250898361206\n",
      "Epoch 4/10, Batch Loss: 0.4875798225402832\n",
      "Epoch 4/10, Batch Loss: 0.3717551827430725\n",
      "Epoch 4/10, Batch Loss: 0.5099446773529053\n",
      "Epoch 4/10, Batch Loss: 0.498054176568985\n",
      "Epoch 4/10, Batch Loss: 0.5610713958740234\n",
      "Epoch 4/10, Batch Loss: 0.7293927073478699\n",
      "Epoch 4/10, Batch Loss: 0.44900089502334595\n",
      "Epoch 4/10, Batch Loss: 0.4485229253768921\n",
      "Epoch 4/10, Batch Loss: 0.44160836935043335\n",
      "Epoch 4/10, Batch Loss: 0.635306179523468\n",
      "Epoch 4/10, Batch Loss: 0.23224453628063202\n",
      "Epoch 4/10, Batch Loss: 0.5932838320732117\n",
      "Epoch 4/10, Batch Loss: 0.5015314817428589\n",
      "Epoch 4/10, Batch Loss: 0.5606515407562256\n",
      "Epoch 4/10, Batch Loss: 0.5862779021263123\n",
      "Epoch 4/10, Batch Loss: 0.46767735481262207\n",
      "Epoch 4/10, Batch Loss: 0.37416931986808777\n",
      "Epoch 4/10, Batch Loss: 0.5778732895851135\n",
      "Epoch 4/10, Batch Loss: 0.3023620843887329\n",
      "Epoch 4/10, Batch Loss: 0.41472482681274414\n",
      "Epoch 4/10, Batch Loss: 1.029541254043579\n",
      "Epoch 4/10, Batch Loss: 0.38253092765808105\n",
      "Epoch 4/10, Batch Loss: 1.0276424884796143\n",
      "Epoch 4/10, Batch Loss: 0.35336217284202576\n",
      "Epoch 4/10, Batch Loss: 0.7719024419784546\n",
      "Epoch 4/10, Batch Loss: 0.3771873712539673\n",
      "Epoch 4/10, Batch Loss: 0.5774922370910645\n",
      "Epoch 4/10, Batch Loss: 0.5934727191925049\n",
      "Epoch 4/10, Batch Loss: 0.4848003089427948\n",
      "Epoch 4/10, Batch Loss: 0.8553939461708069\n",
      "Epoch 4/10, Batch Loss: 0.5701769590377808\n",
      "Epoch 4/10, Batch Loss: 0.5766910910606384\n",
      "Epoch 4/10, Batch Loss: 0.6797496676445007\n",
      "Epoch 4/10, Batch Loss: 0.3256451189517975\n",
      "Epoch 4/10, Batch Loss: 0.5267421007156372\n",
      "Epoch 4/10, Batch Loss: 0.6039595007896423\n",
      "Epoch 4/10, Batch Loss: 0.43408286571502686\n",
      "Epoch 4/10, Batch Loss: 0.6662054657936096\n",
      "Epoch 4/10, Batch Loss: 0.49925994873046875\n",
      "Epoch 4/10, Batch Loss: 0.7328718304634094\n",
      "Epoch 4/10, Batch Loss: 0.6292579770088196\n",
      "Epoch 4/10, Batch Loss: 0.6688898205757141\n",
      "Epoch 4/10, Batch Loss: 0.28928786516189575\n",
      "Epoch 4/10, Batch Loss: 0.4018409550189972\n",
      "Epoch 4/10, Batch Loss: 0.5941027998924255\n",
      "Epoch 4/10, Batch Loss: 0.5562496781349182\n",
      "Epoch 4/10, Batch Loss: 0.5475769639015198\n",
      "Epoch 4/10, Batch Loss: 0.6044442057609558\n",
      "Epoch 4/10, Batch Loss: 0.46929606795310974\n",
      "Epoch 4/10, Batch Loss: 0.8156583309173584\n",
      "Epoch 4/10, Batch Loss: 0.5996102094650269\n",
      "Epoch 4/10, Batch Loss: 0.5885072946548462\n",
      "Epoch 4/10, Batch Loss: 0.5771763324737549\n",
      "Epoch 4/10, Batch Loss: 0.6997562050819397\n",
      "Epoch 4/10, Batch Loss: 0.5060871243476868\n",
      "Epoch 4/10, Batch Loss: 0.6269295811653137\n",
      "Epoch 4/10, Batch Loss: 0.6061859726905823\n",
      "Epoch 4/10, Batch Loss: 0.6756293177604675\n",
      "Epoch 4/10, Batch Loss: 0.6423230171203613\n",
      "Epoch 4/10, Batch Loss: 0.3617141544818878\n",
      "Epoch 4/10, Batch Loss: 0.7130168080329895\n",
      "Epoch 4/10, Batch Loss: 0.6217189431190491\n",
      "Epoch 4/10, Batch Loss: 0.8429654240608215\n",
      "Epoch 4/10, Batch Loss: 0.5263023376464844\n",
      "Epoch 4/10, Batch Loss: 0.615024209022522\n",
      "Epoch 4/10, Batch Loss: 0.762798011302948\n",
      "Epoch 4/10, Batch Loss: 0.7839795351028442\n",
      "Epoch 4/10, Batch Loss: 0.8219985961914062\n",
      "Epoch 4/10, Batch Loss: 0.6084067821502686\n",
      "Epoch 4/10, Batch Loss: 0.4103483259677887\n",
      "Epoch 4/10, Batch Loss: 0.5173367857933044\n",
      "Epoch 4/10, Batch Loss: 0.5992869138717651\n",
      "Epoch 4/10, Batch Loss: 0.5460159182548523\n",
      "Epoch 4/10, Batch Loss: 0.6708310842514038\n",
      "Epoch 4/10, Batch Loss: 0.5367124080657959\n",
      "Epoch 4/10, Batch Loss: 0.5760558843612671\n",
      "Epoch 4/10, Batch Loss: 0.4971197545528412\n",
      "Epoch 4/10, Batch Loss: 0.7908374667167664\n",
      "Epoch 4/10, Batch Loss: 0.7021505832672119\n",
      "Epoch 4/10, Batch Loss: 0.6136177182197571\n",
      "Epoch 4/10, Batch Loss: 0.5473707318305969\n",
      "Epoch 4/10, Batch Loss: 0.7442070841789246\n",
      "Epoch 4/10, Batch Loss: 0.5747073888778687\n",
      "Epoch 4/10, Batch Loss: 0.8325316905975342\n",
      "Epoch 4/10, Batch Loss: 0.6493833065032959\n",
      "Epoch 4/10, Batch Loss: 0.7118082046508789\n",
      "Epoch 4/10, Batch Loss: 0.621859610080719\n",
      "Epoch 4/10, Batch Loss: 0.5490477681159973\n",
      "Epoch 4/10, Batch Loss: 0.7612773776054382\n",
      "Epoch 4/10, Batch Loss: 0.7118202447891235\n",
      "Epoch 4/10, Batch Loss: 0.652762770652771\n",
      "Epoch 4/10, Batch Loss: 0.5515109300613403\n",
      "Epoch 4/10, Batch Loss: 0.7463939785957336\n",
      "Epoch 4/10, Batch Loss: 0.6966884732246399\n",
      "Epoch 4/10, Batch Loss: 0.5385329723358154\n",
      "Epoch 4/10, Batch Loss: 0.827427089214325\n",
      "Epoch 4/10, Batch Loss: 0.5803319811820984\n",
      "Epoch 4/10, Batch Loss: 0.7317306995391846\n",
      "Epoch 4/10, Batch Loss: 0.5800402760505676\n",
      "Epoch 4/10, Batch Loss: 0.7383647561073303\n",
      "Epoch 4/10, Batch Loss: 0.5372992157936096\n",
      "Epoch 4/10, Batch Loss: 0.5424370169639587\n",
      "Epoch 4/10, Batch Loss: 0.7335686087608337\n",
      "Epoch 4/10, Batch Loss: 0.5910240411758423\n",
      "Epoch 4/10, Batch Loss: 0.45135387778282166\n",
      "Epoch 4/10, Batch Loss: 0.48406076431274414\n",
      "Epoch 4/10, Batch Loss: 0.55205899477005\n",
      "Epoch 4/10, Batch Loss: 0.5407034754753113\n",
      "Epoch 4/10, Batch Loss: 0.6206414699554443\n",
      "Epoch 4/10, Batch Loss: 0.5492572784423828\n",
      "Epoch 4/10, Batch Loss: 0.6428321599960327\n",
      "Epoch 4/10, Batch Loss: 0.6510089039802551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Batch Loss: 0.4831252098083496\n",
      "Epoch 4/10, Batch Loss: 0.5784982442855835\n",
      "Epoch 4/10, Batch Loss: 0.4964717626571655\n",
      "Epoch 4/10, Batch Loss: 0.7707061171531677\n",
      "Epoch 4/10, Batch Loss: 0.7003787159919739\n",
      "Epoch 4/10, Batch Loss: 0.656517744064331\n",
      "Epoch 4/10, Batch Loss: 0.83979332447052\n",
      "Epoch 4/10, Batch Loss: 0.6652956008911133\n",
      "Epoch 4/10, Batch Loss: 0.3633812665939331\n",
      "Epoch 4/10, Average Training Loss: 0.598670219626505\n",
      "Model saved for epoch 4 at Pre_train_epoch4.pt\n",
      "Validation Accuracy for epoch 4: 0.7276368491321763\n",
      "Epoch 5/10, Batch Loss: 0.3972143530845642\n",
      "Epoch 5/10, Batch Loss: 0.7090664505958557\n",
      "Epoch 5/10, Batch Loss: 0.38202178478240967\n",
      "Epoch 5/10, Batch Loss: 0.6078557372093201\n",
      "Epoch 5/10, Batch Loss: 0.7215650677680969\n",
      "Epoch 5/10, Batch Loss: 0.6081039905548096\n",
      "Epoch 5/10, Batch Loss: 0.5581713914871216\n",
      "Epoch 5/10, Batch Loss: 0.6698586940765381\n",
      "Epoch 5/10, Batch Loss: 0.5992743372917175\n",
      "Epoch 5/10, Batch Loss: 0.6496667265892029\n",
      "Epoch 5/10, Batch Loss: 0.7686629295349121\n",
      "Epoch 5/10, Batch Loss: 0.6307046413421631\n",
      "Epoch 5/10, Batch Loss: 0.5466838479042053\n",
      "Epoch 5/10, Batch Loss: 0.5410158634185791\n",
      "Epoch 5/10, Batch Loss: 0.5938889384269714\n",
      "Epoch 5/10, Batch Loss: 0.509842038154602\n",
      "Epoch 5/10, Batch Loss: 0.7279841899871826\n",
      "Epoch 5/10, Batch Loss: 0.3784196972846985\n",
      "Epoch 5/10, Batch Loss: 0.5510062575340271\n",
      "Epoch 5/10, Batch Loss: 0.41655629873275757\n",
      "Epoch 5/10, Batch Loss: 0.514355480670929\n",
      "Epoch 5/10, Batch Loss: 0.460973858833313\n",
      "Epoch 5/10, Batch Loss: 0.4532668888568878\n",
      "Epoch 5/10, Batch Loss: 0.6512232422828674\n",
      "Epoch 5/10, Batch Loss: 0.7173315286636353\n",
      "Epoch 5/10, Batch Loss: 0.7720244526863098\n",
      "Epoch 5/10, Batch Loss: 0.5697009563446045\n",
      "Epoch 5/10, Batch Loss: 0.5298913717269897\n",
      "Epoch 5/10, Batch Loss: 0.6332417726516724\n",
      "Epoch 5/10, Batch Loss: 0.4842503070831299\n",
      "Epoch 5/10, Batch Loss: 0.8280203938484192\n",
      "Epoch 5/10, Batch Loss: 0.7067893147468567\n",
      "Epoch 5/10, Batch Loss: 0.44437140226364136\n",
      "Epoch 5/10, Batch Loss: 0.6221840381622314\n",
      "Epoch 5/10, Batch Loss: 0.7559460997581482\n",
      "Epoch 5/10, Batch Loss: 0.5343839526176453\n",
      "Epoch 5/10, Batch Loss: 0.633738100528717\n",
      "Epoch 5/10, Batch Loss: 0.6185739636421204\n",
      "Epoch 5/10, Batch Loss: 0.6821728944778442\n",
      "Epoch 5/10, Batch Loss: 0.678091287612915\n",
      "Epoch 5/10, Batch Loss: 0.6655123829841614\n",
      "Epoch 5/10, Batch Loss: 0.5729371905326843\n",
      "Epoch 5/10, Batch Loss: 0.7566073536872864\n",
      "Epoch 5/10, Batch Loss: 0.42334574460983276\n",
      "Epoch 5/10, Batch Loss: 0.3713749349117279\n",
      "Epoch 5/10, Batch Loss: 0.49634742736816406\n",
      "Epoch 5/10, Batch Loss: 0.3713394105434418\n",
      "Epoch 5/10, Batch Loss: 0.47818678617477417\n",
      "Epoch 5/10, Batch Loss: 0.3397192060947418\n",
      "Epoch 5/10, Batch Loss: 0.5124797821044922\n",
      "Epoch 5/10, Batch Loss: 0.7452983260154724\n",
      "Epoch 5/10, Batch Loss: 0.42962726950645447\n",
      "Epoch 5/10, Batch Loss: 0.7484223246574402\n",
      "Epoch 5/10, Batch Loss: 0.4419991374015808\n",
      "Epoch 5/10, Batch Loss: 0.578690767288208\n",
      "Epoch 5/10, Batch Loss: 0.5934420824050903\n",
      "Epoch 5/10, Batch Loss: 0.7430095076560974\n",
      "Epoch 5/10, Batch Loss: 0.7466521263122559\n",
      "Epoch 5/10, Batch Loss: 0.6214278340339661\n",
      "Epoch 5/10, Batch Loss: 0.5374393463134766\n",
      "Epoch 5/10, Batch Loss: 0.5842422842979431\n",
      "Epoch 5/10, Batch Loss: 0.6758525967597961\n",
      "Epoch 5/10, Batch Loss: 0.5478163361549377\n",
      "Epoch 5/10, Batch Loss: 0.7870528697967529\n",
      "Epoch 5/10, Batch Loss: 0.5449051260948181\n",
      "Epoch 5/10, Batch Loss: 0.6178139448165894\n",
      "Epoch 5/10, Batch Loss: 0.6951417326927185\n",
      "Epoch 5/10, Batch Loss: 0.6490863561630249\n",
      "Epoch 5/10, Batch Loss: 0.5287887454032898\n",
      "Epoch 5/10, Batch Loss: 0.5519344210624695\n",
      "Epoch 5/10, Batch Loss: 0.9409297108650208\n",
      "Epoch 5/10, Batch Loss: 0.551426112651825\n",
      "Epoch 5/10, Batch Loss: 0.7360595464706421\n",
      "Epoch 5/10, Batch Loss: 0.4331909120082855\n",
      "Epoch 5/10, Batch Loss: 0.576881468296051\n",
      "Epoch 5/10, Batch Loss: 0.718053936958313\n",
      "Epoch 5/10, Batch Loss: 0.5634684562683105\n",
      "Epoch 5/10, Batch Loss: 0.5428943634033203\n",
      "Epoch 5/10, Batch Loss: 0.535872220993042\n",
      "Epoch 5/10, Batch Loss: 0.5826429724693298\n",
      "Epoch 5/10, Batch Loss: 0.6058769822120667\n",
      "Epoch 5/10, Batch Loss: 0.6663105487823486\n",
      "Epoch 5/10, Batch Loss: 0.6569505929946899\n",
      "Epoch 5/10, Batch Loss: 0.951458752155304\n",
      "Epoch 5/10, Batch Loss: 0.8129541277885437\n",
      "Epoch 5/10, Batch Loss: 0.5765155553817749\n",
      "Epoch 5/10, Batch Loss: 0.6242669820785522\n",
      "Epoch 5/10, Batch Loss: 0.6837362051010132\n",
      "Epoch 5/10, Batch Loss: 0.4540563225746155\n",
      "Epoch 5/10, Batch Loss: 0.6207048892974854\n",
      "Epoch 5/10, Batch Loss: 0.6607221364974976\n",
      "Epoch 5/10, Batch Loss: 0.5884254574775696\n",
      "Epoch 5/10, Batch Loss: 0.5409387946128845\n",
      "Epoch 5/10, Batch Loss: 0.6573376059532166\n",
      "Epoch 5/10, Batch Loss: 0.7683730721473694\n",
      "Epoch 5/10, Batch Loss: 0.707020103931427\n",
      "Epoch 5/10, Batch Loss: 0.6241120100021362\n",
      "Epoch 5/10, Batch Loss: 0.70408034324646\n",
      "Epoch 5/10, Batch Loss: 0.620603084564209\n",
      "Epoch 5/10, Batch Loss: 0.49094587564468384\n",
      "Epoch 5/10, Batch Loss: 0.6208862662315369\n",
      "Epoch 5/10, Batch Loss: 0.7250270843505859\n",
      "Epoch 5/10, Batch Loss: 0.5756928324699402\n",
      "Epoch 5/10, Batch Loss: 0.6326337456703186\n",
      "Epoch 5/10, Batch Loss: 0.7473574876785278\n",
      "Epoch 5/10, Batch Loss: 0.6087884902954102\n",
      "Epoch 5/10, Batch Loss: 0.5344029664993286\n",
      "Epoch 5/10, Batch Loss: 0.6314931511878967\n",
      "Epoch 5/10, Batch Loss: 0.681281328201294\n",
      "Epoch 5/10, Batch Loss: 0.5379349589347839\n",
      "Epoch 5/10, Batch Loss: 0.46202731132507324\n",
      "Epoch 5/10, Batch Loss: 0.4012487828731537\n",
      "Epoch 5/10, Batch Loss: 0.648716151714325\n",
      "Epoch 5/10, Batch Loss: 0.6164999008178711\n",
      "Epoch 5/10, Batch Loss: 0.5011488795280457\n",
      "Epoch 5/10, Batch Loss: 0.6501051783561707\n",
      "Epoch 5/10, Batch Loss: 0.5230419635772705\n",
      "Epoch 5/10, Batch Loss: 0.6025220155715942\n",
      "Epoch 5/10, Batch Loss: 0.6119113564491272\n",
      "Epoch 5/10, Batch Loss: 0.5070381760597229\n",
      "Epoch 5/10, Batch Loss: 0.466671347618103\n",
      "Epoch 5/10, Batch Loss: 0.676888644695282\n",
      "Epoch 5/10, Batch Loss: 0.7195754647254944\n",
      "Epoch 5/10, Batch Loss: 0.6930063366889954\n",
      "Epoch 5/10, Batch Loss: 0.5926790237426758\n",
      "Epoch 5/10, Batch Loss: 0.6002519726753235\n",
      "Epoch 5/10, Batch Loss: 0.6757625341415405\n",
      "Epoch 5/10, Batch Loss: 0.49713706970214844\n",
      "Epoch 5/10, Batch Loss: 0.5829275250434875\n",
      "Epoch 5/10, Batch Loss: 0.5891802906990051\n",
      "Epoch 5/10, Batch Loss: 0.5908547639846802\n",
      "Epoch 5/10, Batch Loss: 0.6938557028770447\n",
      "Epoch 5/10, Batch Loss: 0.8168958425521851\n",
      "Epoch 5/10, Batch Loss: 0.7135029435157776\n",
      "Epoch 5/10, Batch Loss: 0.74728924036026\n",
      "Epoch 5/10, Batch Loss: 0.4605756103992462\n",
      "Epoch 5/10, Batch Loss: 0.5948989391326904\n",
      "Epoch 5/10, Batch Loss: 0.5996677875518799\n",
      "Epoch 5/10, Batch Loss: 0.7448278069496155\n",
      "Epoch 5/10, Batch Loss: 0.6132689714431763\n",
      "Epoch 5/10, Batch Loss: 0.5040172338485718\n",
      "Epoch 5/10, Batch Loss: 0.6256054043769836\n",
      "Epoch 5/10, Batch Loss: 0.6653043627738953\n",
      "Epoch 5/10, Batch Loss: 0.9014115333557129\n",
      "Epoch 5/10, Batch Loss: 0.5876107811927795\n",
      "Epoch 5/10, Batch Loss: 0.7298981547355652\n",
      "Epoch 5/10, Batch Loss: 0.6736825108528137\n",
      "Epoch 5/10, Batch Loss: 0.6145291328430176\n",
      "Epoch 5/10, Batch Loss: 0.810835063457489\n",
      "Epoch 5/10, Batch Loss: 0.5642419457435608\n",
      "Epoch 5/10, Batch Loss: 0.405318945646286\n",
      "Epoch 5/10, Batch Loss: 0.6138350367546082\n",
      "Epoch 5/10, Batch Loss: 0.5476546287536621\n",
      "Epoch 5/10, Batch Loss: 0.6052285432815552\n",
      "Epoch 5/10, Batch Loss: 0.556704044342041\n",
      "Epoch 5/10, Batch Loss: 0.6377338767051697\n",
      "Epoch 5/10, Batch Loss: 0.55696040391922\n",
      "Epoch 5/10, Batch Loss: 0.486151784658432\n",
      "Epoch 5/10, Batch Loss: 0.47509005665779114\n",
      "Epoch 5/10, Batch Loss: 0.3913520574569702\n",
      "Epoch 5/10, Batch Loss: 0.512824296951294\n",
      "Epoch 5/10, Batch Loss: 0.5409006476402283\n",
      "Epoch 5/10, Batch Loss: 0.5435447692871094\n",
      "Epoch 5/10, Batch Loss: 0.658338189125061\n",
      "Epoch 5/10, Batch Loss: 0.5673604607582092\n",
      "Epoch 5/10, Batch Loss: 0.7334953546524048\n",
      "Epoch 5/10, Batch Loss: 0.5401894450187683\n",
      "Epoch 5/10, Batch Loss: 0.7415098547935486\n",
      "Epoch 5/10, Batch Loss: 0.7925426363945007\n",
      "Epoch 5/10, Batch Loss: 0.40743935108184814\n",
      "Epoch 5/10, Batch Loss: 0.4528385102748871\n",
      "Epoch 5/10, Batch Loss: 0.7837479114532471\n",
      "Epoch 5/10, Batch Loss: 0.649433970451355\n",
      "Epoch 5/10, Batch Loss: 0.5618888735771179\n",
      "Epoch 5/10, Batch Loss: 0.6988657712936401\n",
      "Epoch 5/10, Batch Loss: 0.37236663699150085\n",
      "Epoch 5/10, Batch Loss: 0.5468317270278931\n",
      "Epoch 5/10, Batch Loss: 0.4326258599758148\n",
      "Epoch 5/10, Batch Loss: 0.4860411584377289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Batch Loss: 0.837300717830658\n",
      "Epoch 5/10, Batch Loss: 0.5587681531906128\n",
      "Epoch 5/10, Batch Loss: 0.592712938785553\n",
      "Epoch 5/10, Batch Loss: 0.5983709096908569\n",
      "Epoch 5/10, Batch Loss: 0.6030000448226929\n",
      "Epoch 5/10, Batch Loss: 0.3380046784877777\n",
      "Epoch 5/10, Batch Loss: 0.42045944929122925\n",
      "Epoch 5/10, Batch Loss: 0.5578382015228271\n",
      "Epoch 5/10, Batch Loss: 0.7255436778068542\n",
      "Epoch 5/10, Batch Loss: 0.7206558585166931\n",
      "Epoch 5/10, Batch Loss: 0.6045880913734436\n",
      "Epoch 5/10, Batch Loss: 0.7149237990379333\n",
      "Epoch 5/10, Batch Loss: 0.2685834467411041\n",
      "Epoch 5/10, Batch Loss: 0.6010447144508362\n",
      "Epoch 5/10, Batch Loss: 0.7089847326278687\n",
      "Epoch 5/10, Batch Loss: 0.7819063663482666\n",
      "Epoch 5/10, Batch Loss: 0.5732846260070801\n",
      "Epoch 5/10, Batch Loss: 0.5870869159698486\n",
      "Epoch 5/10, Batch Loss: 0.56510990858078\n",
      "Epoch 5/10, Batch Loss: 0.5014272928237915\n",
      "Epoch 5/10, Batch Loss: 0.8660175204277039\n",
      "Epoch 5/10, Batch Loss: 0.6429541110992432\n",
      "Epoch 5/10, Batch Loss: 0.5540784597396851\n",
      "Epoch 5/10, Batch Loss: 0.6560501456260681\n",
      "Epoch 5/10, Batch Loss: 0.770514965057373\n",
      "Epoch 5/10, Batch Loss: 0.41930001974105835\n",
      "Epoch 5/10, Batch Loss: 0.6268102526664734\n",
      "Epoch 5/10, Batch Loss: 0.5100696086883545\n",
      "Epoch 5/10, Batch Loss: 0.7159586548805237\n",
      "Epoch 5/10, Batch Loss: 0.5805489420890808\n",
      "Epoch 5/10, Batch Loss: 0.40039652585983276\n",
      "Epoch 5/10, Batch Loss: 0.4484976828098297\n",
      "Epoch 5/10, Batch Loss: 0.6582636833190918\n",
      "Epoch 5/10, Batch Loss: 0.6027679443359375\n",
      "Epoch 5/10, Batch Loss: 0.5937637090682983\n",
      "Epoch 5/10, Batch Loss: 0.5251836180686951\n",
      "Epoch 5/10, Batch Loss: 0.44310152530670166\n",
      "Epoch 5/10, Batch Loss: 0.7183191776275635\n",
      "Epoch 5/10, Batch Loss: 0.4430578052997589\n",
      "Epoch 5/10, Batch Loss: 0.6353466510772705\n",
      "Epoch 5/10, Batch Loss: 0.72823166847229\n",
      "Epoch 5/10, Batch Loss: 0.6510850787162781\n",
      "Epoch 5/10, Batch Loss: 0.31749287247657776\n",
      "Epoch 5/10, Batch Loss: 0.6507405042648315\n",
      "Epoch 5/10, Batch Loss: 0.594413161277771\n",
      "Epoch 5/10, Batch Loss: 0.8832910060882568\n",
      "Epoch 5/10, Batch Loss: 0.5899629592895508\n",
      "Epoch 5/10, Batch Loss: 0.5410452485084534\n",
      "Epoch 5/10, Batch Loss: 0.435321569442749\n",
      "Epoch 5/10, Batch Loss: 0.4962323009967804\n",
      "Epoch 5/10, Batch Loss: 0.8068533539772034\n",
      "Epoch 5/10, Batch Loss: 0.6499548554420471\n",
      "Epoch 5/10, Batch Loss: 0.653817355632782\n",
      "Epoch 5/10, Batch Loss: 0.7392974495887756\n",
      "Epoch 5/10, Batch Loss: 0.47291433811187744\n",
      "Epoch 5/10, Batch Loss: 0.6010549068450928\n",
      "Epoch 5/10, Batch Loss: 0.6418566703796387\n",
      "Epoch 5/10, Batch Loss: 0.6129328608512878\n",
      "Epoch 5/10, Batch Loss: 0.889304518699646\n",
      "Epoch 5/10, Batch Loss: 0.8412797451019287\n",
      "Epoch 5/10, Batch Loss: 0.47160184383392334\n",
      "Epoch 5/10, Batch Loss: 0.46216487884521484\n",
      "Epoch 5/10, Batch Loss: 0.6493573784828186\n",
      "Epoch 5/10, Batch Loss: 0.5883183479309082\n",
      "Epoch 5/10, Batch Loss: 0.8766172528266907\n",
      "Epoch 5/10, Batch Loss: 0.6517165303230286\n",
      "Epoch 5/10, Batch Loss: 0.48223432898521423\n",
      "Epoch 5/10, Batch Loss: 0.5707934498786926\n",
      "Epoch 5/10, Batch Loss: 0.5880078077316284\n",
      "Epoch 5/10, Batch Loss: 0.49709421396255493\n",
      "Epoch 5/10, Batch Loss: 0.6989529132843018\n",
      "Epoch 5/10, Batch Loss: 0.6973772644996643\n",
      "Epoch 5/10, Batch Loss: 0.4744834899902344\n",
      "Epoch 5/10, Batch Loss: 0.4344109296798706\n",
      "Epoch 5/10, Batch Loss: 0.6104891300201416\n",
      "Epoch 5/10, Batch Loss: 0.7456397414207458\n",
      "Epoch 5/10, Batch Loss: 0.5073909163475037\n",
      "Epoch 5/10, Batch Loss: 0.6532411575317383\n",
      "Epoch 5/10, Batch Loss: 0.39963531494140625\n",
      "Epoch 5/10, Batch Loss: 0.8237208127975464\n",
      "Epoch 5/10, Batch Loss: 0.6937885284423828\n",
      "Epoch 5/10, Batch Loss: 0.6469991207122803\n",
      "Epoch 5/10, Batch Loss: 0.5157472491264343\n",
      "Epoch 5/10, Batch Loss: 0.6306090354919434\n",
      "Epoch 5/10, Batch Loss: 0.705233633518219\n",
      "Epoch 5/10, Batch Loss: 0.7438842058181763\n",
      "Epoch 5/10, Batch Loss: 0.5598256587982178\n",
      "Epoch 5/10, Batch Loss: 0.6241429448127747\n",
      "Epoch 5/10, Batch Loss: 0.7611516714096069\n",
      "Epoch 5/10, Batch Loss: 0.4333381652832031\n",
      "Epoch 5/10, Batch Loss: 0.8001063466072083\n",
      "Epoch 5/10, Batch Loss: 0.6048835515975952\n",
      "Epoch 5/10, Batch Loss: 0.6870659589767456\n",
      "Epoch 5/10, Batch Loss: 0.41935211420059204\n",
      "Epoch 5/10, Batch Loss: 0.4872201979160309\n",
      "Epoch 5/10, Batch Loss: 0.6490305662155151\n",
      "Epoch 5/10, Batch Loss: 0.7743564248085022\n",
      "Epoch 5/10, Batch Loss: 0.6265663504600525\n",
      "Epoch 5/10, Batch Loss: 0.7240535616874695\n",
      "Epoch 5/10, Batch Loss: 0.7369233965873718\n",
      "Epoch 5/10, Batch Loss: 0.6116276383399963\n",
      "Epoch 5/10, Batch Loss: 0.6906386613845825\n",
      "Epoch 5/10, Batch Loss: 0.8515176773071289\n",
      "Epoch 5/10, Batch Loss: 0.6421070694923401\n",
      "Epoch 5/10, Batch Loss: 0.547766387462616\n",
      "Epoch 5/10, Batch Loss: 0.5785604119300842\n",
      "Epoch 5/10, Batch Loss: 0.48701730370521545\n",
      "Epoch 5/10, Batch Loss: 0.5431579351425171\n",
      "Epoch 5/10, Batch Loss: 0.5365369915962219\n",
      "Epoch 5/10, Batch Loss: 0.6532258987426758\n",
      "Epoch 5/10, Batch Loss: 0.5335688591003418\n",
      "Epoch 5/10, Batch Loss: 0.4689207077026367\n",
      "Epoch 5/10, Batch Loss: 0.45343416929244995\n",
      "Epoch 5/10, Batch Loss: 0.6471168994903564\n",
      "Epoch 5/10, Batch Loss: 0.5767446160316467\n",
      "Epoch 5/10, Batch Loss: 0.5865997076034546\n",
      "Epoch 5/10, Batch Loss: 0.5716167688369751\n",
      "Epoch 5/10, Batch Loss: 0.5736415982246399\n",
      "Epoch 5/10, Batch Loss: 0.5610694885253906\n",
      "Epoch 5/10, Batch Loss: 0.5477215647697449\n",
      "Epoch 5/10, Batch Loss: 0.5564970970153809\n",
      "Epoch 5/10, Batch Loss: 0.6631582975387573\n",
      "Epoch 5/10, Batch Loss: 0.4233951270580292\n",
      "Epoch 5/10, Batch Loss: 0.6470171213150024\n",
      "Epoch 5/10, Batch Loss: 0.2958683371543884\n",
      "Epoch 5/10, Batch Loss: 0.6712654232978821\n",
      "Epoch 5/10, Batch Loss: 0.6744291186332703\n",
      "Epoch 5/10, Batch Loss: 0.4917534291744232\n",
      "Epoch 5/10, Batch Loss: 0.7367783188819885\n",
      "Epoch 5/10, Batch Loss: 0.812667965888977\n",
      "Epoch 5/10, Batch Loss: 0.45436492562294006\n",
      "Epoch 5/10, Batch Loss: 0.7342597246170044\n",
      "Epoch 5/10, Batch Loss: 0.5721049904823303\n",
      "Epoch 5/10, Batch Loss: 0.46750158071517944\n",
      "Epoch 5/10, Batch Loss: 0.390529602766037\n",
      "Epoch 5/10, Batch Loss: 0.693374752998352\n",
      "Epoch 5/10, Batch Loss: 0.8494852781295776\n",
      "Epoch 5/10, Batch Loss: 0.5109354257583618\n",
      "Epoch 5/10, Batch Loss: 0.5376489758491516\n",
      "Epoch 5/10, Batch Loss: 0.5212445259094238\n",
      "Epoch 5/10, Batch Loss: 0.5879952311515808\n",
      "Epoch 5/10, Batch Loss: 0.4870792031288147\n",
      "Epoch 5/10, Batch Loss: 0.7535189986228943\n",
      "Epoch 5/10, Batch Loss: 0.378772109746933\n",
      "Epoch 5/10, Batch Loss: 0.5836436748504639\n",
      "Epoch 5/10, Batch Loss: 0.4877610206604004\n",
      "Epoch 5/10, Batch Loss: 0.5820324420928955\n",
      "Epoch 5/10, Batch Loss: 0.5627830624580383\n",
      "Epoch 5/10, Batch Loss: 0.8559121489524841\n",
      "Epoch 5/10, Batch Loss: 0.6235733032226562\n",
      "Epoch 5/10, Batch Loss: 0.7585429549217224\n",
      "Epoch 5/10, Batch Loss: 0.5608458518981934\n",
      "Epoch 5/10, Batch Loss: 0.4286866784095764\n",
      "Epoch 5/10, Batch Loss: 0.7992176413536072\n",
      "Epoch 5/10, Batch Loss: 0.712856113910675\n",
      "Epoch 5/10, Batch Loss: 0.5546725988388062\n",
      "Epoch 5/10, Batch Loss: 0.41705647110939026\n",
      "Epoch 5/10, Batch Loss: 0.48681214451789856\n",
      "Epoch 5/10, Batch Loss: 0.4278913736343384\n",
      "Epoch 5/10, Batch Loss: 0.6414251923561096\n",
      "Epoch 5/10, Batch Loss: 0.6229496002197266\n",
      "Epoch 5/10, Batch Loss: 0.40561285614967346\n",
      "Epoch 5/10, Batch Loss: 0.5520576238632202\n",
      "Epoch 5/10, Batch Loss: 0.5873580574989319\n",
      "Epoch 5/10, Batch Loss: 0.5981589555740356\n",
      "Epoch 5/10, Batch Loss: 0.6321760416030884\n",
      "Epoch 5/10, Batch Loss: 0.45331722497940063\n",
      "Epoch 5/10, Batch Loss: 0.5799381732940674\n",
      "Epoch 5/10, Batch Loss: 0.47328129410743713\n",
      "Epoch 5/10, Batch Loss: 0.527595043182373\n",
      "Epoch 5/10, Batch Loss: 0.6118385791778564\n",
      "Epoch 5/10, Batch Loss: 0.7541037201881409\n",
      "Epoch 5/10, Batch Loss: 0.27755144238471985\n",
      "Epoch 5/10, Batch Loss: 0.4154285490512848\n",
      "Epoch 5/10, Batch Loss: 0.36669695377349854\n",
      "Epoch 5/10, Batch Loss: 0.39911285042762756\n",
      "Epoch 5/10, Batch Loss: 0.5748422741889954\n",
      "Epoch 5/10, Batch Loss: 0.6677758097648621\n",
      "Epoch 5/10, Batch Loss: 0.5907962918281555\n",
      "Epoch 5/10, Batch Loss: 0.5651929378509521\n",
      "Epoch 5/10, Batch Loss: 0.5707720518112183\n",
      "Epoch 5/10, Batch Loss: 0.6378960013389587\n",
      "Epoch 5/10, Batch Loss: 0.5678179264068604\n",
      "Epoch 5/10, Batch Loss: 0.7736737728118896\n",
      "Epoch 5/10, Batch Loss: 0.5460438132286072\n",
      "Epoch 5/10, Batch Loss: 0.663305401802063\n",
      "Epoch 5/10, Batch Loss: 0.537136971950531\n",
      "Epoch 5/10, Batch Loss: 0.5876995921134949\n",
      "Epoch 5/10, Batch Loss: 0.6748133897781372\n",
      "Epoch 5/10, Batch Loss: 0.7687955498695374\n",
      "Epoch 5/10, Batch Loss: 0.6341632604598999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Batch Loss: 0.567374587059021\n",
      "Epoch 5/10, Batch Loss: 0.506483256816864\n",
      "Epoch 5/10, Batch Loss: 0.48919951915740967\n",
      "Epoch 5/10, Batch Loss: 0.4945330023765564\n",
      "Epoch 5/10, Batch Loss: 0.659608781337738\n",
      "Epoch 5/10, Batch Loss: 0.5881980657577515\n",
      "Epoch 5/10, Batch Loss: 0.701559841632843\n",
      "Epoch 5/10, Batch Loss: 0.49371182918548584\n",
      "Epoch 5/10, Batch Loss: 0.47109732031822205\n",
      "Epoch 5/10, Batch Loss: 0.5792324542999268\n",
      "Epoch 5/10, Batch Loss: 0.6176326274871826\n",
      "Epoch 5/10, Batch Loss: 0.4430297017097473\n",
      "Epoch 5/10, Batch Loss: 0.7083331346511841\n",
      "Epoch 5/10, Batch Loss: 0.7360939383506775\n",
      "Epoch 5/10, Batch Loss: 0.6521527767181396\n",
      "Epoch 5/10, Batch Loss: 0.6630277633666992\n",
      "Epoch 5/10, Batch Loss: 0.31550705432891846\n",
      "Epoch 5/10, Batch Loss: 0.5840118527412415\n",
      "Epoch 5/10, Batch Loss: 0.5081456303596497\n",
      "Epoch 5/10, Batch Loss: 0.32096296548843384\n",
      "Epoch 5/10, Batch Loss: 0.4702383875846863\n",
      "Epoch 5/10, Batch Loss: 0.6000988483428955\n",
      "Epoch 5/10, Batch Loss: 0.4792003631591797\n",
      "Epoch 5/10, Batch Loss: 0.67311030626297\n",
      "Epoch 5/10, Batch Loss: 0.5188125967979431\n",
      "Epoch 5/10, Batch Loss: 0.754615843296051\n",
      "Epoch 5/10, Batch Loss: 0.9053789973258972\n",
      "Epoch 5/10, Batch Loss: 0.5585530400276184\n",
      "Epoch 5/10, Batch Loss: 0.7782467007637024\n",
      "Epoch 5/10, Batch Loss: 0.8384979963302612\n",
      "Epoch 5/10, Batch Loss: 0.43842169642448425\n",
      "Epoch 5/10, Batch Loss: 0.5172973275184631\n",
      "Epoch 5/10, Batch Loss: 0.43915030360221863\n",
      "Epoch 5/10, Batch Loss: 0.4868406653404236\n",
      "Epoch 5/10, Batch Loss: 0.48088669776916504\n",
      "Epoch 5/10, Batch Loss: 0.565094530582428\n",
      "Epoch 5/10, Batch Loss: 0.6590729355812073\n",
      "Epoch 5/10, Batch Loss: 0.5393058657646179\n",
      "Epoch 5/10, Batch Loss: 0.693264365196228\n",
      "Epoch 5/10, Batch Loss: 0.6086132526397705\n",
      "Epoch 5/10, Batch Loss: 0.6133264899253845\n",
      "Epoch 5/10, Batch Loss: 0.5811405777931213\n",
      "Epoch 5/10, Batch Loss: 0.5164182782173157\n",
      "Epoch 5/10, Batch Loss: 0.5627841949462891\n",
      "Epoch 5/10, Batch Loss: 0.6733419299125671\n",
      "Epoch 5/10, Batch Loss: 0.5412245988845825\n",
      "Epoch 5/10, Batch Loss: 0.8163688778877258\n",
      "Epoch 5/10, Batch Loss: 0.4897882342338562\n",
      "Epoch 5/10, Batch Loss: 0.5171794295310974\n",
      "Epoch 5/10, Batch Loss: 0.703150749206543\n",
      "Epoch 5/10, Batch Loss: 0.6256658434867859\n",
      "Epoch 5/10, Batch Loss: 0.4040940999984741\n",
      "Epoch 5/10, Batch Loss: 0.5675754547119141\n",
      "Epoch 5/10, Batch Loss: 0.6683127880096436\n",
      "Epoch 5/10, Batch Loss: 0.5346997380256653\n",
      "Epoch 5/10, Batch Loss: 0.7469806671142578\n",
      "Epoch 5/10, Average Training Loss: 0.5971702524054219\n",
      "Model saved for epoch 5 at Pre_train_epoch5.pt\n",
      "Validation Accuracy for epoch 5: 0.7261348464619493\n",
      "Epoch 6/10, Batch Loss: 0.7361396551132202\n",
      "Epoch 6/10, Batch Loss: 0.7972481846809387\n",
      "Epoch 6/10, Batch Loss: 0.5477470755577087\n",
      "Epoch 6/10, Batch Loss: 0.51301509141922\n",
      "Epoch 6/10, Batch Loss: 0.5776240229606628\n",
      "Epoch 6/10, Batch Loss: 0.5921809077262878\n",
      "Epoch 6/10, Batch Loss: 0.565898060798645\n",
      "Epoch 6/10, Batch Loss: 0.535863995552063\n",
      "Epoch 6/10, Batch Loss: 0.792942225933075\n",
      "Epoch 6/10, Batch Loss: 0.4625481069087982\n",
      "Epoch 6/10, Batch Loss: 0.6539918184280396\n",
      "Epoch 6/10, Batch Loss: 0.5531997084617615\n",
      "Epoch 6/10, Batch Loss: 0.6608841419219971\n",
      "Epoch 6/10, Batch Loss: 0.619761049747467\n",
      "Epoch 6/10, Batch Loss: 0.5444081425666809\n",
      "Epoch 6/10, Batch Loss: 0.4983924329280853\n",
      "Epoch 6/10, Batch Loss: 0.579284131526947\n",
      "Epoch 6/10, Batch Loss: 0.595430850982666\n",
      "Epoch 6/10, Batch Loss: 0.6645772457122803\n",
      "Epoch 6/10, Batch Loss: 0.624666154384613\n",
      "Epoch 6/10, Batch Loss: 0.634986400604248\n",
      "Epoch 6/10, Batch Loss: 0.5008956789970398\n",
      "Epoch 6/10, Batch Loss: 0.6424646973609924\n",
      "Epoch 6/10, Batch Loss: 0.6498146653175354\n",
      "Epoch 6/10, Batch Loss: 0.7726943492889404\n",
      "Epoch 6/10, Batch Loss: 0.5342687964439392\n",
      "Epoch 6/10, Batch Loss: 0.49541375041007996\n",
      "Epoch 6/10, Batch Loss: 0.5565916299819946\n",
      "Epoch 6/10, Batch Loss: 0.45351484417915344\n",
      "Epoch 6/10, Batch Loss: 0.5161327719688416\n",
      "Epoch 6/10, Batch Loss: 0.6362560391426086\n",
      "Epoch 6/10, Batch Loss: 0.5070003867149353\n",
      "Epoch 6/10, Batch Loss: 0.4805050194263458\n",
      "Epoch 6/10, Batch Loss: 0.7015960216522217\n",
      "Epoch 6/10, Batch Loss: 0.6203854084014893\n",
      "Epoch 6/10, Batch Loss: 0.5242623090744019\n",
      "Epoch 6/10, Batch Loss: 0.370626837015152\n",
      "Epoch 6/10, Batch Loss: 0.490786075592041\n",
      "Epoch 6/10, Batch Loss: 0.6981685161590576\n",
      "Epoch 6/10, Batch Loss: 0.5458527207374573\n",
      "Epoch 6/10, Batch Loss: 0.7155632376670837\n",
      "Epoch 6/10, Batch Loss: 0.3417685925960541\n",
      "Epoch 6/10, Batch Loss: 0.5741757154464722\n",
      "Epoch 6/10, Batch Loss: 0.4314342439174652\n",
      "Epoch 6/10, Batch Loss: 0.7103761434555054\n",
      "Epoch 6/10, Batch Loss: 0.527745246887207\n",
      "Epoch 6/10, Batch Loss: 0.8179438710212708\n",
      "Epoch 6/10, Batch Loss: 0.5054856538772583\n",
      "Epoch 6/10, Batch Loss: 0.3712269365787506\n",
      "Epoch 6/10, Batch Loss: 0.6214677691459656\n",
      "Epoch 6/10, Batch Loss: 0.6365765333175659\n",
      "Epoch 6/10, Batch Loss: 0.7127452492713928\n",
      "Epoch 6/10, Batch Loss: 0.611549437046051\n",
      "Epoch 6/10, Batch Loss: 0.7316406965255737\n",
      "Epoch 6/10, Batch Loss: 0.9839838147163391\n",
      "Epoch 6/10, Batch Loss: 0.4752746820449829\n",
      "Epoch 6/10, Batch Loss: 0.6800243854522705\n",
      "Epoch 6/10, Batch Loss: 0.7282246947288513\n",
      "Epoch 6/10, Batch Loss: 0.7346882224082947\n",
      "Epoch 6/10, Batch Loss: 0.7080380916595459\n",
      "Epoch 6/10, Batch Loss: 0.474254846572876\n",
      "Epoch 6/10, Batch Loss: 0.5862215161323547\n",
      "Epoch 6/10, Batch Loss: 0.5222172141075134\n",
      "Epoch 6/10, Batch Loss: 0.5555109977722168\n",
      "Epoch 6/10, Batch Loss: 0.598795473575592\n",
      "Epoch 6/10, Batch Loss: 0.6126064658164978\n",
      "Epoch 6/10, Batch Loss: 0.40473631024360657\n",
      "Epoch 6/10, Batch Loss: 0.4309033453464508\n",
      "Epoch 6/10, Batch Loss: 0.63578200340271\n",
      "Epoch 6/10, Batch Loss: 0.6462625861167908\n",
      "Epoch 6/10, Batch Loss: 0.678153932094574\n",
      "Epoch 6/10, Batch Loss: 0.5254765748977661\n",
      "Epoch 6/10, Batch Loss: 0.5010359287261963\n",
      "Epoch 6/10, Batch Loss: 0.3869035840034485\n",
      "Epoch 6/10, Batch Loss: 0.6274077892303467\n",
      "Epoch 6/10, Batch Loss: 0.7119061350822449\n",
      "Epoch 6/10, Batch Loss: 0.5116899013519287\n",
      "Epoch 6/10, Batch Loss: 0.7396756410598755\n",
      "Epoch 6/10, Batch Loss: 0.5440903902053833\n",
      "Epoch 6/10, Batch Loss: 0.638608992099762\n",
      "Epoch 6/10, Batch Loss: 0.7523385882377625\n",
      "Epoch 6/10, Batch Loss: 0.6789910197257996\n",
      "Epoch 6/10, Batch Loss: 0.409494549036026\n",
      "Epoch 6/10, Batch Loss: 0.5711449384689331\n",
      "Epoch 6/10, Batch Loss: 0.5893093943595886\n",
      "Epoch 6/10, Batch Loss: 0.4625239670276642\n",
      "Epoch 6/10, Batch Loss: 0.7197222709655762\n",
      "Epoch 6/10, Batch Loss: 0.46556800603866577\n",
      "Epoch 6/10, Batch Loss: 0.7128821611404419\n",
      "Epoch 6/10, Batch Loss: 0.842001736164093\n",
      "Epoch 6/10, Batch Loss: 0.7333344221115112\n",
      "Epoch 6/10, Batch Loss: 0.5760518312454224\n",
      "Epoch 6/10, Batch Loss: 0.5735717415809631\n",
      "Epoch 6/10, Batch Loss: 0.629590630531311\n",
      "Epoch 6/10, Batch Loss: 0.36642906069755554\n",
      "Epoch 6/10, Batch Loss: 0.5067099332809448\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e9e650ee731a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# lossÍ∞Ä outputsÏùò Îëê Î≤àÏß∏ Í∞íÏûÖÎãàÎã§.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pre-trainÏö©\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[-8]  # nÎ≤àÏß∏ Î†àÏù¥Ïñ¥Ïùò hidden statesÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "data_A = pd.read_csv(\"output1.csv\")  # data set A ÌååÏùºÎ™ÖÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B ÌååÏùºÎ™ÖÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "# Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "model_path = \"Pre-trained.pt\"\n",
    "\n",
    "# X_train, Y_train ÏÉùÏÑ±\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():  # Ï§ëÎ≥µ Ï†úÍ±∞Î•º ÌïòÏßÄ ÏïäÍ≥† ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "#print(\"X_train\\n\", X_train[:10])\n",
    "#print(\"Y_train\\n\", Y_train[:10])\n",
    "        \n",
    "# BERT ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Î™®Îç∏ Î°úÎìú\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = CustomBertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "\n",
    "# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î•º BERTÏùò ÏûÖÎ†• ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "max_len = 128  # ÏûÖÎ†• ÏãúÌÄÄÏä§Ïùò ÏµúÎåÄ Í∏∏Ïù¥\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # ÌôòÏûê Ï†ïÎ≥¥ Î∞è Ï¶ùÏÉÅ\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä\n",
    "                        max_length = max_len,         # ÏµúÎåÄ Í∏∏Ïù¥ ÏßÄÏ†ï\n",
    "                        pad_to_max_length = True,     # Ìå®Îî©ÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ ÏµúÎåÄ Í∏∏Ïù¥Î°ú ÎßûÏ∂§\n",
    "                        return_attention_mask = True, # Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±\n",
    "                        return_tensors = 'pt',        # PyTorch ÌÖêÏÑúÎ°ú Î∞òÌôò\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î∞è Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = 0.8\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=1-train_size, random_state=42)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# GPU ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Î™®Îç∏ÏùÑ GPUÎ°ú Ïù¥Îèô\n",
    "model.to(device)\n",
    "\n",
    "# ÏòµÌã∞ÎßàÏù¥Ï†Ä Î∞è ÌïôÏäµÎ•† ÏÑ§Ï†ï\n",
    "# Í∏∞Î≥∏ ÌïôÏäµÎ•† : 2e-6\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ÏóêÌè≠ ÏÑ§Ï†ï\n",
    "epochs = 10\n",
    "\n",
    "# ÌïôÏäµ Î£®ÌîÑ\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[1]  # lossÍ∞Ä outputsÏùò Îëê Î≤àÏß∏ Í∞íÏûÖÎãàÎã§.\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Batch Loss: {loss.item()}')\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # Î™®Îç∏ Ï†ÄÏû• Î∞è ÌèâÍ∞Ä\n",
    "    model_save_path = f\"Pre_train_epoch{epoch + 1}_BERT_Large.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved for epoch {epoch + 1} at {model_save_path}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs[0]  # logitsÍ∞Ä outputsÏùò Ï≤´ Î≤àÏß∏ Í∞íÏûÖÎãàÎã§.\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "\n",
    "    print(f'Validation Accuracy for epoch {epoch + 1}: {val_accuracy / len(val_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d715e452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCC\\anaconda3\\envs\\biotf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1/20, Batch Loss: 0.3308902084827423\n",
      "Epoch 1/20, Batch Loss: 0.14640307426452637\n",
      "Epoch 1/20, Batch Loss: 0.12646996974945068\n",
      "Epoch 1/20, Batch Loss: 0.43731361627578735\n",
      "Epoch 1/20, Batch Loss: 0.3690773546695709\n",
      "Epoch 1/20, Batch Loss: 0.3478721082210541\n",
      "Epoch 1/20, Batch Loss: 0.4624733626842499\n",
      "Epoch 1/20, Batch Loss: 0.08641890436410904\n",
      "Epoch 1/20, Batch Loss: 0.19177794456481934\n",
      "Epoch 1/20, Batch Loss: 0.14352092146873474\n",
      "Epoch 1/20, Batch Loss: 0.34213677048683167\n",
      "Epoch 1/20, Batch Loss: 0.18757636845111847\n",
      "Epoch 1/20, Batch Loss: 0.3164503872394562\n",
      "Epoch 1/20, Batch Loss: 0.3291611075401306\n",
      "Epoch 1/20, Batch Loss: 0.7779244780540466\n",
      "Epoch 1/20, Batch Loss: 0.5579509139060974\n",
      "Epoch 1/20, Batch Loss: 0.40172895789146423\n",
      "Epoch 1/20, Batch Loss: 0.09963130950927734\n",
      "Epoch 1/20, Batch Loss: 0.13323451578617096\n",
      "Epoch 1/20, Batch Loss: 0.3186250627040863\n",
      "Epoch 1/20, Batch Loss: 0.13826097548007965\n",
      "Epoch 1/20, Batch Loss: 0.415262907743454\n",
      "Epoch 1/20, Batch Loss: 0.4338319003582001\n",
      "Epoch 1/20, Batch Loss: 0.12625841796398163\n",
      "Epoch 1/20, Batch Loss: 0.43368566036224365\n",
      "Epoch 1/20, Batch Loss: 0.31441932916641235\n",
      "Epoch 1/20, Batch Loss: 0.6733564138412476\n",
      "Epoch 1/20, Batch Loss: 0.3422096371650696\n",
      "Epoch 1/20, Batch Loss: 0.32835447788238525\n",
      "Epoch 1/20, Batch Loss: 0.44145214557647705\n",
      "Epoch 1/20, Batch Loss: 0.10282588750123978\n",
      "Epoch 1/20, Batch Loss: 0.21709972620010376\n",
      "Epoch 1/20, Batch Loss: 0.3852477967739105\n",
      "Epoch 1/20, Batch Loss: 0.417077898979187\n",
      "Epoch 1/20, Batch Loss: 0.29139143228530884\n",
      "Epoch 1/20, Batch Loss: 0.24291619658470154\n",
      "Epoch 1/20, Batch Loss: 0.5213398337364197\n",
      "Epoch 1/20, Batch Loss: 0.09204529970884323\n",
      "Epoch 1/20, Batch Loss: 0.420268177986145\n",
      "Epoch 1/20, Batch Loss: 0.1324479579925537\n",
      "Epoch 1/20, Batch Loss: 0.47779783606529236\n",
      "Epoch 1/20, Batch Loss: 0.20009514689445496\n",
      "Epoch 1/20, Batch Loss: 0.44921356439590454\n",
      "Epoch 1/20, Batch Loss: 0.13486331701278687\n",
      "Epoch 1/20, Batch Loss: 0.13084784150123596\n",
      "Epoch 1/20, Batch Loss: 0.26522377133369446\n",
      "Epoch 1/20, Batch Loss: 0.14237812161445618\n",
      "Epoch 1/20, Batch Loss: 0.47641780972480774\n",
      "Epoch 1/20, Batch Loss: 0.5877021551132202\n",
      "Epoch 1/20, Batch Loss: 0.3375537395477295\n",
      "Epoch 1/20, Batch Loss: 0.23312599956989288\n",
      "Epoch 1/20, Batch Loss: 0.19233059883117676\n",
      "Epoch 1/20, Batch Loss: 0.3430163562297821\n",
      "Epoch 1/20, Batch Loss: 0.3238036036491394\n",
      "Epoch 1/20, Batch Loss: 0.3317820429801941\n",
      "Epoch 1/20, Batch Loss: 0.24479331076145172\n",
      "Epoch 1/20, Batch Loss: 0.22469832003116608\n",
      "Epoch 1/20, Batch Loss: 0.37032100558280945\n",
      "Epoch 1/20, Batch Loss: 0.07751280814409256\n",
      "Epoch 1/20, Batch Loss: 0.3035986125469208\n",
      "Epoch 1/20, Batch Loss: 0.3095443546772003\n",
      "Epoch 1/20, Batch Loss: 0.7689459323883057\n",
      "Epoch 1/20, Batch Loss: 0.12142949551343918\n",
      "Epoch 1/20, Batch Loss: 0.6790818572044373\n",
      "Epoch 1/20, Batch Loss: 0.22724050283432007\n",
      "Epoch 1/20, Batch Loss: 0.14448942244052887\n",
      "Epoch 1/20, Batch Loss: 0.6725825071334839\n",
      "Epoch 1/20, Batch Loss: 0.30851635336875916\n",
      "Epoch 1/20, Batch Loss: 0.422503262758255\n",
      "Epoch 1/20, Batch Loss: 0.17027731239795685\n",
      "Epoch 1/20, Batch Loss: 0.2839643061161041\n",
      "Epoch 1/20, Batch Loss: 0.3638935685157776\n",
      "Epoch 1/20, Batch Loss: 0.22876739501953125\n",
      "Epoch 1/20, Batch Loss: 0.07796779274940491\n",
      "Epoch 1/20, Batch Loss: 0.3539324700832367\n",
      "Epoch 1/20, Batch Loss: 0.20149780809879303\n",
      "Epoch 1/20, Batch Loss: 0.3307622969150543\n",
      "Epoch 1/20, Batch Loss: 0.3339231312274933\n",
      "Epoch 1/20, Batch Loss: 0.12748827040195465\n",
      "Epoch 1/20, Batch Loss: 0.2585386037826538\n",
      "Epoch 1/20, Batch Loss: 0.3028406798839569\n",
      "Epoch 1/20, Batch Loss: 0.15094837546348572\n",
      "Epoch 1/20, Batch Loss: 0.35868728160858154\n",
      "Epoch 1/20, Batch Loss: 0.33127328753471375\n",
      "Epoch 1/20, Batch Loss: 0.2268761545419693\n",
      "Epoch 1/20, Batch Loss: 0.43910709023475647\n",
      "Epoch 1/20, Batch Loss: 0.3131982684135437\n",
      "Epoch 1/20, Batch Loss: 0.3593296408653259\n",
      "Epoch 1/20, Batch Loss: 0.20194792747497559\n",
      "Epoch 1/20, Batch Loss: 0.2284134030342102\n",
      "Epoch 1/20, Batch Loss: 0.32888567447662354\n",
      "Epoch 1/20, Batch Loss: 0.3203674256801605\n",
      "Epoch 1/20, Batch Loss: 0.6902123093605042\n",
      "Epoch 1/20, Batch Loss: 0.22404678165912628\n",
      "Epoch 1/20, Batch Loss: 0.4769352376461029\n",
      "Epoch 1/20, Batch Loss: 0.5688139200210571\n",
      "Epoch 1/20, Batch Loss: 0.684929609298706\n",
      "Epoch 1/20, Batch Loss: 0.2536136507987976\n",
      "Epoch 1/20, Batch Loss: 0.28651532530784607\n",
      "Epoch 1/20, Batch Loss: 0.626308798789978\n",
      "Epoch 1/20, Batch Loss: 0.3420085906982422\n",
      "Epoch 1/20, Batch Loss: 0.3501248359680176\n",
      "Epoch 1/20, Batch Loss: 0.7131290435791016\n",
      "Epoch 1/20, Batch Loss: 0.35600557923316956\n",
      "Epoch 1/20, Batch Loss: 0.11542513221502304\n",
      "Epoch 1/20, Batch Loss: 0.12007904797792435\n",
      "Epoch 1/20, Batch Loss: 0.4464374780654907\n",
      "Epoch 1/20, Batch Loss: 0.17514649033546448\n",
      "Epoch 1/20, Batch Loss: 0.4252343773841858\n",
      "Epoch 1/20, Batch Loss: 0.3449360132217407\n",
      "Epoch 1/20, Batch Loss: 0.3610057830810547\n",
      "Epoch 1/20, Batch Loss: 0.20374996960163116\n",
      "Epoch 1/20, Batch Loss: 0.42441806197166443\n",
      "Epoch 1/20, Batch Loss: 0.2525939345359802\n",
      "Epoch 1/20, Batch Loss: 0.3225979208946228\n",
      "Epoch 1/20, Batch Loss: 0.4450558125972748\n",
      "Epoch 1/20, Batch Loss: 0.1306513249874115\n",
      "Epoch 1/20, Batch Loss: 0.3204319179058075\n",
      "Epoch 1/20, Batch Loss: 0.20148532092571259\n",
      "Epoch 1/20, Batch Loss: 0.27242380380630493\n",
      "Epoch 1/20, Batch Loss: 0.1199600026011467\n",
      "Epoch 1/20, Batch Loss: 0.48789873719215393\n",
      "Epoch 1/20, Batch Loss: 0.06095336377620697\n",
      "Epoch 1/20, Average Training Loss: 0.31848630406023043\n",
      "Model saved for epoch 1 at Fine_tuned_epoch1_BERT_Large.pt\n",
      "Validation Accuracy for epoch 1: 0.8915770609318997\n",
      "Epoch 2/20, Batch Loss: 0.26572635769844055\n",
      "Epoch 2/20, Batch Loss: 0.14352640509605408\n",
      "Epoch 2/20, Batch Loss: 0.22710773348808289\n",
      "Epoch 2/20, Batch Loss: 0.2452363222837448\n",
      "Epoch 2/20, Batch Loss: 0.5731071829795837\n",
      "Epoch 2/20, Batch Loss: 0.07452777028083801\n",
      "Epoch 2/20, Batch Loss: 0.25006675720214844\n",
      "Epoch 2/20, Batch Loss: 0.23501938581466675\n",
      "Epoch 2/20, Batch Loss: 0.3023451566696167\n",
      "Epoch 2/20, Batch Loss: 0.8788044452667236\n",
      "Epoch 2/20, Batch Loss: 0.3458232581615448\n",
      "Epoch 2/20, Batch Loss: 0.4239738881587982\n",
      "Epoch 2/20, Batch Loss: 0.18744251132011414\n",
      "Epoch 2/20, Batch Loss: 0.30065515637397766\n",
      "Epoch 2/20, Batch Loss: 0.2231011837720871\n",
      "Epoch 2/20, Batch Loss: 0.20141659677028656\n",
      "Epoch 2/20, Batch Loss: 0.10516655445098877\n",
      "Epoch 2/20, Batch Loss: 0.7101466059684753\n",
      "Epoch 2/20, Batch Loss: 0.21089568734169006\n",
      "Epoch 2/20, Batch Loss: 0.3024275600910187\n",
      "Epoch 2/20, Batch Loss: 0.4534744322299957\n",
      "Epoch 2/20, Batch Loss: 0.37008601427078247\n",
      "Epoch 2/20, Batch Loss: 0.3265651762485504\n",
      "Epoch 2/20, Batch Loss: 0.20943528413772583\n",
      "Epoch 2/20, Batch Loss: 0.31408020853996277\n",
      "Epoch 2/20, Batch Loss: 0.22174276411533356\n",
      "Epoch 2/20, Batch Loss: 0.09975922107696533\n",
      "Epoch 2/20, Batch Loss: 0.32587766647338867\n",
      "Epoch 2/20, Batch Loss: 0.299290269613266\n",
      "Epoch 2/20, Batch Loss: 0.33662670850753784\n",
      "Epoch 2/20, Batch Loss: 0.2687333822250366\n",
      "Epoch 2/20, Batch Loss: 0.14692559838294983\n",
      "Epoch 2/20, Batch Loss: 0.322870671749115\n",
      "Epoch 2/20, Batch Loss: 0.1317114233970642\n",
      "Epoch 2/20, Batch Loss: 0.19475647807121277\n",
      "Epoch 2/20, Batch Loss: 0.32829755544662476\n",
      "Epoch 2/20, Batch Loss: 0.20958906412124634\n",
      "Epoch 2/20, Batch Loss: 0.21821464598178864\n",
      "Epoch 2/20, Batch Loss: 0.08383844792842865\n",
      "Epoch 2/20, Batch Loss: 0.4839552044868469\n",
      "Epoch 2/20, Batch Loss: 0.35445237159729004\n",
      "Epoch 2/20, Batch Loss: 0.47419995069503784\n",
      "Epoch 2/20, Batch Loss: 0.32314521074295044\n",
      "Epoch 2/20, Batch Loss: 0.33190444111824036\n",
      "Epoch 2/20, Batch Loss: 0.22103318572044373\n",
      "Epoch 2/20, Batch Loss: 0.24458830058574677\n",
      "Epoch 2/20, Batch Loss: 0.08249194920063019\n",
      "Epoch 2/20, Batch Loss: 0.5495433211326599\n",
      "Epoch 2/20, Batch Loss: 0.25837552547454834\n",
      "Epoch 2/20, Batch Loss: 0.08067251741886139\n",
      "Epoch 2/20, Batch Loss: 0.09971145540475845\n",
      "Epoch 2/20, Batch Loss: 0.3326691687107086\n",
      "Epoch 2/20, Batch Loss: 0.6009229421615601\n",
      "Epoch 2/20, Batch Loss: 0.4443614184856415\n",
      "Epoch 2/20, Batch Loss: 0.1851433515548706\n",
      "Epoch 2/20, Batch Loss: 0.5552586913108826\n",
      "Epoch 2/20, Batch Loss: 0.5795471668243408\n",
      "Epoch 2/20, Batch Loss: 0.3314255177974701\n",
      "Epoch 2/20, Batch Loss: 0.40970370173454285\n",
      "Epoch 2/20, Batch Loss: 0.30694201588630676\n",
      "Epoch 2/20, Batch Loss: 0.2049872875213623\n",
      "Epoch 2/20, Batch Loss: 0.6723104119300842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Batch Loss: 0.4249988794326782\n",
      "Epoch 2/20, Batch Loss: 0.5439640879631042\n",
      "Epoch 2/20, Batch Loss: 0.21461258828639984\n",
      "Epoch 2/20, Batch Loss: 0.33965763449668884\n",
      "Epoch 2/20, Batch Loss: 0.0884118527173996\n",
      "Epoch 2/20, Batch Loss: 0.42314207553863525\n",
      "Epoch 2/20, Batch Loss: 0.3232156038284302\n",
      "Epoch 2/20, Batch Loss: 0.3046879768371582\n",
      "Epoch 2/20, Batch Loss: 0.4255339503288269\n",
      "Epoch 2/20, Batch Loss: 0.3486599922180176\n",
      "Epoch 2/20, Batch Loss: 0.2701864242553711\n",
      "Epoch 2/20, Batch Loss: 0.2589777410030365\n",
      "Epoch 2/20, Batch Loss: 0.23708772659301758\n",
      "Epoch 2/20, Batch Loss: 0.30392563343048096\n",
      "Epoch 2/20, Batch Loss: 0.24922388792037964\n",
      "Epoch 2/20, Batch Loss: 0.25344884395599365\n",
      "Epoch 2/20, Batch Loss: 0.22737576067447662\n",
      "Epoch 2/20, Batch Loss: 0.46291157603263855\n",
      "Epoch 2/20, Batch Loss: 0.35108277201652527\n",
      "Epoch 2/20, Batch Loss: 0.44227826595306396\n",
      "Epoch 2/20, Batch Loss: 0.20201782882213593\n",
      "Epoch 2/20, Batch Loss: 0.21920746564865112\n",
      "Epoch 2/20, Batch Loss: 0.5138790011405945\n",
      "Epoch 2/20, Batch Loss: 0.2026781588792801\n",
      "Epoch 2/20, Batch Loss: 0.39832907915115356\n",
      "Epoch 2/20, Batch Loss: 0.22998937964439392\n",
      "Epoch 2/20, Batch Loss: 0.2503010034561157\n",
      "Epoch 2/20, Batch Loss: 0.204802006483078\n",
      "Epoch 2/20, Batch Loss: 0.4396434426307678\n",
      "Epoch 2/20, Batch Loss: 0.3336252272129059\n",
      "Epoch 2/20, Batch Loss: 0.35091453790664673\n",
      "Epoch 2/20, Batch Loss: 0.4678082764148712\n",
      "Epoch 2/20, Batch Loss: 0.4055492877960205\n",
      "Epoch 2/20, Batch Loss: 0.2027369737625122\n",
      "Epoch 2/20, Batch Loss: 0.1811280995607376\n",
      "Epoch 2/20, Batch Loss: 0.46451660990715027\n",
      "Epoch 2/20, Batch Loss: 0.43436938524246216\n",
      "Epoch 2/20, Batch Loss: 0.11252312362194061\n",
      "Epoch 2/20, Batch Loss: 0.22489024698734283\n",
      "Epoch 2/20, Batch Loss: 0.27590033411979675\n",
      "Epoch 2/20, Batch Loss: 0.20008490979671478\n",
      "Epoch 2/20, Batch Loss: 0.5542566776275635\n",
      "Epoch 2/20, Batch Loss: 0.5870641469955444\n",
      "Epoch 2/20, Batch Loss: 0.563285231590271\n",
      "Epoch 2/20, Batch Loss: 0.21617110073566437\n",
      "Epoch 2/20, Batch Loss: 0.3393338620662689\n",
      "Epoch 2/20, Batch Loss: 0.23823927342891693\n",
      "Epoch 2/20, Batch Loss: 0.21909528970718384\n",
      "Epoch 2/20, Batch Loss: 0.5516204833984375\n",
      "Epoch 2/20, Batch Loss: 0.5089601278305054\n",
      "Epoch 2/20, Batch Loss: 0.12977483868598938\n",
      "Epoch 2/20, Batch Loss: 0.30715444684028625\n",
      "Epoch 2/20, Batch Loss: 0.4311116337776184\n",
      "Epoch 2/20, Batch Loss: 0.5720158815383911\n",
      "Epoch 2/20, Batch Loss: 0.3587500751018524\n",
      "Epoch 2/20, Batch Loss: 0.14706552028656006\n",
      "Epoch 2/20, Batch Loss: 0.3333739936351776\n",
      "Epoch 2/20, Batch Loss: 0.16187557578086853\n",
      "Epoch 2/20, Batch Loss: 0.2150799185037613\n",
      "Epoch 2/20, Batch Loss: 0.41608741879463196\n",
      "Epoch 2/20, Batch Loss: 0.5994590520858765\n",
      "Epoch 2/20, Average Training Loss: 0.3207625108884602\n",
      "Model saved for epoch 2 at Fine_tuned_epoch2_BERT_Large.pt\n",
      "Validation Accuracy for epoch 2: 0.8900089605734768\n",
      "Epoch 3/20, Batch Loss: 0.24747014045715332\n",
      "Epoch 3/20, Batch Loss: 0.1115860566496849\n",
      "Epoch 3/20, Batch Loss: 0.18715858459472656\n",
      "Epoch 3/20, Batch Loss: 0.2337721437215805\n",
      "Epoch 3/20, Batch Loss: 0.4608106017112732\n",
      "Epoch 3/20, Batch Loss: 0.6390714049339294\n",
      "Epoch 3/20, Batch Loss: 0.7046451568603516\n",
      "Epoch 3/20, Batch Loss: 0.4560813307762146\n",
      "Epoch 3/20, Batch Loss: 0.21831537783145905\n",
      "Epoch 3/20, Batch Loss: 0.31936073303222656\n",
      "Epoch 3/20, Batch Loss: 0.23932123184204102\n",
      "Epoch 3/20, Batch Loss: 0.5199706554412842\n",
      "Epoch 3/20, Batch Loss: 0.25764328241348267\n",
      "Epoch 3/20, Batch Loss: 0.4096916913986206\n",
      "Epoch 3/20, Batch Loss: 0.3532279431819916\n",
      "Epoch 3/20, Batch Loss: 0.3179614245891571\n",
      "Epoch 3/20, Batch Loss: 0.3203403353691101\n",
      "Epoch 3/20, Batch Loss: 0.2493739128112793\n",
      "Epoch 3/20, Batch Loss: 0.10877526551485062\n",
      "Epoch 3/20, Batch Loss: 0.25542646646499634\n",
      "Epoch 3/20, Batch Loss: 0.4241084158420563\n",
      "Epoch 3/20, Batch Loss: 0.3673754930496216\n",
      "Epoch 3/20, Batch Loss: 0.29774340987205505\n",
      "Epoch 3/20, Batch Loss: 0.15002845227718353\n",
      "Epoch 3/20, Batch Loss: 0.23924793303012848\n",
      "Epoch 3/20, Batch Loss: 0.3169570565223694\n",
      "Epoch 3/20, Batch Loss: 0.12706336379051208\n",
      "Epoch 3/20, Batch Loss: 0.3838140368461609\n",
      "Epoch 3/20, Batch Loss: 0.1344054639339447\n",
      "Epoch 3/20, Batch Loss: 0.23014579713344574\n",
      "Epoch 3/20, Batch Loss: 0.3774149417877197\n",
      "Epoch 3/20, Batch Loss: 0.4707743227481842\n",
      "Epoch 3/20, Batch Loss: 0.3163259029388428\n",
      "Epoch 3/20, Batch Loss: 0.16023336350917816\n",
      "Epoch 3/20, Batch Loss: 0.27060288190841675\n",
      "Epoch 3/20, Batch Loss: 0.36064428091049194\n",
      "Epoch 3/20, Batch Loss: 0.528748631477356\n",
      "Epoch 3/20, Batch Loss: 0.1059405729174614\n",
      "Epoch 3/20, Batch Loss: 0.46075254678726196\n",
      "Epoch 3/20, Batch Loss: 0.1137913390994072\n",
      "Epoch 3/20, Batch Loss: 0.3555806279182434\n",
      "Epoch 3/20, Batch Loss: 0.5103256702423096\n",
      "Epoch 3/20, Batch Loss: 0.6973333358764648\n",
      "Epoch 3/20, Batch Loss: 0.542620062828064\n",
      "Epoch 3/20, Batch Loss: 0.24390125274658203\n",
      "Epoch 3/20, Batch Loss: 0.3156299889087677\n",
      "Epoch 3/20, Batch Loss: 0.2837264835834503\n",
      "Epoch 3/20, Batch Loss: 0.35008785128593445\n",
      "Epoch 3/20, Batch Loss: 0.11131905764341354\n",
      "Epoch 3/20, Batch Loss: 0.3190537691116333\n",
      "Epoch 3/20, Batch Loss: 0.23733846843242645\n",
      "Epoch 3/20, Batch Loss: 0.30104315280914307\n",
      "Epoch 3/20, Batch Loss: 0.21387691795825958\n",
      "Epoch 3/20, Batch Loss: 0.19885846972465515\n",
      "Epoch 3/20, Batch Loss: 0.19702363014221191\n",
      "Epoch 3/20, Batch Loss: 0.22776560485363007\n",
      "Epoch 3/20, Batch Loss: 0.6423750519752502\n",
      "Epoch 3/20, Batch Loss: 0.4456692934036255\n",
      "Epoch 3/20, Batch Loss: 0.1914960891008377\n",
      "Epoch 3/20, Batch Loss: 0.21910743415355682\n",
      "Epoch 3/20, Batch Loss: 0.577592670917511\n",
      "Epoch 3/20, Batch Loss: 0.35346725583076477\n",
      "Epoch 3/20, Batch Loss: 0.2223660945892334\n",
      "Epoch 3/20, Batch Loss: 0.11135263741016388\n",
      "Epoch 3/20, Batch Loss: 0.09662702679634094\n",
      "Epoch 3/20, Batch Loss: 0.31442791223526\n",
      "Epoch 3/20, Batch Loss: 0.3338868021965027\n",
      "Epoch 3/20, Batch Loss: 0.2544611692428589\n",
      "Epoch 3/20, Batch Loss: 0.06722202152013779\n",
      "Epoch 3/20, Batch Loss: 0.09397923946380615\n",
      "Epoch 3/20, Batch Loss: 0.3142736554145813\n",
      "Epoch 3/20, Batch Loss: 0.24321620166301727\n",
      "Epoch 3/20, Batch Loss: 0.3372145891189575\n",
      "Epoch 3/20, Batch Loss: 0.34264469146728516\n",
      "Epoch 3/20, Batch Loss: 0.4210214614868164\n",
      "Epoch 3/20, Batch Loss: 0.34492895007133484\n",
      "Epoch 3/20, Batch Loss: 0.3063157796859741\n",
      "Epoch 3/20, Batch Loss: 0.5785850286483765\n",
      "Epoch 3/20, Batch Loss: 0.21313157677650452\n",
      "Epoch 3/20, Batch Loss: 0.33652329444885254\n",
      "Epoch 3/20, Batch Loss: 0.5229682326316833\n",
      "Epoch 3/20, Batch Loss: 0.4267231822013855\n",
      "Epoch 3/20, Batch Loss: 0.5502147078514099\n",
      "Epoch 3/20, Batch Loss: 0.38654059171676636\n",
      "Epoch 3/20, Batch Loss: 0.32593244314193726\n",
      "Epoch 3/20, Batch Loss: 0.12962143123149872\n",
      "Epoch 3/20, Batch Loss: 0.34364181756973267\n",
      "Epoch 3/20, Batch Loss: 0.30443257093429565\n",
      "Epoch 3/20, Batch Loss: 0.4987732172012329\n",
      "Epoch 3/20, Batch Loss: 0.2171826809644699\n",
      "Epoch 3/20, Batch Loss: 0.45514392852783203\n",
      "Epoch 3/20, Batch Loss: 0.16024620831012726\n",
      "Epoch 3/20, Batch Loss: 0.5337263345718384\n",
      "Epoch 3/20, Batch Loss: 0.3419433534145355\n",
      "Epoch 3/20, Batch Loss: 0.24165210127830505\n",
      "Epoch 3/20, Batch Loss: 0.5051068663597107\n",
      "Epoch 3/20, Batch Loss: 0.6829342246055603\n",
      "Epoch 3/20, Batch Loss: 0.4347050189971924\n",
      "Epoch 3/20, Batch Loss: 0.12958131730556488\n",
      "Epoch 3/20, Batch Loss: 0.44081002473831177\n",
      "Epoch 3/20, Batch Loss: 0.41593948006629944\n",
      "Epoch 3/20, Batch Loss: 0.1948779672384262\n",
      "Epoch 3/20, Batch Loss: 0.3187377452850342\n",
      "Epoch 3/20, Batch Loss: 0.10970573127269745\n",
      "Epoch 3/20, Batch Loss: 0.24079583585262299\n",
      "Epoch 3/20, Batch Loss: 0.26967504620552063\n",
      "Epoch 3/20, Batch Loss: 0.18187439441680908\n",
      "Epoch 3/20, Batch Loss: 0.21752411127090454\n",
      "Epoch 3/20, Batch Loss: 0.2714149057865143\n",
      "Epoch 3/20, Batch Loss: 0.1284790188074112\n",
      "Epoch 3/20, Batch Loss: 0.4324173033237457\n",
      "Epoch 3/20, Batch Loss: 0.3218679428100586\n",
      "Epoch 3/20, Batch Loss: 0.7953970432281494\n",
      "Epoch 3/20, Batch Loss: 0.31758588552474976\n",
      "Epoch 3/20, Batch Loss: 0.19291509687900543\n",
      "Epoch 3/20, Batch Loss: 0.2174539715051651\n",
      "Epoch 3/20, Batch Loss: 0.1548546403646469\n",
      "Epoch 3/20, Batch Loss: 0.2495497763156891\n",
      "Epoch 3/20, Batch Loss: 0.20589569211006165\n",
      "Epoch 3/20, Batch Loss: 0.3833703398704529\n",
      "Epoch 3/20, Batch Loss: 0.360665887594223\n",
      "Epoch 3/20, Batch Loss: 0.15016132593154907\n",
      "Epoch 3/20, Batch Loss: 0.08817747980356216\n",
      "Epoch 3/20, Average Training Loss: 0.31516023652582636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved for epoch 3 at Fine_tuned_epoch3_BERT_Large.pt\n",
      "Validation Accuracy for epoch 3: 0.8915770609318997\n",
      "Epoch 4/20, Batch Loss: 0.44169414043426514\n",
      "Epoch 4/20, Batch Loss: 0.2499275654554367\n",
      "Epoch 4/20, Batch Loss: 0.3145858645439148\n",
      "Epoch 4/20, Batch Loss: 0.36423948407173157\n",
      "Epoch 4/20, Batch Loss: 0.08441095054149628\n",
      "Epoch 4/20, Batch Loss: 0.45803046226501465\n",
      "Epoch 4/20, Batch Loss: 0.24256236851215363\n",
      "Epoch 4/20, Batch Loss: 0.07667956501245499\n",
      "Epoch 4/20, Batch Loss: 0.4053555428981781\n",
      "Epoch 4/20, Batch Loss: 0.25008949637413025\n",
      "Epoch 4/20, Batch Loss: 0.1888357698917389\n",
      "Epoch 4/20, Batch Loss: 0.22686167061328888\n",
      "Epoch 4/20, Batch Loss: 0.32194435596466064\n",
      "Epoch 4/20, Batch Loss: 0.32705387473106384\n",
      "Epoch 4/20, Batch Loss: 0.1298724263906479\n",
      "Epoch 4/20, Batch Loss: 0.12496038526296616\n",
      "Epoch 4/20, Batch Loss: 0.3317922353744507\n",
      "Epoch 4/20, Batch Loss: 0.5296743512153625\n",
      "Epoch 4/20, Batch Loss: 0.6884816884994507\n",
      "Epoch 4/20, Batch Loss: 0.20833174884319305\n",
      "Epoch 4/20, Batch Loss: 0.22791196405887604\n",
      "Epoch 4/20, Batch Loss: 0.3421395719051361\n",
      "Epoch 4/20, Batch Loss: 0.2127918154001236\n",
      "Epoch 4/20, Batch Loss: 0.5250222682952881\n",
      "Epoch 4/20, Batch Loss: 0.6867730021476746\n",
      "Epoch 4/20, Batch Loss: 0.3138883411884308\n",
      "Epoch 4/20, Batch Loss: 0.21496154367923737\n",
      "Epoch 4/20, Batch Loss: 0.09479432553052902\n",
      "Epoch 4/20, Batch Loss: 0.125143364071846\n",
      "Epoch 4/20, Batch Loss: 0.26968902349472046\n",
      "Epoch 4/20, Batch Loss: 0.12095704674720764\n",
      "Epoch 4/20, Batch Loss: 0.11946248263120651\n",
      "Epoch 4/20, Batch Loss: 0.17506255209445953\n",
      "Epoch 4/20, Batch Loss: 0.31755873560905457\n",
      "Epoch 4/20, Batch Loss: 0.43926486372947693\n",
      "Epoch 4/20, Batch Loss: 0.07151150703430176\n",
      "Epoch 4/20, Batch Loss: 0.1064896360039711\n",
      "Epoch 4/20, Batch Loss: 0.5578168034553528\n",
      "Epoch 4/20, Batch Loss: 0.4089450240135193\n",
      "Epoch 4/20, Batch Loss: 0.4314693510532379\n",
      "Epoch 4/20, Batch Loss: 0.42708253860473633\n",
      "Epoch 4/20, Batch Loss: 0.327718585729599\n",
      "Epoch 4/20, Batch Loss: 0.22674857079982758\n",
      "Epoch 4/20, Batch Loss: 0.7052287459373474\n",
      "Epoch 4/20, Batch Loss: 0.34841322898864746\n",
      "Epoch 4/20, Batch Loss: 0.32998794317245483\n",
      "Epoch 4/20, Batch Loss: 0.35485681891441345\n",
      "Epoch 4/20, Batch Loss: 0.09657733887434006\n",
      "Epoch 4/20, Batch Loss: 0.3148925006389618\n",
      "Epoch 4/20, Batch Loss: 0.3151199221611023\n",
      "Epoch 4/20, Batch Loss: 0.41548073291778564\n",
      "Epoch 4/20, Batch Loss: 0.24375972151756287\n",
      "Epoch 4/20, Batch Loss: 0.4751700162887573\n",
      "Epoch 4/20, Batch Loss: 0.41167932748794556\n",
      "Epoch 4/20, Batch Loss: 0.13185779750347137\n",
      "Epoch 4/20, Batch Loss: 0.3146056532859802\n",
      "Epoch 4/20, Batch Loss: 0.323510080575943\n",
      "Epoch 4/20, Batch Loss: 0.326474130153656\n",
      "Epoch 4/20, Batch Loss: 0.30407029390335083\n",
      "Epoch 4/20, Batch Loss: 0.33312875032424927\n",
      "Epoch 4/20, Batch Loss: 0.255472868680954\n",
      "Epoch 4/20, Batch Loss: 0.24936914443969727\n",
      "Epoch 4/20, Batch Loss: 0.3529331684112549\n",
      "Epoch 4/20, Batch Loss: 0.13339847326278687\n",
      "Epoch 4/20, Batch Loss: 0.0851992815732956\n",
      "Epoch 4/20, Batch Loss: 0.268820583820343\n",
      "Epoch 4/20, Batch Loss: 0.27063748240470886\n",
      "Epoch 4/20, Batch Loss: 0.21780720353126526\n",
      "Epoch 4/20, Batch Loss: 0.31177425384521484\n",
      "Epoch 4/20, Batch Loss: 0.0820733830332756\n",
      "Epoch 4/20, Batch Loss: 0.333830326795578\n",
      "Epoch 4/20, Batch Loss: 0.4220905900001526\n",
      "Epoch 4/20, Batch Loss: 0.32970744371414185\n",
      "Epoch 4/20, Batch Loss: 0.1965363770723343\n",
      "Epoch 4/20, Batch Loss: 0.33800768852233887\n",
      "Epoch 4/20, Batch Loss: 0.33577510714530945\n",
      "Epoch 4/20, Batch Loss: 0.3432599604129791\n",
      "Epoch 4/20, Batch Loss: 0.3360641896724701\n",
      "Epoch 4/20, Batch Loss: 0.21369782090187073\n",
      "Epoch 4/20, Batch Loss: 0.608577311038971\n",
      "Epoch 4/20, Batch Loss: 0.4129033386707306\n",
      "Epoch 4/20, Batch Loss: 0.2446090430021286\n",
      "Epoch 4/20, Batch Loss: 0.4731808304786682\n",
      "Epoch 4/20, Batch Loss: 0.13316228985786438\n",
      "Epoch 4/20, Batch Loss: 0.2438921332359314\n",
      "Epoch 4/20, Batch Loss: 0.21857000887393951\n",
      "Epoch 4/20, Batch Loss: 0.5521906018257141\n",
      "Epoch 4/20, Batch Loss: 0.3577340245246887\n",
      "Epoch 4/20, Batch Loss: 0.3334273397922516\n",
      "Epoch 4/20, Batch Loss: 0.3089819550514221\n",
      "Epoch 4/20, Batch Loss: 0.1060834750533104\n",
      "Epoch 4/20, Batch Loss: 0.2349226027727127\n",
      "Epoch 4/20, Batch Loss: 0.2125091552734375\n",
      "Epoch 4/20, Batch Loss: 0.19018100202083588\n",
      "Epoch 4/20, Batch Loss: 0.19763582944869995\n",
      "Epoch 4/20, Batch Loss: 0.2980318069458008\n",
      "Epoch 4/20, Batch Loss: 0.3347688913345337\n",
      "Epoch 4/20, Batch Loss: 0.5668887495994568\n",
      "Epoch 4/20, Batch Loss: 0.3364979922771454\n",
      "Epoch 4/20, Batch Loss: 0.30976200103759766\n",
      "Epoch 4/20, Batch Loss: 0.42700162529945374\n",
      "Epoch 4/20, Batch Loss: 0.556047797203064\n",
      "Epoch 4/20, Batch Loss: 0.08151137083768845\n",
      "Epoch 4/20, Batch Loss: 0.34256798028945923\n",
      "Epoch 4/20, Batch Loss: 0.2506276071071625\n",
      "Epoch 4/20, Batch Loss: 0.10013562440872192\n",
      "Epoch 4/20, Batch Loss: 0.462453693151474\n",
      "Epoch 4/20, Batch Loss: 0.22701792418956757\n",
      "Epoch 4/20, Batch Loss: 0.5654454827308655\n",
      "Epoch 4/20, Batch Loss: 0.23211751878261566\n",
      "Epoch 4/20, Batch Loss: 0.7232160568237305\n",
      "Epoch 4/20, Batch Loss: 0.46529442071914673\n",
      "Epoch 4/20, Batch Loss: 0.2525706887245178\n",
      "Epoch 4/20, Batch Loss: 0.6614195108413696\n",
      "Epoch 4/20, Batch Loss: 0.20104257762432098\n",
      "Epoch 4/20, Batch Loss: 0.6004907488822937\n",
      "Epoch 4/20, Batch Loss: 0.3558439314365387\n",
      "Epoch 4/20, Batch Loss: 0.31509891152381897\n",
      "Epoch 4/20, Batch Loss: 0.23243141174316406\n",
      "Epoch 4/20, Batch Loss: 0.3018246591091156\n",
      "Epoch 4/20, Batch Loss: 0.5525076985359192\n",
      "Epoch 4/20, Batch Loss: 0.3038625419139862\n",
      "Epoch 4/20, Batch Loss: 0.1137612834572792\n",
      "Epoch 4/20, Average Training Loss: 0.3132411110691908\n",
      "Model saved for epoch 4 at Fine_tuned_epoch4_BERT_Large.pt\n",
      "Validation Accuracy for epoch 4: 0.8931451612903226\n",
      "Epoch 5/20, Batch Loss: 0.2014944702386856\n",
      "Epoch 5/20, Batch Loss: 0.18770542740821838\n",
      "Epoch 5/20, Batch Loss: 0.20191173255443573\n",
      "Epoch 5/20, Batch Loss: 0.3346320390701294\n",
      "Epoch 5/20, Batch Loss: 0.4563556909561157\n",
      "Epoch 5/20, Batch Loss: 0.3949524462223053\n",
      "Epoch 5/20, Batch Loss: 0.7729367613792419\n",
      "Epoch 5/20, Batch Loss: 0.2752705514431\n",
      "Epoch 5/20, Batch Loss: 0.37043195962905884\n",
      "Epoch 5/20, Batch Loss: 0.3984106183052063\n",
      "Epoch 5/20, Batch Loss: 0.1942795068025589\n",
      "Epoch 5/20, Batch Loss: 0.32761260867118835\n",
      "Epoch 5/20, Batch Loss: 0.4124164283275604\n",
      "Epoch 5/20, Batch Loss: 0.21657389402389526\n",
      "Epoch 5/20, Batch Loss: 0.26489248871803284\n",
      "Epoch 5/20, Batch Loss: 0.2586462199687958\n",
      "Epoch 5/20, Batch Loss: 0.46787765622138977\n",
      "Epoch 5/20, Batch Loss: 0.35934504866600037\n",
      "Epoch 5/20, Batch Loss: 0.3304212689399719\n",
      "Epoch 5/20, Batch Loss: 0.3301709294319153\n",
      "Epoch 5/20, Batch Loss: 0.29068833589553833\n",
      "Epoch 5/20, Batch Loss: 0.35873979330062866\n",
      "Epoch 5/20, Batch Loss: 0.3530912399291992\n",
      "Epoch 5/20, Batch Loss: 0.3248525559902191\n",
      "Epoch 5/20, Batch Loss: 0.09985993802547455\n",
      "Epoch 5/20, Batch Loss: 0.32791998982429504\n",
      "Epoch 5/20, Batch Loss: 0.23622523248195648\n",
      "Epoch 5/20, Batch Loss: 0.21754826605319977\n",
      "Epoch 5/20, Batch Loss: 0.30083709955215454\n",
      "Epoch 5/20, Batch Loss: 0.48626092076301575\n",
      "Epoch 5/20, Batch Loss: 0.25160324573516846\n",
      "Epoch 5/20, Batch Loss: 0.1950279325246811\n",
      "Epoch 5/20, Batch Loss: 0.40744921565055847\n",
      "Epoch 5/20, Batch Loss: 0.4377139210700989\n",
      "Epoch 5/20, Batch Loss: 0.23749250173568726\n",
      "Epoch 5/20, Batch Loss: 0.6576302647590637\n",
      "Epoch 5/20, Batch Loss: 0.49064695835113525\n",
      "Epoch 5/20, Batch Loss: 0.3543607294559479\n",
      "Epoch 5/20, Batch Loss: 0.3055451810359955\n",
      "Epoch 5/20, Batch Loss: 0.06980021297931671\n",
      "Epoch 5/20, Batch Loss: 0.3169669508934021\n",
      "Epoch 5/20, Batch Loss: 0.31450945138931274\n",
      "Epoch 5/20, Batch Loss: 0.19389764964580536\n",
      "Epoch 5/20, Batch Loss: 0.2343737632036209\n",
      "Epoch 5/20, Batch Loss: 0.09425857663154602\n",
      "Epoch 5/20, Batch Loss: 0.3447335362434387\n",
      "Epoch 5/20, Batch Loss: 0.0855615958571434\n",
      "Epoch 5/20, Batch Loss: 0.44618260860443115\n",
      "Epoch 5/20, Batch Loss: 0.2896250784397125\n",
      "Epoch 5/20, Batch Loss: 0.3657357692718506\n",
      "Epoch 5/20, Batch Loss: 0.2133539766073227\n",
      "Epoch 5/20, Batch Loss: 0.21400879323482513\n",
      "Epoch 5/20, Batch Loss: 0.13660822808742523\n",
      "Epoch 5/20, Batch Loss: 0.5593458414077759\n",
      "Epoch 5/20, Batch Loss: 0.4413197636604309\n",
      "Epoch 5/20, Batch Loss: 0.34137848019599915\n",
      "Epoch 5/20, Batch Loss: 0.4894925057888031\n",
      "Epoch 5/20, Batch Loss: 0.22907842695713043\n",
      "Epoch 5/20, Batch Loss: 0.33986178040504456\n",
      "Epoch 5/20, Batch Loss: 0.579827070236206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Batch Loss: 0.545096755027771\n",
      "Epoch 5/20, Batch Loss: 0.09181273728609085\n",
      "Epoch 5/20, Batch Loss: 0.1913040429353714\n",
      "Epoch 5/20, Batch Loss: 0.0846710130572319\n",
      "Epoch 5/20, Batch Loss: 0.3274506628513336\n",
      "Epoch 5/20, Batch Loss: 0.3061172068119049\n",
      "Epoch 5/20, Batch Loss: 0.3061218559741974\n",
      "Epoch 5/20, Batch Loss: 0.4521753489971161\n",
      "Epoch 5/20, Batch Loss: 0.24797192215919495\n",
      "Epoch 5/20, Batch Loss: 0.13990555703639984\n",
      "Epoch 5/20, Batch Loss: 0.10803885012865067\n",
      "Epoch 5/20, Batch Loss: 0.47448107600212097\n",
      "Epoch 5/20, Batch Loss: 0.5225085020065308\n",
      "Epoch 5/20, Batch Loss: 0.43173378705978394\n",
      "Epoch 5/20, Batch Loss: 0.4584067165851593\n",
      "Epoch 5/20, Batch Loss: 0.448576420545578\n",
      "Epoch 5/20, Batch Loss: 0.31164249777793884\n",
      "Epoch 5/20, Batch Loss: 0.15844422578811646\n",
      "Epoch 5/20, Batch Loss: 0.5310812592506409\n",
      "Epoch 5/20, Batch Loss: 0.20239347219467163\n",
      "Epoch 5/20, Batch Loss: 0.12966813147068024\n",
      "Epoch 5/20, Batch Loss: 0.38668692111968994\n",
      "Epoch 5/20, Batch Loss: 0.3709001839160919\n",
      "Epoch 5/20, Batch Loss: 0.28548064827919006\n",
      "Epoch 5/20, Batch Loss: 0.14179378747940063\n",
      "Epoch 5/20, Batch Loss: 0.24137118458747864\n",
      "Epoch 5/20, Batch Loss: 0.246331587433815\n",
      "Epoch 5/20, Batch Loss: 0.23055388033390045\n",
      "Epoch 5/20, Batch Loss: 0.24942311644554138\n",
      "Epoch 5/20, Batch Loss: 0.3451659679412842\n",
      "Epoch 5/20, Batch Loss: 0.24760429561138153\n",
      "Epoch 5/20, Batch Loss: 0.3400653898715973\n",
      "Epoch 5/20, Batch Loss: 0.19189779460430145\n",
      "Epoch 5/20, Batch Loss: 0.10164164006710052\n",
      "Epoch 5/20, Batch Loss: 0.46157827973365784\n",
      "Epoch 5/20, Batch Loss: 0.22410601377487183\n",
      "Epoch 5/20, Batch Loss: 0.22936977446079254\n",
      "Epoch 5/20, Batch Loss: 0.211915522813797\n",
      "Epoch 5/20, Batch Loss: 0.2089167833328247\n",
      "Epoch 5/20, Batch Loss: 0.3210214376449585\n",
      "Epoch 5/20, Batch Loss: 0.0945838987827301\n",
      "Epoch 5/20, Batch Loss: 0.46569427847862244\n",
      "Epoch 5/20, Batch Loss: 0.5144240260124207\n",
      "Epoch 5/20, Batch Loss: 0.2055829018354416\n",
      "Epoch 5/20, Batch Loss: 0.0649302676320076\n",
      "Epoch 5/20, Batch Loss: 0.21912410855293274\n",
      "Epoch 5/20, Batch Loss: 0.3770757019519806\n",
      "Epoch 5/20, Batch Loss: 0.391310453414917\n",
      "Epoch 5/20, Batch Loss: 0.44112738966941833\n",
      "Epoch 5/20, Batch Loss: 0.4826209545135498\n",
      "Epoch 5/20, Batch Loss: 0.21014191210269928\n",
      "Epoch 5/20, Batch Loss: 0.165666401386261\n",
      "Epoch 5/20, Batch Loss: 0.09104595333337784\n",
      "Epoch 5/20, Batch Loss: 0.4416215419769287\n",
      "Epoch 5/20, Batch Loss: 0.3327914774417877\n",
      "Epoch 5/20, Batch Loss: 0.3334006369113922\n",
      "Epoch 5/20, Batch Loss: 0.6991299390792847\n",
      "Epoch 5/20, Batch Loss: 0.48332417011260986\n",
      "Epoch 5/20, Batch Loss: 0.4295099973678589\n",
      "Epoch 5/20, Batch Loss: 0.69790118932724\n",
      "Epoch 5/20, Batch Loss: 0.4405350089073181\n",
      "Epoch 5/20, Batch Loss: 0.23894189298152924\n",
      "Epoch 5/20, Batch Loss: 0.17309033870697021\n",
      "Epoch 5/20, Average Training Loss: 0.3161687139815431\n",
      "Model saved for epoch 5 at Fine_tuned_epoch5_BERT_Large.pt\n",
      "Validation Accuracy for epoch 5: 0.8884408602150538\n",
      "Epoch 6/20, Batch Loss: 0.0893411710858345\n",
      "Epoch 6/20, Batch Loss: 0.3184751272201538\n",
      "Epoch 6/20, Batch Loss: 0.11674381047487259\n",
      "Epoch 6/20, Batch Loss: 0.32796865701675415\n",
      "Epoch 6/20, Batch Loss: 0.3402610421180725\n",
      "Epoch 6/20, Batch Loss: 0.2250061184167862\n",
      "Epoch 6/20, Batch Loss: 0.09513000398874283\n",
      "Epoch 6/20, Batch Loss: 0.4142739772796631\n",
      "Epoch 6/20, Batch Loss: 0.2471618950366974\n",
      "Epoch 6/20, Batch Loss: 0.6235124468803406\n",
      "Epoch 6/20, Batch Loss: 0.21949970722198486\n",
      "Epoch 6/20, Batch Loss: 0.396006315946579\n",
      "Epoch 6/20, Batch Loss: 0.41693922877311707\n",
      "Epoch 6/20, Batch Loss: 0.33007851243019104\n",
      "Epoch 6/20, Batch Loss: 0.1274622678756714\n",
      "Epoch 6/20, Batch Loss: 0.2121690958738327\n",
      "Epoch 6/20, Batch Loss: 0.09286119788885117\n",
      "Epoch 6/20, Batch Loss: 0.25981464982032776\n",
      "Epoch 6/20, Batch Loss: 0.1406404674053192\n",
      "Epoch 6/20, Batch Loss: 0.46256086230278015\n",
      "Epoch 6/20, Batch Loss: 0.3333373963832855\n",
      "Epoch 6/20, Batch Loss: 0.4770660698413849\n",
      "Epoch 6/20, Batch Loss: 0.21963009238243103\n",
      "Epoch 6/20, Batch Loss: 0.39649736881256104\n",
      "Epoch 6/20, Batch Loss: 0.588605523109436\n",
      "Epoch 6/20, Batch Loss: 0.2510729432106018\n",
      "Epoch 6/20, Batch Loss: 0.42855125665664673\n",
      "Epoch 6/20, Batch Loss: 0.4555681049823761\n",
      "Epoch 6/20, Batch Loss: 0.39558354020118713\n",
      "Epoch 6/20, Batch Loss: 0.3464822471141815\n",
      "Epoch 6/20, Batch Loss: 0.4411877989768982\n",
      "Epoch 6/20, Batch Loss: 0.3097586929798126\n",
      "Epoch 6/20, Batch Loss: 0.5222010016441345\n",
      "Epoch 6/20, Batch Loss: 0.5183005332946777\n",
      "Epoch 6/20, Batch Loss: 0.4234830439090729\n",
      "Epoch 6/20, Batch Loss: 0.12768757343292236\n",
      "Epoch 6/20, Batch Loss: 0.27946171164512634\n",
      "Epoch 6/20, Batch Loss: 0.21400688588619232\n",
      "Epoch 6/20, Batch Loss: 0.08779741823673248\n",
      "Epoch 6/20, Batch Loss: 0.4723505973815918\n",
      "Epoch 6/20, Batch Loss: 0.20352263748645782\n",
      "Epoch 6/20, Batch Loss: 0.521016538143158\n",
      "Epoch 6/20, Batch Loss: 0.3065071105957031\n",
      "Epoch 6/20, Batch Loss: 0.32347163558006287\n",
      "Epoch 6/20, Batch Loss: 0.11952311545610428\n",
      "Epoch 6/20, Batch Loss: 0.3284258544445038\n",
      "Epoch 6/20, Batch Loss: 0.3281368613243103\n",
      "Epoch 6/20, Batch Loss: 0.14002671837806702\n",
      "Epoch 6/20, Batch Loss: 0.326481431722641\n",
      "Epoch 6/20, Batch Loss: 0.427619606256485\n",
      "Epoch 6/20, Batch Loss: 0.26104027032852173\n",
      "Epoch 6/20, Batch Loss: 0.12785515189170837\n",
      "Epoch 6/20, Batch Loss: 0.49034371972084045\n",
      "Epoch 6/20, Batch Loss: 0.2694365680217743\n",
      "Epoch 6/20, Batch Loss: 0.3926844000816345\n",
      "Epoch 6/20, Batch Loss: 0.11257778108119965\n",
      "Epoch 6/20, Batch Loss: 0.33199411630630493\n",
      "Epoch 6/20, Batch Loss: 0.24061353504657745\n",
      "Epoch 6/20, Batch Loss: 0.09112317860126495\n",
      "Epoch 6/20, Batch Loss: 0.4398581087589264\n",
      "Epoch 6/20, Batch Loss: 0.516156017780304\n",
      "Epoch 6/20, Batch Loss: 0.2123717963695526\n",
      "Epoch 6/20, Batch Loss: 0.6790145039558411\n",
      "Epoch 6/20, Batch Loss: 0.31588464975357056\n",
      "Epoch 6/20, Batch Loss: 0.5670265555381775\n",
      "Epoch 6/20, Batch Loss: 0.12970086932182312\n",
      "Epoch 6/20, Batch Loss: 0.2890917956829071\n",
      "Epoch 6/20, Batch Loss: 0.31544777750968933\n",
      "Epoch 6/20, Batch Loss: 0.33391040563583374\n",
      "Epoch 6/20, Batch Loss: 0.22118134796619415\n",
      "Epoch 6/20, Batch Loss: 0.2448689341545105\n",
      "Epoch 6/20, Batch Loss: 0.35534870624542236\n",
      "Epoch 6/20, Batch Loss: 0.10515575110912323\n",
      "Epoch 6/20, Batch Loss: 0.28465381264686584\n",
      "Epoch 6/20, Batch Loss: 0.08373671025037766\n",
      "Epoch 6/20, Batch Loss: 0.3316781222820282\n",
      "Epoch 6/20, Batch Loss: 0.1071629524230957\n",
      "Epoch 6/20, Batch Loss: 0.24973450601100922\n",
      "Epoch 6/20, Batch Loss: 0.6014077067375183\n",
      "Epoch 6/20, Batch Loss: 0.44612136483192444\n",
      "Epoch 6/20, Batch Loss: 0.3416263461112976\n",
      "Epoch 6/20, Batch Loss: 0.5341317653656006\n",
      "Epoch 6/20, Batch Loss: 0.5460156798362732\n",
      "Epoch 6/20, Batch Loss: 0.26548874378204346\n",
      "Epoch 6/20, Batch Loss: 0.24661551415920258\n",
      "Epoch 6/20, Batch Loss: 0.32853326201438904\n",
      "Epoch 6/20, Batch Loss: 0.32431256771087646\n",
      "Epoch 6/20, Batch Loss: 0.508582353591919\n",
      "Epoch 6/20, Batch Loss: 0.44115182757377625\n",
      "Epoch 6/20, Batch Loss: 0.2054506093263626\n",
      "Epoch 6/20, Batch Loss: 0.12908773124217987\n",
      "Epoch 6/20, Batch Loss: 0.3247290551662445\n",
      "Epoch 6/20, Batch Loss: 0.3000487983226776\n",
      "Epoch 6/20, Batch Loss: 0.2726089060306549\n",
      "Epoch 6/20, Batch Loss: 0.14152811467647552\n",
      "Epoch 6/20, Batch Loss: 0.4290930926799774\n",
      "Epoch 6/20, Batch Loss: 0.21509012579917908\n",
      "Epoch 6/20, Batch Loss: 0.1149400845170021\n",
      "Epoch 6/20, Batch Loss: 0.3324403166770935\n",
      "Epoch 6/20, Batch Loss: 0.40993955731391907\n",
      "Epoch 6/20, Batch Loss: 0.7144251465797424\n",
      "Epoch 6/20, Batch Loss: 0.326016902923584\n",
      "Epoch 6/20, Batch Loss: 0.20054177939891815\n",
      "Epoch 6/20, Batch Loss: 0.2538832426071167\n",
      "Epoch 6/20, Batch Loss: 0.3520175814628601\n",
      "Epoch 6/20, Batch Loss: 0.13680697977542877\n",
      "Epoch 6/20, Batch Loss: 0.07948614656925201\n",
      "Epoch 6/20, Batch Loss: 0.23569481074810028\n",
      "Epoch 6/20, Batch Loss: 0.22493940591812134\n",
      "Epoch 6/20, Batch Loss: 0.2794634699821472\n",
      "Epoch 6/20, Batch Loss: 0.3005642890930176\n",
      "Epoch 6/20, Batch Loss: 0.34931066632270813\n",
      "Epoch 6/20, Batch Loss: 0.4778577983379364\n",
      "Epoch 6/20, Batch Loss: 0.20400767028331757\n",
      "Epoch 6/20, Batch Loss: 0.41312775015830994\n",
      "Epoch 6/20, Batch Loss: 0.26802489161491394\n",
      "Epoch 6/20, Batch Loss: 0.3350558578968048\n",
      "Epoch 6/20, Batch Loss: 0.42968249320983887\n",
      "Epoch 6/20, Batch Loss: 0.09576664119958878\n",
      "Epoch 6/20, Batch Loss: 0.2107582986354828\n",
      "Epoch 6/20, Batch Loss: 0.19703345000743866\n",
      "Epoch 6/20, Batch Loss: 0.6752049326896667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Batch Loss: 0.05702095851302147\n",
      "Epoch 6/20, Average Training Loss: 0.3101183300519862\n",
      "Model saved for epoch 6 at Fine_tuned_epoch6_BERT_Large.pt\n",
      "Validation Accuracy for epoch 6: 0.8915770609318997\n",
      "Epoch 7/20, Batch Loss: 0.243715301156044\n",
      "Epoch 7/20, Batch Loss: 0.07560081034898758\n",
      "Epoch 7/20, Batch Loss: 0.19830553233623505\n",
      "Epoch 7/20, Batch Loss: 0.15946219861507416\n",
      "Epoch 7/20, Batch Loss: 0.10080860555171967\n",
      "Epoch 7/20, Batch Loss: 0.133074089884758\n",
      "Epoch 7/20, Batch Loss: 0.5572410225868225\n",
      "Epoch 7/20, Batch Loss: 0.34711337089538574\n",
      "Epoch 7/20, Batch Loss: 0.3354780972003937\n",
      "Epoch 7/20, Batch Loss: 0.2137962281703949\n",
      "Epoch 7/20, Batch Loss: 0.4720677435398102\n",
      "Epoch 7/20, Batch Loss: 0.21800081431865692\n",
      "Epoch 7/20, Batch Loss: 0.2266256958246231\n",
      "Epoch 7/20, Batch Loss: 0.28760331869125366\n",
      "Epoch 7/20, Batch Loss: 0.22590728104114532\n",
      "Epoch 7/20, Batch Loss: 0.23111240565776825\n",
      "Epoch 7/20, Batch Loss: 0.20700323581695557\n",
      "Epoch 7/20, Batch Loss: 0.18938034772872925\n",
      "Epoch 7/20, Batch Loss: 0.22062483429908752\n",
      "Epoch 7/20, Batch Loss: 0.20465174317359924\n",
      "Epoch 7/20, Batch Loss: 0.45747360587120056\n",
      "Epoch 7/20, Batch Loss: 0.427249014377594\n",
      "Epoch 7/20, Batch Loss: 0.21506665647029877\n",
      "Epoch 7/20, Batch Loss: 0.4428882896900177\n",
      "Epoch 7/20, Batch Loss: 0.7089770436286926\n",
      "Epoch 7/20, Batch Loss: 0.21041826903820038\n",
      "Epoch 7/20, Batch Loss: 0.3595762848854065\n",
      "Epoch 7/20, Batch Loss: 0.4261397123336792\n",
      "Epoch 7/20, Batch Loss: 0.41314002871513367\n",
      "Epoch 7/20, Batch Loss: 0.41178178787231445\n",
      "Epoch 7/20, Batch Loss: 0.11948065459728241\n",
      "Epoch 7/20, Batch Loss: 0.4311729073524475\n",
      "Epoch 7/20, Batch Loss: 0.23208162188529968\n",
      "Epoch 7/20, Batch Loss: 0.31940221786499023\n",
      "Epoch 7/20, Batch Loss: 0.29994845390319824\n",
      "Epoch 7/20, Batch Loss: 0.2206665277481079\n",
      "Epoch 7/20, Batch Loss: 0.22556443512439728\n",
      "Epoch 7/20, Batch Loss: 0.30824339389801025\n",
      "Epoch 7/20, Batch Loss: 0.1009649932384491\n",
      "Epoch 7/20, Batch Loss: 0.20382589101791382\n",
      "Epoch 7/20, Batch Loss: 0.1255805343389511\n",
      "Epoch 7/20, Batch Loss: 0.3063945472240448\n",
      "Epoch 7/20, Batch Loss: 0.234971284866333\n",
      "Epoch 7/20, Batch Loss: 0.08411721885204315\n",
      "Epoch 7/20, Batch Loss: 0.41833803057670593\n",
      "Epoch 7/20, Batch Loss: 0.2199951857328415\n",
      "Epoch 7/20, Batch Loss: 0.22243748605251312\n",
      "Epoch 7/20, Batch Loss: 0.33641472458839417\n",
      "Epoch 7/20, Batch Loss: 0.20865781605243683\n",
      "Epoch 7/20, Batch Loss: 0.3253045976161957\n",
      "Epoch 7/20, Batch Loss: 0.667080283164978\n",
      "Epoch 7/20, Batch Loss: 0.3459756076335907\n",
      "Epoch 7/20, Batch Loss: 0.20390675961971283\n",
      "Epoch 7/20, Batch Loss: 0.22698937356472015\n",
      "Epoch 7/20, Batch Loss: 0.6707404255867004\n",
      "Epoch 7/20, Batch Loss: 0.5199450254440308\n",
      "Epoch 7/20, Batch Loss: 0.3677242696285248\n",
      "Epoch 7/20, Batch Loss: 0.38936859369277954\n",
      "Epoch 7/20, Batch Loss: 0.23508629202842712\n",
      "Epoch 7/20, Batch Loss: 0.552669107913971\n",
      "Epoch 7/20, Batch Loss: 0.4408597946166992\n",
      "Epoch 7/20, Batch Loss: 0.20212775468826294\n",
      "Epoch 7/20, Batch Loss: 0.10080865025520325\n",
      "Epoch 7/20, Batch Loss: 0.5123872756958008\n",
      "Epoch 7/20, Batch Loss: 0.6474968791007996\n",
      "Epoch 7/20, Batch Loss: 0.4027073383331299\n",
      "Epoch 7/20, Batch Loss: 0.2318088859319687\n",
      "Epoch 7/20, Batch Loss: 0.2270979881286621\n",
      "Epoch 7/20, Batch Loss: 0.2990829646587372\n",
      "Epoch 7/20, Batch Loss: 0.29308173060417175\n",
      "Epoch 7/20, Batch Loss: 0.3277474045753479\n",
      "Epoch 7/20, Batch Loss: 0.8185518383979797\n",
      "Epoch 7/20, Batch Loss: 0.2574891448020935\n",
      "Epoch 7/20, Batch Loss: 0.40878725051879883\n",
      "Epoch 7/20, Batch Loss: 0.15795697271823883\n",
      "Epoch 7/20, Batch Loss: 0.3799012005329132\n",
      "Epoch 7/20, Batch Loss: 0.23112823069095612\n",
      "Epoch 7/20, Batch Loss: 0.3313709795475006\n",
      "Epoch 7/20, Batch Loss: 0.3047163784503937\n",
      "Epoch 7/20, Batch Loss: 0.09773200750350952\n",
      "Epoch 7/20, Batch Loss: 0.4361972510814667\n",
      "Epoch 7/20, Batch Loss: 0.12458690255880356\n",
      "Epoch 7/20, Batch Loss: 0.4746762812137604\n",
      "Epoch 7/20, Batch Loss: 0.2998289167881012\n",
      "Epoch 7/20, Batch Loss: 0.4381468892097473\n",
      "Epoch 7/20, Batch Loss: 0.2460601031780243\n",
      "Epoch 7/20, Batch Loss: 0.5538107752799988\n",
      "Epoch 7/20, Batch Loss: 0.10256435722112656\n",
      "Epoch 7/20, Batch Loss: 0.43880897760391235\n",
      "Epoch 7/20, Batch Loss: 0.6508405804634094\n",
      "Epoch 7/20, Batch Loss: 0.3104332983493805\n",
      "Epoch 7/20, Batch Loss: 0.5703058242797852\n",
      "Epoch 7/20, Batch Loss: 0.2181331068277359\n",
      "Epoch 7/20, Batch Loss: 0.48041340708732605\n",
      "Epoch 7/20, Batch Loss: 0.11803838610649109\n",
      "Epoch 7/20, Batch Loss: 0.11413787305355072\n",
      "Epoch 7/20, Batch Loss: 0.32415756583213806\n",
      "Epoch 7/20, Batch Loss: 0.2077980637550354\n",
      "Epoch 7/20, Batch Loss: 0.4920874834060669\n",
      "Epoch 7/20, Batch Loss: 0.20651650428771973\n",
      "Epoch 7/20, Batch Loss: 0.2616318464279175\n",
      "Epoch 7/20, Batch Loss: 0.09673237800598145\n",
      "Epoch 7/20, Batch Loss: 0.23044531047344208\n",
      "Epoch 7/20, Batch Loss: 0.3563656210899353\n",
      "Epoch 7/20, Batch Loss: 0.20674216747283936\n",
      "Epoch 7/20, Batch Loss: 0.37133094668388367\n",
      "Epoch 7/20, Batch Loss: 0.5467156171798706\n",
      "Epoch 7/20, Batch Loss: 0.4267037510871887\n",
      "Epoch 7/20, Batch Loss: 0.24117042124271393\n",
      "Epoch 7/20, Batch Loss: 0.4223583936691284\n",
      "Epoch 7/20, Batch Loss: 0.32560715079307556\n",
      "Epoch 7/20, Batch Loss: 0.21326451003551483\n",
      "Epoch 7/20, Batch Loss: 0.2756839692592621\n",
      "Epoch 7/20, Batch Loss: 0.260299414396286\n",
      "Epoch 7/20, Batch Loss: 0.2152404636144638\n",
      "Epoch 7/20, Batch Loss: 0.13369612395763397\n",
      "Epoch 7/20, Batch Loss: 0.358846515417099\n",
      "Epoch 7/20, Batch Loss: 0.2202398031949997\n",
      "Epoch 7/20, Batch Loss: 0.6797460913658142\n",
      "Epoch 7/20, Batch Loss: 0.12315565347671509\n",
      "Epoch 7/20, Batch Loss: 0.23016588389873505\n",
      "Epoch 7/20, Batch Loss: 0.5236035585403442\n",
      "Epoch 7/20, Batch Loss: 0.12202172726392746\n",
      "Epoch 7/20, Average Training Loss: 0.31191448915780073\n",
      "Model saved for epoch 7 at Fine_tuned_epoch7_BERT_Large.pt\n",
      "Validation Accuracy for epoch 7: 0.8900089605734768\n",
      "Epoch 8/20, Batch Loss: 0.520840585231781\n",
      "Epoch 8/20, Batch Loss: 0.3077932894229889\n",
      "Epoch 8/20, Batch Loss: 0.30524206161499023\n",
      "Epoch 8/20, Batch Loss: 0.31232020258903503\n",
      "Epoch 8/20, Batch Loss: 0.4331210255622864\n",
      "Epoch 8/20, Batch Loss: 0.20219871401786804\n",
      "Epoch 8/20, Batch Loss: 0.33099856972694397\n",
      "Epoch 8/20, Batch Loss: 0.3194858729839325\n",
      "Epoch 8/20, Batch Loss: 0.3430924415588379\n",
      "Epoch 8/20, Batch Loss: 0.24339234828948975\n",
      "Epoch 8/20, Batch Loss: 0.08964259177446365\n",
      "Epoch 8/20, Batch Loss: 0.23135657608509064\n",
      "Epoch 8/20, Batch Loss: 0.09001194685697556\n",
      "Epoch 8/20, Batch Loss: 0.22174233198165894\n",
      "Epoch 8/20, Batch Loss: 0.07205545157194138\n",
      "Epoch 8/20, Batch Loss: 0.06634413450956345\n",
      "Epoch 8/20, Batch Loss: 0.2943771779537201\n",
      "Epoch 8/20, Batch Loss: 0.4762575924396515\n",
      "Epoch 8/20, Batch Loss: 0.4037741422653198\n",
      "Epoch 8/20, Batch Loss: 0.43470650911331177\n",
      "Epoch 8/20, Batch Loss: 0.20068249106407166\n",
      "Epoch 8/20, Batch Loss: 0.09115684032440186\n",
      "Epoch 8/20, Batch Loss: 0.34733352065086365\n",
      "Epoch 8/20, Batch Loss: 0.22651280462741852\n",
      "Epoch 8/20, Batch Loss: 0.22355963289737701\n",
      "Epoch 8/20, Batch Loss: 0.21283024549484253\n",
      "Epoch 8/20, Batch Loss: 0.3304761052131653\n",
      "Epoch 8/20, Batch Loss: 0.456620991230011\n",
      "Epoch 8/20, Batch Loss: 0.4462027847766876\n",
      "Epoch 8/20, Batch Loss: 0.30428171157836914\n",
      "Epoch 8/20, Batch Loss: 0.33626845479011536\n",
      "Epoch 8/20, Batch Loss: 0.352893203496933\n",
      "Epoch 8/20, Batch Loss: 0.3423125147819519\n",
      "Epoch 8/20, Batch Loss: 0.22491703927516937\n",
      "Epoch 8/20, Batch Loss: 0.15088339149951935\n",
      "Epoch 8/20, Batch Loss: 0.43838757276535034\n",
      "Epoch 8/20, Batch Loss: 0.3962589502334595\n",
      "Epoch 8/20, Batch Loss: 0.06727229803800583\n",
      "Epoch 8/20, Batch Loss: 0.19622884690761566\n",
      "Epoch 8/20, Batch Loss: 0.45314350724220276\n",
      "Epoch 8/20, Batch Loss: 0.23033776879310608\n",
      "Epoch 8/20, Batch Loss: 0.21813103556632996\n",
      "Epoch 8/20, Batch Loss: 0.2244226634502411\n",
      "Epoch 8/20, Batch Loss: 0.7699992060661316\n",
      "Epoch 8/20, Batch Loss: 0.45057010650634766\n",
      "Epoch 8/20, Batch Loss: 0.07992375642061234\n",
      "Epoch 8/20, Batch Loss: 0.2262982279062271\n",
      "Epoch 8/20, Batch Loss: 0.47054144740104675\n",
      "Epoch 8/20, Batch Loss: 0.29776403307914734\n",
      "Epoch 8/20, Batch Loss: 0.47851911187171936\n",
      "Epoch 8/20, Batch Loss: 0.10871090739965439\n",
      "Epoch 8/20, Batch Loss: 0.43720346689224243\n",
      "Epoch 8/20, Batch Loss: 0.3160046637058258\n",
      "Epoch 8/20, Batch Loss: 0.6651925444602966\n",
      "Epoch 8/20, Batch Loss: 0.3139365613460541\n",
      "Epoch 8/20, Batch Loss: 0.19559811055660248\n",
      "Epoch 8/20, Batch Loss: 0.2624640464782715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Batch Loss: 0.48426830768585205\n",
      "Epoch 8/20, Batch Loss: 0.262050062417984\n",
      "Epoch 8/20, Batch Loss: 0.3231600821018219\n",
      "Epoch 8/20, Batch Loss: 0.2254493236541748\n",
      "Epoch 8/20, Batch Loss: 0.22737933695316315\n",
      "Epoch 8/20, Batch Loss: 0.23151002824306488\n",
      "Epoch 8/20, Batch Loss: 0.3516201674938202\n",
      "Epoch 8/20, Batch Loss: 0.16941368579864502\n",
      "Epoch 8/20, Batch Loss: 0.5181132555007935\n",
      "Epoch 8/20, Batch Loss: 0.1318100094795227\n",
      "Epoch 8/20, Batch Loss: 0.2100490927696228\n",
      "Epoch 8/20, Batch Loss: 0.1976119875907898\n",
      "Epoch 8/20, Batch Loss: 0.44589486718177795\n",
      "Epoch 8/20, Batch Loss: 0.40800684690475464\n",
      "Epoch 8/20, Batch Loss: 0.5147242546081543\n",
      "Epoch 8/20, Batch Loss: 0.23760978877544403\n",
      "Epoch 8/20, Batch Loss: 0.2901090085506439\n",
      "Epoch 8/20, Batch Loss: 0.6227566599845886\n",
      "Epoch 8/20, Batch Loss: 0.4298724830150604\n",
      "Epoch 8/20, Batch Loss: 0.4868530333042145\n",
      "Epoch 8/20, Batch Loss: 0.35648322105407715\n",
      "Epoch 8/20, Batch Loss: 0.5221354961395264\n",
      "Epoch 8/20, Batch Loss: 0.09898529201745987\n",
      "Epoch 8/20, Batch Loss: 0.32452839612960815\n",
      "Epoch 8/20, Batch Loss: 0.3584569990634918\n",
      "Epoch 8/20, Batch Loss: 0.35908353328704834\n",
      "Epoch 8/20, Batch Loss: 0.44014981389045715\n",
      "Epoch 8/20, Batch Loss: 0.41086485981941223\n",
      "Epoch 8/20, Batch Loss: 0.27963849902153015\n",
      "Epoch 8/20, Batch Loss: 0.4008459448814392\n",
      "Epoch 8/20, Batch Loss: 0.25049251317977905\n",
      "Epoch 8/20, Batch Loss: 0.2643826901912689\n",
      "Epoch 8/20, Batch Loss: 0.24682624638080597\n",
      "Epoch 8/20, Batch Loss: 0.3294948935508728\n",
      "Epoch 8/20, Batch Loss: 0.21384230256080627\n",
      "Epoch 8/20, Batch Loss: 0.1301424503326416\n",
      "Epoch 8/20, Batch Loss: 0.13207682967185974\n",
      "Epoch 8/20, Batch Loss: 0.4623754918575287\n",
      "Epoch 8/20, Batch Loss: 0.3432750999927521\n",
      "Epoch 8/20, Batch Loss: 0.3016490042209625\n",
      "Epoch 8/20, Batch Loss: 0.365211546421051\n",
      "Epoch 8/20, Batch Loss: 0.3259351849555969\n",
      "Epoch 8/20, Batch Loss: 0.3001263439655304\n",
      "Epoch 8/20, Batch Loss: 0.19453224539756775\n",
      "Epoch 8/20, Batch Loss: 0.13536520302295685\n",
      "Epoch 8/20, Batch Loss: 0.32061147689819336\n",
      "Epoch 8/20, Batch Loss: 0.09650516510009766\n",
      "Epoch 8/20, Batch Loss: 0.5077590346336365\n",
      "Epoch 8/20, Batch Loss: 0.4289877116680145\n",
      "Epoch 8/20, Batch Loss: 0.24993789196014404\n",
      "Epoch 8/20, Batch Loss: 0.22388744354248047\n",
      "Epoch 8/20, Batch Loss: 0.33023571968078613\n",
      "Epoch 8/20, Batch Loss: 0.08768466860055923\n",
      "Epoch 8/20, Batch Loss: 0.4135100543498993\n",
      "Epoch 8/20, Batch Loss: 0.31747397780418396\n",
      "Epoch 8/20, Batch Loss: 0.4464413523674011\n",
      "Epoch 8/20, Batch Loss: 0.19139735400676727\n",
      "Epoch 8/20, Batch Loss: 0.2383033037185669\n",
      "Epoch 8/20, Batch Loss: 0.21141208708286285\n",
      "Epoch 8/20, Batch Loss: 0.46212559938430786\n",
      "Epoch 8/20, Batch Loss: 0.09542597085237503\n",
      "Epoch 8/20, Batch Loss: 0.09084363281726837\n",
      "Epoch 8/20, Batch Loss: 0.3950287103652954\n",
      "Epoch 8/20, Batch Loss: 0.3942491412162781\n",
      "Epoch 8/20, Batch Loss: 0.5465591549873352\n",
      "Epoch 8/20, Batch Loss: 0.6190610527992249\n",
      "Epoch 8/20, Average Training Loss: 0.3107739594893727\n",
      "Model saved for epoch 8 at Fine_tuned_epoch8_BERT_Large.pt\n",
      "Validation Accuracy for epoch 8: 0.8915770609318997\n",
      "Epoch 9/20, Batch Loss: 0.18275168538093567\n",
      "Epoch 9/20, Batch Loss: 0.31307581067085266\n",
      "Epoch 9/20, Batch Loss: 0.3046298325061798\n",
      "Epoch 9/20, Batch Loss: 0.5170636177062988\n",
      "Epoch 9/20, Batch Loss: 0.25003790855407715\n",
      "Epoch 9/20, Batch Loss: 0.26957669854164124\n",
      "Epoch 9/20, Batch Loss: 0.505816638469696\n",
      "Epoch 9/20, Batch Loss: 0.5325486063957214\n",
      "Epoch 9/20, Batch Loss: 0.23151849210262299\n",
      "Epoch 9/20, Batch Loss: 0.22569242119789124\n",
      "Epoch 9/20, Batch Loss: 0.4888443648815155\n",
      "Epoch 9/20, Batch Loss: 0.3354874551296234\n",
      "Epoch 9/20, Batch Loss: 0.3107972741127014\n",
      "Epoch 9/20, Batch Loss: 0.3363298177719116\n",
      "Epoch 9/20, Batch Loss: 0.3103901445865631\n",
      "Epoch 9/20, Batch Loss: 0.32412412762641907\n",
      "Epoch 9/20, Batch Loss: 0.23813509941101074\n",
      "Epoch 9/20, Batch Loss: 0.22594918310642242\n",
      "Epoch 9/20, Batch Loss: 0.3439118266105652\n",
      "Epoch 9/20, Batch Loss: 0.30197426676750183\n",
      "Epoch 9/20, Batch Loss: 0.18062762916088104\n",
      "Epoch 9/20, Batch Loss: 0.22218158841133118\n",
      "Epoch 9/20, Batch Loss: 0.39024680852890015\n",
      "Epoch 9/20, Batch Loss: 0.21137061715126038\n",
      "Epoch 9/20, Batch Loss: 0.20559707283973694\n",
      "Epoch 9/20, Batch Loss: 0.3259894549846649\n",
      "Epoch 9/20, Batch Loss: 0.45851531624794006\n",
      "Epoch 9/20, Batch Loss: 0.4810671806335449\n",
      "Epoch 9/20, Batch Loss: 0.22862094640731812\n",
      "Epoch 9/20, Batch Loss: 0.559745728969574\n",
      "Epoch 9/20, Batch Loss: 0.45172935724258423\n",
      "Epoch 9/20, Batch Loss: 0.2599055767059326\n",
      "Epoch 9/20, Batch Loss: 0.13096126914024353\n",
      "Epoch 9/20, Batch Loss: 0.4188134968280792\n",
      "Epoch 9/20, Batch Loss: 0.17758998274803162\n",
      "Epoch 9/20, Batch Loss: 0.543773353099823\n",
      "Epoch 9/20, Batch Loss: 0.30882468819618225\n",
      "Epoch 9/20, Batch Loss: 0.4799109995365143\n",
      "Epoch 9/20, Batch Loss: 0.30075329542160034\n",
      "Epoch 9/20, Batch Loss: 0.42505592107772827\n",
      "Epoch 9/20, Batch Loss: 0.10574789345264435\n",
      "Epoch 9/20, Batch Loss: 0.3139202892780304\n",
      "Epoch 9/20, Batch Loss: 0.08059404045343399\n",
      "Epoch 9/20, Batch Loss: 0.24608324468135834\n",
      "Epoch 9/20, Batch Loss: 0.23764130473136902\n",
      "Epoch 9/20, Batch Loss: 0.45387136936187744\n",
      "Epoch 9/20, Batch Loss: 0.18980778753757477\n",
      "Epoch 9/20, Batch Loss: 0.5593441724777222\n",
      "Epoch 9/20, Batch Loss: 0.0913160964846611\n",
      "Epoch 9/20, Batch Loss: 0.317515105009079\n",
      "Epoch 9/20, Batch Loss: 0.11477772891521454\n",
      "Epoch 9/20, Batch Loss: 0.40921545028686523\n",
      "Epoch 9/20, Batch Loss: 0.18149220943450928\n",
      "Epoch 9/20, Batch Loss: 0.285432368516922\n",
      "Epoch 9/20, Batch Loss: 0.06856109201908112\n",
      "Epoch 9/20, Batch Loss: 0.7202852368354797\n",
      "Epoch 9/20, Batch Loss: 0.29248929023742676\n",
      "Epoch 9/20, Batch Loss: 0.2137192189693451\n",
      "Epoch 9/20, Batch Loss: 0.3975025415420532\n",
      "Epoch 9/20, Batch Loss: 0.3097189664840698\n",
      "Epoch 9/20, Batch Loss: 0.17682883143424988\n",
      "Epoch 9/20, Batch Loss: 0.5656383037567139\n",
      "Epoch 9/20, Batch Loss: 0.3452349305152893\n",
      "Epoch 9/20, Batch Loss: 0.1281266063451767\n",
      "Epoch 9/20, Batch Loss: 0.3396786153316498\n",
      "Epoch 9/20, Batch Loss: 0.35562193393707275\n",
      "Epoch 9/20, Batch Loss: 0.2520158290863037\n",
      "Epoch 9/20, Batch Loss: 0.08149410039186478\n",
      "Epoch 9/20, Batch Loss: 0.15029890835285187\n",
      "Epoch 9/20, Batch Loss: 0.08473725616931915\n",
      "Epoch 9/20, Batch Loss: 0.3046669363975525\n",
      "Epoch 9/20, Batch Loss: 0.2513664960861206\n",
      "Epoch 9/20, Batch Loss: 0.5073443055152893\n",
      "Epoch 9/20, Batch Loss: 0.09394437074661255\n",
      "Epoch 9/20, Batch Loss: 0.24334728717803955\n",
      "Epoch 9/20, Batch Loss: 0.2068554162979126\n",
      "Epoch 9/20, Batch Loss: 0.2164839804172516\n",
      "Epoch 9/20, Batch Loss: 0.5689091086387634\n",
      "Epoch 9/20, Batch Loss: 0.25457850098609924\n",
      "Epoch 9/20, Batch Loss: 0.19562937319278717\n",
      "Epoch 9/20, Batch Loss: 0.4728181064128876\n",
      "Epoch 9/20, Batch Loss: 0.3619414269924164\n",
      "Epoch 9/20, Batch Loss: 0.1939060389995575\n",
      "Epoch 9/20, Batch Loss: 0.20982927083969116\n",
      "Epoch 9/20, Batch Loss: 0.0807756781578064\n",
      "Epoch 9/20, Batch Loss: 0.2040059119462967\n",
      "Epoch 9/20, Batch Loss: 0.21928302943706512\n",
      "Epoch 9/20, Batch Loss: 0.18392223119735718\n",
      "Epoch 9/20, Batch Loss: 0.228529691696167\n",
      "Epoch 9/20, Batch Loss: 0.5735602974891663\n",
      "Epoch 9/20, Batch Loss: 0.21712461113929749\n",
      "Epoch 9/20, Batch Loss: 0.24958603084087372\n",
      "Epoch 9/20, Batch Loss: 0.37279993295669556\n",
      "Epoch 9/20, Batch Loss: 0.32183730602264404\n",
      "Epoch 9/20, Batch Loss: 0.2021876871585846\n",
      "Epoch 9/20, Batch Loss: 0.4246337115764618\n",
      "Epoch 9/20, Batch Loss: 0.3285605311393738\n",
      "Epoch 9/20, Batch Loss: 0.5343360304832458\n",
      "Epoch 9/20, Batch Loss: 0.29181742668151855\n",
      "Epoch 9/20, Batch Loss: 0.2598126232624054\n",
      "Epoch 9/20, Batch Loss: 0.3998635411262512\n",
      "Epoch 9/20, Batch Loss: 0.19301261007785797\n",
      "Epoch 9/20, Batch Loss: 0.3306993842124939\n",
      "Epoch 9/20, Batch Loss: 0.3957163095474243\n",
      "Epoch 9/20, Batch Loss: 0.48720425367355347\n",
      "Epoch 9/20, Batch Loss: 0.4024232029914856\n",
      "Epoch 9/20, Batch Loss: 0.19782620668411255\n",
      "Epoch 9/20, Batch Loss: 0.12376511096954346\n",
      "Epoch 9/20, Batch Loss: 0.1718910187482834\n",
      "Epoch 9/20, Batch Loss: 0.6545350551605225\n",
      "Epoch 9/20, Batch Loss: 0.34464797377586365\n",
      "Epoch 9/20, Batch Loss: 0.24669106304645538\n",
      "Epoch 9/20, Batch Loss: 0.07947282493114471\n",
      "Epoch 9/20, Batch Loss: 0.3848292827606201\n",
      "Epoch 9/20, Batch Loss: 0.22032935917377472\n",
      "Epoch 9/20, Batch Loss: 0.37837815284729004\n",
      "Epoch 9/20, Batch Loss: 0.4153890609741211\n",
      "Epoch 9/20, Batch Loss: 0.41027796268463135\n",
      "Epoch 9/20, Batch Loss: 0.47165825963020325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Batch Loss: 0.3356807827949524\n",
      "Epoch 9/20, Batch Loss: 0.37545090913772583\n",
      "Epoch 9/20, Batch Loss: 0.2232290357351303\n",
      "Epoch 9/20, Batch Loss: 0.09584622830152512\n",
      "Epoch 9/20, Average Training Loss: 0.304811847282619\n",
      "Model saved for epoch 9 at Fine_tuned_epoch9_BERT_Large.pt\n",
      "Validation Accuracy for epoch 9: 0.8915770609318997\n",
      "Epoch 10/20, Batch Loss: 0.11155981570482254\n",
      "Epoch 10/20, Batch Loss: 0.5969366431236267\n",
      "Epoch 10/20, Batch Loss: 0.30927595496177673\n",
      "Epoch 10/20, Batch Loss: 0.32679200172424316\n",
      "Epoch 10/20, Batch Loss: 0.21726766228675842\n",
      "Epoch 10/20, Batch Loss: 0.3540950417518616\n",
      "Epoch 10/20, Batch Loss: 0.2854781448841095\n",
      "Epoch 10/20, Batch Loss: 0.21634066104888916\n",
      "Epoch 10/20, Batch Loss: 0.30813390016555786\n",
      "Epoch 10/20, Batch Loss: 0.5944252014160156\n",
      "Epoch 10/20, Batch Loss: 0.2316715568304062\n",
      "Epoch 10/20, Batch Loss: 0.10060626268386841\n",
      "Epoch 10/20, Batch Loss: 0.4919961094856262\n",
      "Epoch 10/20, Batch Loss: 0.5033677816390991\n",
      "Epoch 10/20, Batch Loss: 0.38512957096099854\n",
      "Epoch 10/20, Batch Loss: 0.27120187878608704\n",
      "Epoch 10/20, Batch Loss: 0.0990108773112297\n",
      "Epoch 10/20, Batch Loss: 0.32855236530303955\n",
      "Epoch 10/20, Batch Loss: 0.3860168159008026\n",
      "Epoch 10/20, Batch Loss: 0.1272677779197693\n",
      "Epoch 10/20, Batch Loss: 0.11949898302555084\n",
      "Epoch 10/20, Batch Loss: 0.3082883358001709\n",
      "Epoch 10/20, Batch Loss: 0.5257461071014404\n",
      "Epoch 10/20, Batch Loss: 0.4232380986213684\n",
      "Epoch 10/20, Batch Loss: 0.2835841178894043\n",
      "Epoch 10/20, Batch Loss: 0.292031466960907\n",
      "Epoch 10/20, Batch Loss: 0.3797769248485565\n",
      "Epoch 10/20, Batch Loss: 0.23703962564468384\n",
      "Epoch 10/20, Batch Loss: 0.11931870877742767\n",
      "Epoch 10/20, Batch Loss: 0.33003392815589905\n",
      "Epoch 10/20, Batch Loss: 0.1952725201845169\n",
      "Epoch 10/20, Batch Loss: 0.305487722158432\n",
      "Epoch 10/20, Batch Loss: 0.36402106285095215\n",
      "Epoch 10/20, Batch Loss: 0.07268297672271729\n",
      "Epoch 10/20, Batch Loss: 0.32524508237838745\n",
      "Epoch 10/20, Batch Loss: 0.42066076397895813\n",
      "Epoch 10/20, Batch Loss: 0.18033693730831146\n",
      "Epoch 10/20, Batch Loss: 0.4604734778404236\n",
      "Epoch 10/20, Batch Loss: 0.2759149372577667\n",
      "Epoch 10/20, Batch Loss: 0.365611732006073\n",
      "Epoch 10/20, Batch Loss: 0.23903042078018188\n",
      "Epoch 10/20, Batch Loss: 0.29970020055770874\n",
      "Epoch 10/20, Batch Loss: 0.25595828890800476\n",
      "Epoch 10/20, Batch Loss: 0.22799967229366302\n",
      "Epoch 10/20, Batch Loss: 0.3740127980709076\n",
      "Epoch 10/20, Batch Loss: 0.19622036814689636\n",
      "Epoch 10/20, Batch Loss: 0.3728378415107727\n",
      "Epoch 10/20, Batch Loss: 0.40897002816200256\n",
      "Epoch 10/20, Batch Loss: 0.22219593822956085\n",
      "Epoch 10/20, Batch Loss: 0.31527888774871826\n",
      "Epoch 10/20, Batch Loss: 0.186874657869339\n",
      "Epoch 10/20, Batch Loss: 0.22598104178905487\n",
      "Epoch 10/20, Batch Loss: 0.2836458683013916\n",
      "Epoch 10/20, Batch Loss: 0.20441369712352753\n",
      "Epoch 10/20, Batch Loss: 0.28610461950302124\n",
      "Epoch 10/20, Batch Loss: 0.1817791908979416\n",
      "Epoch 10/20, Batch Loss: 0.2553918957710266\n",
      "Epoch 10/20, Batch Loss: 0.29444828629493713\n",
      "Epoch 10/20, Batch Loss: 0.310632586479187\n",
      "Epoch 10/20, Batch Loss: 0.18843935430049896\n",
      "Epoch 10/20, Batch Loss: 0.4020320475101471\n",
      "Epoch 10/20, Batch Loss: 0.10376499593257904\n",
      "Epoch 10/20, Batch Loss: 0.2008119821548462\n",
      "Epoch 10/20, Batch Loss: 0.1994592398405075\n",
      "Epoch 10/20, Batch Loss: 0.37120720744132996\n",
      "Epoch 10/20, Batch Loss: 0.39656496047973633\n",
      "Epoch 10/20, Batch Loss: 0.7192010283470154\n",
      "Epoch 10/20, Batch Loss: 0.7008822560310364\n",
      "Epoch 10/20, Batch Loss: 0.5452709794044495\n",
      "Epoch 10/20, Batch Loss: 0.19756031036376953\n",
      "Epoch 10/20, Batch Loss: 0.20337966084480286\n",
      "Epoch 10/20, Batch Loss: 0.2248849719762802\n",
      "Epoch 10/20, Batch Loss: 0.422810435295105\n",
      "Epoch 10/20, Batch Loss: 0.45440736413002014\n",
      "Epoch 10/20, Batch Loss: 0.35555189847946167\n",
      "Epoch 10/20, Batch Loss: 0.23747046291828156\n",
      "Epoch 10/20, Batch Loss: 0.24445664882659912\n",
      "Epoch 10/20, Batch Loss: 0.2960749864578247\n",
      "Epoch 10/20, Batch Loss: 0.47863441705703735\n",
      "Epoch 10/20, Batch Loss: 0.23975835740566254\n",
      "Epoch 10/20, Batch Loss: 0.17517465353012085\n",
      "Epoch 10/20, Batch Loss: 0.11991394311189651\n",
      "Epoch 10/20, Batch Loss: 0.3116309344768524\n",
      "Epoch 10/20, Batch Loss: 0.18708723783493042\n",
      "Epoch 10/20, Batch Loss: 0.4350319802761078\n",
      "Epoch 10/20, Batch Loss: 0.19854825735092163\n",
      "Epoch 10/20, Batch Loss: 0.19826073944568634\n",
      "Epoch 10/20, Batch Loss: 0.47855302691459656\n",
      "Epoch 10/20, Batch Loss: 0.2932787239551544\n",
      "Epoch 10/20, Batch Loss: 0.5384496450424194\n",
      "Epoch 10/20, Batch Loss: 0.3264026939868927\n",
      "Epoch 10/20, Batch Loss: 0.2700352370738983\n",
      "Epoch 10/20, Batch Loss: 0.32577285170555115\n",
      "Epoch 10/20, Batch Loss: 0.22773630917072296\n",
      "Epoch 10/20, Batch Loss: 0.2822796404361725\n",
      "Epoch 10/20, Batch Loss: 0.35026979446411133\n",
      "Epoch 10/20, Batch Loss: 0.36362695693969727\n",
      "Epoch 10/20, Batch Loss: 0.4643596410751343\n",
      "Epoch 10/20, Batch Loss: 0.2687090337276459\n",
      "Epoch 10/20, Batch Loss: 0.3221098482608795\n",
      "Epoch 10/20, Batch Loss: 0.2673133611679077\n",
      "Epoch 10/20, Batch Loss: 0.35029298067092896\n",
      "Epoch 10/20, Batch Loss: 0.27412721514701843\n",
      "Epoch 10/20, Batch Loss: 0.16630646586418152\n",
      "Epoch 10/20, Batch Loss: 0.22938129305839539\n",
      "Epoch 10/20, Batch Loss: 0.20989161729812622\n",
      "Epoch 10/20, Batch Loss: 0.11180653423070908\n",
      "Epoch 10/20, Batch Loss: 0.07129313796758652\n",
      "Epoch 10/20, Batch Loss: 0.6294488906860352\n",
      "Epoch 10/20, Batch Loss: 0.3709893226623535\n",
      "Epoch 10/20, Batch Loss: 0.3309800326824188\n",
      "Epoch 10/20, Batch Loss: 0.391666054725647\n",
      "Epoch 10/20, Batch Loss: 0.2899951934814453\n",
      "Epoch 10/20, Batch Loss: 0.6424381732940674\n",
      "Epoch 10/20, Batch Loss: 0.2697560489177704\n",
      "Epoch 10/20, Batch Loss: 0.2646118700504303\n",
      "Epoch 10/20, Batch Loss: 0.29132387042045593\n",
      "Epoch 10/20, Batch Loss: 0.18000546097755432\n",
      "Epoch 10/20, Batch Loss: 0.1278538852930069\n",
      "Epoch 10/20, Batch Loss: 0.20559705793857574\n",
      "Epoch 10/20, Batch Loss: 0.07958178222179413\n",
      "Epoch 10/20, Batch Loss: 0.12381164729595184\n",
      "Epoch 10/20, Batch Loss: 0.6157960295677185\n",
      "Epoch 10/20, Average Training Loss: 0.3023169793612589\n",
      "Model saved for epoch 10 at Fine_tuned_epoch10_BERT_Large.pt\n",
      "Validation Accuracy for epoch 10: 0.8915770609318997\n",
      "Epoch 11/20, Batch Loss: 0.3323219418525696\n",
      "Epoch 11/20, Batch Loss: 0.7595369219779968\n",
      "Epoch 11/20, Batch Loss: 0.0613475926220417\n",
      "Epoch 11/20, Batch Loss: 0.0891156792640686\n",
      "Epoch 11/20, Batch Loss: 0.205365851521492\n",
      "Epoch 11/20, Batch Loss: 0.1395130455493927\n",
      "Epoch 11/20, Batch Loss: 0.14958380162715912\n",
      "Epoch 11/20, Batch Loss: 0.2644613981246948\n",
      "Epoch 11/20, Batch Loss: 0.09898928552865982\n",
      "Epoch 11/20, Batch Loss: 0.2755393981933594\n",
      "Epoch 11/20, Batch Loss: 0.07976632565259933\n",
      "Epoch 11/20, Batch Loss: 0.5146522521972656\n",
      "Epoch 11/20, Batch Loss: 0.40886014699935913\n",
      "Epoch 11/20, Batch Loss: 0.07328052818775177\n",
      "Epoch 11/20, Batch Loss: 0.2511906027793884\n",
      "Epoch 11/20, Batch Loss: 0.401597797870636\n",
      "Epoch 11/20, Batch Loss: 0.3114543557167053\n",
      "Epoch 11/20, Batch Loss: 0.32573410868644714\n",
      "Epoch 11/20, Batch Loss: 0.22324715554714203\n",
      "Epoch 11/20, Batch Loss: 0.2840113639831543\n",
      "Epoch 11/20, Batch Loss: 0.22547736763954163\n",
      "Epoch 11/20, Batch Loss: 0.3009442090988159\n",
      "Epoch 11/20, Batch Loss: 0.3308420181274414\n",
      "Epoch 11/20, Batch Loss: 0.3535609543323517\n",
      "Epoch 11/20, Batch Loss: 0.31037095189094543\n",
      "Epoch 11/20, Batch Loss: 0.25171536207199097\n",
      "Epoch 11/20, Batch Loss: 0.3918519914150238\n",
      "Epoch 11/20, Batch Loss: 0.32577094435691833\n",
      "Epoch 11/20, Batch Loss: 0.633384108543396\n",
      "Epoch 11/20, Batch Loss: 0.25386911630630493\n",
      "Epoch 11/20, Batch Loss: 0.271922767162323\n",
      "Epoch 11/20, Batch Loss: 0.27432572841644287\n",
      "Epoch 11/20, Batch Loss: 0.2039543241262436\n",
      "Epoch 11/20, Batch Loss: 0.1986789107322693\n",
      "Epoch 11/20, Batch Loss: 0.26502758264541626\n",
      "Epoch 11/20, Batch Loss: 0.31673985719680786\n",
      "Epoch 11/20, Batch Loss: 0.19478772580623627\n",
      "Epoch 11/20, Batch Loss: 0.2684836685657501\n",
      "Epoch 11/20, Batch Loss: 0.3797163963317871\n",
      "Epoch 11/20, Batch Loss: 0.1125052347779274\n",
      "Epoch 11/20, Batch Loss: 0.11013486236333847\n",
      "Epoch 11/20, Batch Loss: 0.3303147554397583\n",
      "Epoch 11/20, Batch Loss: 0.21559111773967743\n",
      "Epoch 11/20, Batch Loss: 0.15487578511238098\n",
      "Epoch 11/20, Batch Loss: 0.20010459423065186\n",
      "Epoch 11/20, Batch Loss: 0.2744767367839813\n",
      "Epoch 11/20, Batch Loss: 0.077577143907547\n",
      "Epoch 11/20, Batch Loss: 0.1676238775253296\n",
      "Epoch 11/20, Batch Loss: 0.31958356499671936\n",
      "Epoch 11/20, Batch Loss: 0.06091620400547981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Batch Loss: 0.5336858630180359\n",
      "Epoch 11/20, Batch Loss: 0.052354030311107635\n",
      "Epoch 11/20, Batch Loss: 0.16520437598228455\n",
      "Epoch 11/20, Batch Loss: 0.30349934101104736\n",
      "Epoch 11/20, Batch Loss: 0.3084453344345093\n",
      "Epoch 11/20, Batch Loss: 0.5167108774185181\n",
      "Epoch 11/20, Batch Loss: 0.4835541248321533\n",
      "Epoch 11/20, Batch Loss: 0.05393841117620468\n",
      "Epoch 11/20, Batch Loss: 0.11627361923456192\n",
      "Epoch 11/20, Batch Loss: 0.3222385346889496\n",
      "Epoch 11/20, Batch Loss: 0.44505128264427185\n",
      "Epoch 11/20, Batch Loss: 0.31007638573646545\n",
      "Epoch 11/20, Batch Loss: 0.17127801477909088\n",
      "Epoch 11/20, Batch Loss: 0.0697362869977951\n",
      "Epoch 11/20, Batch Loss: 0.43099698424339294\n",
      "Epoch 11/20, Batch Loss: 0.5408755540847778\n",
      "Epoch 11/20, Batch Loss: 0.3268386125564575\n",
      "Epoch 11/20, Batch Loss: 0.37497594952583313\n",
      "Epoch 11/20, Batch Loss: 0.17370693385601044\n",
      "Epoch 11/20, Batch Loss: 0.3635988235473633\n",
      "Epoch 11/20, Batch Loss: 0.21087589859962463\n",
      "Epoch 11/20, Batch Loss: 0.3973011076450348\n",
      "Epoch 11/20, Batch Loss: 0.12738759815692902\n",
      "Epoch 11/20, Batch Loss: 0.37635448575019836\n",
      "Epoch 11/20, Batch Loss: 0.3098754584789276\n",
      "Epoch 11/20, Batch Loss: 0.40141233801841736\n",
      "Epoch 11/20, Batch Loss: 0.19789445400238037\n",
      "Epoch 11/20, Batch Loss: 0.17803405225276947\n",
      "Epoch 11/20, Batch Loss: 0.2390005737543106\n",
      "Epoch 11/20, Batch Loss: 0.4538780450820923\n",
      "Epoch 11/20, Batch Loss: 0.37047475576400757\n",
      "Epoch 11/20, Batch Loss: 0.3043433725833893\n",
      "Epoch 11/20, Batch Loss: 0.38981685042381287\n",
      "Epoch 11/20, Batch Loss: 0.24039843678474426\n",
      "Epoch 11/20, Batch Loss: 0.1367523968219757\n",
      "Epoch 11/20, Batch Loss: 0.4423971474170685\n",
      "Epoch 11/20, Batch Loss: 0.21289736032485962\n",
      "Epoch 11/20, Batch Loss: 0.21407881379127502\n",
      "Epoch 11/20, Batch Loss: 0.6025651693344116\n",
      "Epoch 11/20, Batch Loss: 0.16509976983070374\n",
      "Epoch 11/20, Batch Loss: 0.3922259509563446\n",
      "Epoch 11/20, Batch Loss: 0.26535505056381226\n",
      "Epoch 11/20, Batch Loss: 0.21368326246738434\n",
      "Epoch 11/20, Batch Loss: 0.23508261144161224\n",
      "Epoch 11/20, Batch Loss: 0.17928093671798706\n",
      "Epoch 11/20, Batch Loss: 0.45429444313049316\n",
      "Epoch 11/20, Batch Loss: 0.18891865015029907\n",
      "Epoch 11/20, Batch Loss: 0.18830569088459015\n",
      "Epoch 11/20, Batch Loss: 0.3028222322463989\n",
      "Epoch 11/20, Batch Loss: 0.13580580055713654\n",
      "Epoch 11/20, Batch Loss: 0.7224969267845154\n",
      "Epoch 11/20, Batch Loss: 0.6146401762962341\n",
      "Epoch 11/20, Batch Loss: 0.3561525344848633\n",
      "Epoch 11/20, Batch Loss: 0.08038868755102158\n",
      "Epoch 11/20, Batch Loss: 0.4325161874294281\n",
      "Epoch 11/20, Batch Loss: 0.3655751347541809\n",
      "Epoch 11/20, Batch Loss: 0.21012485027313232\n",
      "Epoch 11/20, Batch Loss: 0.36420127749443054\n",
      "Epoch 11/20, Batch Loss: 0.45020008087158203\n",
      "Epoch 11/20, Batch Loss: 0.1988152712583542\n",
      "Epoch 11/20, Batch Loss: 0.2502744197845459\n",
      "Epoch 11/20, Batch Loss: 0.24639193713665009\n",
      "Epoch 11/20, Batch Loss: 0.33309662342071533\n",
      "Epoch 11/20, Batch Loss: 0.3283376097679138\n",
      "Epoch 11/20, Batch Loss: 0.23034267127513885\n",
      "Epoch 11/20, Batch Loss: 0.28465571999549866\n",
      "Epoch 11/20, Batch Loss: 0.15165261924266815\n",
      "Epoch 11/20, Batch Loss: 0.07453040778636932\n",
      "Epoch 11/20, Batch Loss: 0.6272232532501221\n",
      "Epoch 11/20, Batch Loss: 0.4270577132701874\n",
      "Epoch 11/20, Batch Loss: 0.074085533618927\n",
      "Epoch 11/20, Batch Loss: 0.20564383268356323\n",
      "Epoch 11/20, Batch Loss: 0.09259847551584244\n",
      "Epoch 11/20, Average Training Loss: 0.28240123100397063\n",
      "Model saved for epoch 11 at Fine_tuned_epoch11_BERT_Large.pt\n",
      "Validation Accuracy for epoch 11: 0.8900089605734768\n",
      "Epoch 12/20, Batch Loss: 0.19147977232933044\n",
      "Epoch 12/20, Batch Loss: 0.3542417585849762\n",
      "Epoch 12/20, Batch Loss: 0.28990644216537476\n",
      "Epoch 12/20, Batch Loss: 0.055321790277957916\n",
      "Epoch 12/20, Batch Loss: 0.6682236790657043\n",
      "Epoch 12/20, Batch Loss: 0.3220667243003845\n",
      "Epoch 12/20, Batch Loss: 0.2656634449958801\n",
      "Epoch 12/20, Batch Loss: 0.07836373150348663\n",
      "Epoch 12/20, Batch Loss: 0.3319847583770752\n",
      "Epoch 12/20, Batch Loss: 0.28799793124198914\n",
      "Epoch 12/20, Batch Loss: 0.21410472691059113\n",
      "Epoch 12/20, Batch Loss: 0.11285464465618134\n",
      "Epoch 12/20, Batch Loss: 0.43510645627975464\n",
      "Epoch 12/20, Batch Loss: 0.17680101096630096\n",
      "Epoch 12/20, Batch Loss: 0.19584576785564423\n",
      "Epoch 12/20, Batch Loss: 0.2647268772125244\n",
      "Epoch 12/20, Batch Loss: 0.25473302602767944\n",
      "Epoch 12/20, Batch Loss: 0.16005422174930573\n",
      "Epoch 12/20, Batch Loss: 0.21716098487377167\n",
      "Epoch 12/20, Batch Loss: 0.8655728101730347\n",
      "Epoch 12/20, Batch Loss: 0.5962918400764465\n",
      "Epoch 12/20, Batch Loss: 0.06877939403057098\n",
      "Epoch 12/20, Batch Loss: 0.1878972351551056\n",
      "Epoch 12/20, Batch Loss: 0.04243301972746849\n",
      "Epoch 12/20, Batch Loss: 0.2660336196422577\n",
      "Epoch 12/20, Batch Loss: 0.5747485160827637\n",
      "Epoch 12/20, Batch Loss: 0.027448013424873352\n",
      "Epoch 12/20, Batch Loss: 0.33780694007873535\n",
      "Epoch 12/20, Batch Loss: 0.1426216959953308\n",
      "Epoch 12/20, Batch Loss: 0.17279751598834991\n",
      "Epoch 12/20, Batch Loss: 0.26911216974258423\n",
      "Epoch 12/20, Batch Loss: 0.10709592700004578\n",
      "Epoch 12/20, Batch Loss: 0.2552785575389862\n",
      "Epoch 12/20, Batch Loss: 0.2730797529220581\n",
      "Epoch 12/20, Batch Loss: 0.3800410330295563\n",
      "Epoch 12/20, Batch Loss: 0.34181225299835205\n",
      "Epoch 12/20, Batch Loss: 0.4006481468677521\n",
      "Epoch 12/20, Batch Loss: 0.277665376663208\n",
      "Epoch 12/20, Batch Loss: 0.28826868534088135\n",
      "Epoch 12/20, Batch Loss: 0.11150593310594559\n",
      "Epoch 12/20, Batch Loss: 0.23005658388137817\n",
      "Epoch 12/20, Batch Loss: 0.24067677557468414\n",
      "Epoch 12/20, Batch Loss: 0.1365397423505783\n",
      "Epoch 12/20, Batch Loss: 0.3448769152164459\n",
      "Epoch 12/20, Batch Loss: 0.3733256757259369\n",
      "Epoch 12/20, Batch Loss: 0.15128232538700104\n",
      "Epoch 12/20, Batch Loss: 0.17719751596450806\n",
      "Epoch 12/20, Batch Loss: 0.2846064865589142\n",
      "Epoch 12/20, Batch Loss: 0.16571372747421265\n",
      "Epoch 12/20, Batch Loss: 0.2397470474243164\n",
      "Epoch 12/20, Batch Loss: 0.16545413434505463\n",
      "Epoch 12/20, Batch Loss: 0.45446014404296875\n",
      "Epoch 12/20, Batch Loss: 0.14337807893753052\n",
      "Epoch 12/20, Batch Loss: 0.22546082735061646\n",
      "Epoch 12/20, Batch Loss: 0.18284904956817627\n",
      "Epoch 12/20, Batch Loss: 0.2062707543373108\n",
      "Epoch 12/20, Batch Loss: 0.21664875745773315\n",
      "Epoch 12/20, Batch Loss: 0.39286738634109497\n",
      "Epoch 12/20, Batch Loss: 0.09926215559244156\n",
      "Epoch 12/20, Batch Loss: 0.4574679434299469\n",
      "Epoch 12/20, Batch Loss: 0.2581852376461029\n",
      "Epoch 12/20, Batch Loss: 0.19162127375602722\n",
      "Epoch 12/20, Batch Loss: 0.3862423598766327\n",
      "Epoch 12/20, Batch Loss: 0.32925015687942505\n",
      "Epoch 12/20, Batch Loss: 0.1768786907196045\n",
      "Epoch 12/20, Batch Loss: 0.2999395728111267\n",
      "Epoch 12/20, Batch Loss: 0.2829362750053406\n",
      "Epoch 12/20, Batch Loss: 0.34557047486305237\n",
      "Epoch 12/20, Batch Loss: 0.085385262966156\n",
      "Epoch 12/20, Batch Loss: 0.14983178675174713\n",
      "Epoch 12/20, Batch Loss: 0.10890592634677887\n",
      "Epoch 12/20, Batch Loss: 0.15098315477371216\n",
      "Epoch 12/20, Batch Loss: 0.3114699125289917\n",
      "Epoch 12/20, Batch Loss: 0.23235858976840973\n",
      "Epoch 12/20, Batch Loss: 0.0647597536444664\n",
      "Epoch 12/20, Batch Loss: 0.32608702778816223\n",
      "Epoch 12/20, Batch Loss: 0.24700213968753815\n",
      "Epoch 12/20, Batch Loss: 0.4356182813644409\n",
      "Epoch 12/20, Batch Loss: 0.11982724070549011\n",
      "Epoch 12/20, Batch Loss: 0.36921632289886475\n",
      "Epoch 12/20, Batch Loss: 0.12740230560302734\n",
      "Epoch 12/20, Batch Loss: 0.28456735610961914\n",
      "Epoch 12/20, Batch Loss: 0.565416693687439\n",
      "Epoch 12/20, Batch Loss: 0.2525358200073242\n",
      "Epoch 12/20, Batch Loss: 0.21188481152057648\n",
      "Epoch 12/20, Batch Loss: 0.36697590351104736\n",
      "Epoch 12/20, Batch Loss: 0.16762670874595642\n",
      "Epoch 12/20, Batch Loss: 0.28585928678512573\n",
      "Epoch 12/20, Batch Loss: 0.12463165074586868\n",
      "Epoch 12/20, Batch Loss: 0.40435993671417236\n",
      "Epoch 12/20, Batch Loss: 0.39740297198295593\n",
      "Epoch 12/20, Batch Loss: 0.1183038130402565\n",
      "Epoch 12/20, Batch Loss: 0.28215157985687256\n",
      "Epoch 12/20, Batch Loss: 0.15561507642269135\n",
      "Epoch 12/20, Batch Loss: 0.23175352811813354\n",
      "Epoch 12/20, Batch Loss: 0.5043688416481018\n",
      "Epoch 12/20, Batch Loss: 0.21845421195030212\n",
      "Epoch 12/20, Batch Loss: 0.38854295015335083\n",
      "Epoch 12/20, Batch Loss: 0.30199193954467773\n",
      "Epoch 12/20, Batch Loss: 0.31302547454833984\n",
      "Epoch 12/20, Batch Loss: 0.14433303475379944\n",
      "Epoch 12/20, Batch Loss: 0.2578285336494446\n",
      "Epoch 12/20, Batch Loss: 0.32147452235221863\n",
      "Epoch 12/20, Batch Loss: 0.07221759110689163\n",
      "Epoch 12/20, Batch Loss: 0.14889228343963623\n",
      "Epoch 12/20, Batch Loss: 0.5679087042808533\n",
      "Epoch 12/20, Batch Loss: 0.3207332193851471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Batch Loss: 0.19912753999233246\n",
      "Epoch 12/20, Batch Loss: 0.332487016916275\n",
      "Epoch 12/20, Batch Loss: 0.20607773959636688\n",
      "Epoch 12/20, Batch Loss: 0.1585891991853714\n",
      "Epoch 12/20, Batch Loss: 0.18150559067726135\n",
      "Epoch 12/20, Batch Loss: 0.12206932157278061\n",
      "Epoch 12/20, Batch Loss: 0.2810411751270294\n",
      "Epoch 12/20, Batch Loss: 0.16532403230667114\n",
      "Epoch 12/20, Batch Loss: 0.1894705593585968\n",
      "Epoch 12/20, Batch Loss: 0.15184880793094635\n",
      "Epoch 12/20, Batch Loss: 0.2027232050895691\n",
      "Epoch 12/20, Batch Loss: 0.2280677706003189\n",
      "Epoch 12/20, Batch Loss: 0.4563782513141632\n",
      "Epoch 12/20, Batch Loss: 0.27855384349823\n",
      "Epoch 12/20, Batch Loss: 0.6962082982063293\n",
      "Epoch 12/20, Batch Loss: 0.05181952193379402\n",
      "Epoch 12/20, Average Training Loss: 0.2606612867940732\n",
      "Model saved for epoch 12 at Fine_tuned_epoch12_BERT_Large.pt\n",
      "Validation Accuracy for epoch 12: 0.8911290322580645\n",
      "Epoch 13/20, Batch Loss: 0.06108178570866585\n",
      "Epoch 13/20, Batch Loss: 0.20412175357341766\n",
      "Epoch 13/20, Batch Loss: 0.120021753013134\n",
      "Epoch 13/20, Batch Loss: 0.25937771797180176\n",
      "Epoch 13/20, Batch Loss: 0.23849882185459137\n",
      "Epoch 13/20, Batch Loss: 0.15986819565296173\n",
      "Epoch 13/20, Batch Loss: 0.03775738924741745\n",
      "Epoch 13/20, Batch Loss: 0.32163384556770325\n",
      "Epoch 13/20, Batch Loss: 0.24016380310058594\n",
      "Epoch 13/20, Batch Loss: 0.20644238591194153\n",
      "Epoch 13/20, Batch Loss: 0.12553374469280243\n",
      "Epoch 13/20, Batch Loss: 0.2405584454536438\n",
      "Epoch 13/20, Batch Loss: 0.2101457267999649\n",
      "Epoch 13/20, Batch Loss: 0.0698762759566307\n",
      "Epoch 13/20, Batch Loss: 0.08895883709192276\n",
      "Epoch 13/20, Batch Loss: 0.04034660384058952\n",
      "Epoch 13/20, Batch Loss: 0.12524199485778809\n",
      "Epoch 13/20, Batch Loss: 0.3145456314086914\n",
      "Epoch 13/20, Batch Loss: 0.19497668743133545\n",
      "Epoch 13/20, Batch Loss: 0.27691715955734253\n",
      "Epoch 13/20, Batch Loss: 0.10277853161096573\n",
      "Epoch 13/20, Batch Loss: 0.22170335054397583\n",
      "Epoch 13/20, Batch Loss: 0.1684453785419464\n",
      "Epoch 13/20, Batch Loss: 0.08891686052083969\n",
      "Epoch 13/20, Batch Loss: 0.22140400111675262\n",
      "Epoch 13/20, Batch Loss: 0.2300892025232315\n",
      "Epoch 13/20, Batch Loss: 0.13265907764434814\n",
      "Epoch 13/20, Batch Loss: 0.16967590153217316\n",
      "Epoch 13/20, Batch Loss: 0.03533490002155304\n",
      "Epoch 13/20, Batch Loss: 0.17838038504123688\n",
      "Epoch 13/20, Batch Loss: 0.09177769720554352\n",
      "Epoch 13/20, Batch Loss: 0.44032055139541626\n",
      "Epoch 13/20, Batch Loss: 0.17365756630897522\n",
      "Epoch 13/20, Batch Loss: 0.04908226802945137\n",
      "Epoch 13/20, Batch Loss: 0.27688315510749817\n",
      "Epoch 13/20, Batch Loss: 0.4224162995815277\n",
      "Epoch 13/20, Batch Loss: 0.1546361893415451\n",
      "Epoch 13/20, Batch Loss: 0.5687354207038879\n",
      "Epoch 13/20, Batch Loss: 0.23008078336715698\n",
      "Epoch 13/20, Batch Loss: 0.4826391935348511\n",
      "Epoch 13/20, Batch Loss: 0.21813882887363434\n",
      "Epoch 13/20, Batch Loss: 0.33692288398742676\n",
      "Epoch 13/20, Batch Loss: 0.2503221035003662\n",
      "Epoch 13/20, Batch Loss: 0.5136599540710449\n",
      "Epoch 13/20, Batch Loss: 0.26306241750717163\n",
      "Epoch 13/20, Batch Loss: 0.1211763471364975\n",
      "Epoch 13/20, Batch Loss: 0.40898188948631287\n",
      "Epoch 13/20, Batch Loss: 0.2573021352291107\n",
      "Epoch 13/20, Batch Loss: 0.3612663149833679\n",
      "Epoch 13/20, Batch Loss: 0.1051405593752861\n",
      "Epoch 13/20, Batch Loss: 0.19998401403427124\n",
      "Epoch 13/20, Batch Loss: 0.25186142325401306\n",
      "Epoch 13/20, Batch Loss: 0.4091581702232361\n",
      "Epoch 13/20, Batch Loss: 0.10404940694570541\n",
      "Epoch 13/20, Batch Loss: 0.21583673357963562\n",
      "Epoch 13/20, Batch Loss: 0.1045953705906868\n",
      "Epoch 13/20, Batch Loss: 0.45978695154190063\n",
      "Epoch 13/20, Batch Loss: 0.2282109409570694\n",
      "Epoch 13/20, Batch Loss: 0.23688331246376038\n",
      "Epoch 13/20, Batch Loss: 0.07578858733177185\n",
      "Epoch 13/20, Batch Loss: 0.15503595769405365\n",
      "Epoch 13/20, Batch Loss: 0.22211399674415588\n",
      "Epoch 13/20, Batch Loss: 0.20072898268699646\n",
      "Epoch 13/20, Batch Loss: 0.09598354995250702\n",
      "Epoch 13/20, Batch Loss: 0.17893829941749573\n",
      "Epoch 13/20, Batch Loss: 0.13699674606323242\n",
      "Epoch 13/20, Batch Loss: 0.32650187611579895\n",
      "Epoch 13/20, Batch Loss: 0.25968101620674133\n",
      "Epoch 13/20, Batch Loss: 0.11379644274711609\n",
      "Epoch 13/20, Batch Loss: 0.26471659541130066\n",
      "Epoch 13/20, Batch Loss: 0.209954172372818\n",
      "Epoch 13/20, Batch Loss: 0.4236190915107727\n",
      "Epoch 13/20, Batch Loss: 0.052357450127601624\n",
      "Epoch 13/20, Batch Loss: 0.5715059041976929\n",
      "Epoch 13/20, Batch Loss: 0.28723379969596863\n",
      "Epoch 13/20, Batch Loss: 0.03767256811261177\n",
      "Epoch 13/20, Batch Loss: 0.33470115065574646\n",
      "Epoch 13/20, Batch Loss: 0.18611788749694824\n",
      "Epoch 13/20, Batch Loss: 0.14908404648303986\n",
      "Epoch 13/20, Batch Loss: 0.27360743284225464\n",
      "Epoch 13/20, Batch Loss: 0.25153854489326477\n",
      "Epoch 13/20, Batch Loss: 0.14790160953998566\n",
      "Epoch 13/20, Batch Loss: 0.3822941184043884\n",
      "Epoch 13/20, Batch Loss: 0.2767227292060852\n",
      "Epoch 13/20, Batch Loss: 0.17198514938354492\n",
      "Epoch 13/20, Batch Loss: 0.24115124344825745\n",
      "Epoch 13/20, Batch Loss: 0.44214436411857605\n",
      "Epoch 13/20, Batch Loss: 0.1855263113975525\n",
      "Epoch 13/20, Batch Loss: 0.3601408004760742\n",
      "Epoch 13/20, Batch Loss: 0.13854895532131195\n",
      "Epoch 13/20, Batch Loss: 0.30613699555397034\n",
      "Epoch 13/20, Batch Loss: 0.24760106205940247\n",
      "Epoch 13/20, Batch Loss: 0.11392297595739365\n",
      "Epoch 13/20, Batch Loss: 0.292035311460495\n",
      "Epoch 13/20, Batch Loss: 0.10273556411266327\n",
      "Epoch 13/20, Batch Loss: 0.13693788647651672\n",
      "Epoch 13/20, Batch Loss: 0.36501544713974\n",
      "Epoch 13/20, Batch Loss: 0.3761235475540161\n",
      "Epoch 13/20, Batch Loss: 0.12691326439380646\n",
      "Epoch 13/20, Batch Loss: 0.28120967745780945\n",
      "Epoch 13/20, Batch Loss: 0.7001854181289673\n",
      "Epoch 13/20, Batch Loss: 0.1314203143119812\n",
      "Epoch 13/20, Batch Loss: 0.11530327796936035\n",
      "Epoch 13/20, Batch Loss: 0.23107710480690002\n",
      "Epoch 13/20, Batch Loss: 0.21228472888469696\n",
      "Epoch 13/20, Batch Loss: 0.543812096118927\n",
      "Epoch 13/20, Batch Loss: 0.2639990746974945\n",
      "Epoch 13/20, Batch Loss: 0.1459362506866455\n",
      "Epoch 13/20, Batch Loss: 0.157235786318779\n",
      "Epoch 13/20, Batch Loss: 0.16376249492168427\n",
      "Epoch 13/20, Batch Loss: 0.10286378860473633\n",
      "Epoch 13/20, Batch Loss: 0.25776511430740356\n",
      "Epoch 13/20, Batch Loss: 0.05609675124287605\n",
      "Epoch 13/20, Batch Loss: 0.0543157123029232\n",
      "Epoch 13/20, Batch Loss: 0.5553534030914307\n",
      "Epoch 13/20, Batch Loss: 0.062130868434906006\n",
      "Epoch 13/20, Batch Loss: 0.05092401057481766\n",
      "Epoch 13/20, Batch Loss: 0.07268897444009781\n",
      "Epoch 13/20, Batch Loss: 0.42420703172683716\n",
      "Epoch 13/20, Batch Loss: 0.1708827167749405\n",
      "Epoch 13/20, Batch Loss: 0.26506662368774414\n",
      "Epoch 13/20, Batch Loss: 0.22995051741600037\n",
      "Epoch 13/20, Batch Loss: 0.019379133358597755\n",
      "Epoch 13/20, Average Training Loss: 0.22366981573281733\n",
      "Model saved for epoch 13 at Fine_tuned_epoch13_BERT_Large.pt\n",
      "Validation Accuracy for epoch 13: 0.8823924731182796\n",
      "Epoch 14/20, Batch Loss: 0.10297717154026031\n",
      "Epoch 14/20, Batch Loss: 0.07788906991481781\n",
      "Epoch 14/20, Batch Loss: 0.16717429459095\n",
      "Epoch 14/20, Batch Loss: 0.19878746569156647\n",
      "Epoch 14/20, Batch Loss: 0.15011243522167206\n",
      "Epoch 14/20, Batch Loss: 0.3264082968235016\n",
      "Epoch 14/20, Batch Loss: 0.18149882555007935\n",
      "Epoch 14/20, Batch Loss: 0.18527379631996155\n",
      "Epoch 14/20, Batch Loss: 0.19660907983779907\n",
      "Epoch 14/20, Batch Loss: 0.06958692520856857\n",
      "Epoch 14/20, Batch Loss: 0.2553616166114807\n",
      "Epoch 14/20, Batch Loss: 0.045400068163871765\n",
      "Epoch 14/20, Batch Loss: 0.11288361251354218\n",
      "Epoch 14/20, Batch Loss: 0.20775268971920013\n",
      "Epoch 14/20, Batch Loss: 0.32540297508239746\n",
      "Epoch 14/20, Batch Loss: 0.2744310796260834\n",
      "Epoch 14/20, Batch Loss: 0.18192709982395172\n",
      "Epoch 14/20, Batch Loss: 0.49329620599746704\n",
      "Epoch 14/20, Batch Loss: 0.17037305235862732\n",
      "Epoch 14/20, Batch Loss: 0.13715974986553192\n",
      "Epoch 14/20, Batch Loss: 0.19074027240276337\n",
      "Epoch 14/20, Batch Loss: 0.10273631662130356\n",
      "Epoch 14/20, Batch Loss: 0.14608179032802582\n",
      "Epoch 14/20, Batch Loss: 0.30822867155075073\n",
      "Epoch 14/20, Batch Loss: 0.24184852838516235\n",
      "Epoch 14/20, Batch Loss: 0.19392739236354828\n",
      "Epoch 14/20, Batch Loss: 0.21208809316158295\n",
      "Epoch 14/20, Batch Loss: 0.1884038895368576\n",
      "Epoch 14/20, Batch Loss: 0.2021172046661377\n",
      "Epoch 14/20, Batch Loss: 0.22792783379554749\n",
      "Epoch 14/20, Batch Loss: 0.02620089054107666\n",
      "Epoch 14/20, Batch Loss: 0.3575766980648041\n",
      "Epoch 14/20, Batch Loss: 0.06916265189647675\n",
      "Epoch 14/20, Batch Loss: 0.21252024173736572\n",
      "Epoch 14/20, Batch Loss: 0.0762818455696106\n",
      "Epoch 14/20, Batch Loss: 0.12309937924146652\n",
      "Epoch 14/20, Batch Loss: 0.12900689244270325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Batch Loss: 0.26276713609695435\n",
      "Epoch 14/20, Batch Loss: 0.46352627873420715\n",
      "Epoch 14/20, Batch Loss: 0.14483094215393066\n",
      "Epoch 14/20, Batch Loss: 0.2605418860912323\n",
      "Epoch 14/20, Batch Loss: 0.21006068587303162\n",
      "Epoch 14/20, Batch Loss: 0.1844845414161682\n",
      "Epoch 14/20, Batch Loss: 0.18021754920482635\n",
      "Epoch 14/20, Batch Loss: 0.14604908227920532\n",
      "Epoch 14/20, Batch Loss: 0.32514405250549316\n",
      "Epoch 14/20, Batch Loss: 0.18880704045295715\n",
      "Epoch 14/20, Batch Loss: 0.06427573412656784\n",
      "Epoch 14/20, Batch Loss: 0.28998202085494995\n",
      "Epoch 14/20, Batch Loss: 0.17924854159355164\n",
      "Epoch 14/20, Batch Loss: 0.11033443361520767\n",
      "Epoch 14/20, Batch Loss: 0.20067590475082397\n",
      "Epoch 14/20, Batch Loss: 0.03883372247219086\n",
      "Epoch 14/20, Batch Loss: 0.24681007862091064\n",
      "Epoch 14/20, Batch Loss: 0.10617101192474365\n",
      "Epoch 14/20, Batch Loss: 0.1415925770998001\n",
      "Epoch 14/20, Batch Loss: 0.10548442602157593\n",
      "Epoch 14/20, Batch Loss: 0.2510630190372467\n",
      "Epoch 14/20, Batch Loss: 0.6142996549606323\n",
      "Epoch 14/20, Batch Loss: 0.15503284335136414\n",
      "Epoch 14/20, Batch Loss: 0.142557755112648\n",
      "Epoch 14/20, Batch Loss: 0.30139440298080444\n",
      "Epoch 14/20, Batch Loss: 0.08650650084018707\n",
      "Epoch 14/20, Batch Loss: 0.25925660133361816\n",
      "Epoch 14/20, Batch Loss: 0.2569893002510071\n",
      "Epoch 14/20, Batch Loss: 0.09545086324214935\n",
      "Epoch 14/20, Batch Loss: 0.08842465281486511\n",
      "Epoch 14/20, Batch Loss: 0.28686657547950745\n",
      "Epoch 14/20, Batch Loss: 0.0936480388045311\n",
      "Epoch 14/20, Batch Loss: 0.12236231565475464\n",
      "Epoch 14/20, Batch Loss: 0.052494242787361145\n",
      "Epoch 14/20, Batch Loss: 0.09398505836725235\n",
      "Epoch 14/20, Batch Loss: 0.29718688130378723\n",
      "Epoch 14/20, Batch Loss: 0.3085530400276184\n",
      "Epoch 14/20, Batch Loss: 0.251302570104599\n",
      "Epoch 14/20, Batch Loss: 0.17108064889907837\n",
      "Epoch 14/20, Batch Loss: 0.18733343482017517\n",
      "Epoch 14/20, Batch Loss: 0.31522804498672485\n",
      "Epoch 14/20, Batch Loss: 0.14503632485866547\n",
      "Epoch 14/20, Batch Loss: 0.21347995102405548\n",
      "Epoch 14/20, Batch Loss: 0.1752418875694275\n",
      "Epoch 14/20, Batch Loss: 0.18095555901527405\n",
      "Epoch 14/20, Batch Loss: 0.2316436916589737\n",
      "Epoch 14/20, Batch Loss: 0.1505231112241745\n",
      "Epoch 14/20, Batch Loss: 0.42169255018234253\n",
      "Epoch 14/20, Batch Loss: 0.06584912538528442\n",
      "Epoch 14/20, Batch Loss: 0.2676742672920227\n",
      "Epoch 14/20, Batch Loss: 0.14294862747192383\n",
      "Epoch 14/20, Batch Loss: 0.12448418140411377\n",
      "Epoch 14/20, Batch Loss: 0.1675940901041031\n",
      "Epoch 14/20, Batch Loss: 0.08021891862154007\n",
      "Epoch 14/20, Batch Loss: 0.3071700930595398\n",
      "Epoch 14/20, Batch Loss: 0.01640496961772442\n",
      "Epoch 14/20, Batch Loss: 0.13080108165740967\n",
      "Epoch 14/20, Batch Loss: 0.29361483454704285\n",
      "Epoch 14/20, Batch Loss: 0.26003676652908325\n",
      "Epoch 14/20, Batch Loss: 0.2369871586561203\n",
      "Epoch 14/20, Batch Loss: 0.4638938009738922\n",
      "Epoch 14/20, Batch Loss: 0.1845093071460724\n",
      "Epoch 14/20, Batch Loss: 0.16472429037094116\n",
      "Epoch 14/20, Batch Loss: 0.0971246212720871\n",
      "Epoch 14/20, Batch Loss: 0.1498836725950241\n",
      "Epoch 14/20, Batch Loss: 0.07638578861951828\n",
      "Epoch 14/20, Batch Loss: 0.16193275153636932\n",
      "Epoch 14/20, Batch Loss: 0.2607608735561371\n",
      "Epoch 14/20, Batch Loss: 0.14842388033866882\n",
      "Epoch 14/20, Batch Loss: 0.30629023909568787\n",
      "Epoch 14/20, Batch Loss: 0.037045255303382874\n",
      "Epoch 14/20, Batch Loss: 0.17352837324142456\n",
      "Epoch 14/20, Batch Loss: 0.10346505045890808\n",
      "Epoch 14/20, Batch Loss: 0.40128228068351746\n",
      "Epoch 14/20, Batch Loss: 0.18820330500602722\n",
      "Epoch 14/20, Batch Loss: 0.17384764552116394\n",
      "Epoch 14/20, Batch Loss: 0.1528092473745346\n",
      "Epoch 14/20, Batch Loss: 0.3614952564239502\n",
      "Epoch 14/20, Batch Loss: 0.3734551966190338\n",
      "Epoch 14/20, Batch Loss: 0.10923975706100464\n",
      "Epoch 14/20, Batch Loss: 0.14559058845043182\n",
      "Epoch 14/20, Batch Loss: 0.12818993628025055\n",
      "Epoch 14/20, Batch Loss: 0.3095654547214508\n",
      "Epoch 14/20, Batch Loss: 0.08427251130342484\n",
      "Epoch 14/20, Batch Loss: 0.33847931027412415\n",
      "Epoch 14/20, Batch Loss: 1.1398223638534546\n",
      "Epoch 14/20, Average Training Loss: 0.20321696050222812\n",
      "Model saved for epoch 14 at Fine_tuned_epoch14_BERT_Large.pt\n",
      "Validation Accuracy for epoch 14: 0.8774641577060932\n",
      "Epoch 15/20, Batch Loss: 0.18923446536064148\n",
      "Epoch 15/20, Batch Loss: 0.1146896481513977\n",
      "Epoch 15/20, Batch Loss: 0.05203068256378174\n",
      "Epoch 15/20, Batch Loss: 0.06543156504631042\n",
      "Epoch 15/20, Batch Loss: 0.12760841846466064\n",
      "Epoch 15/20, Batch Loss: 0.11947223544120789\n",
      "Epoch 15/20, Batch Loss: 0.36247557401657104\n",
      "Epoch 15/20, Batch Loss: 0.13160386681556702\n",
      "Epoch 15/20, Batch Loss: 0.08095172047615051\n",
      "Epoch 15/20, Batch Loss: 0.0516531839966774\n",
      "Epoch 15/20, Batch Loss: 0.19710518419742584\n",
      "Epoch 15/20, Batch Loss: 0.22245949506759644\n",
      "Epoch 15/20, Batch Loss: 0.16803313791751862\n",
      "Epoch 15/20, Batch Loss: 0.3418506383895874\n",
      "Epoch 15/20, Batch Loss: 0.47363314032554626\n",
      "Epoch 15/20, Batch Loss: 0.04813525080680847\n",
      "Epoch 15/20, Batch Loss: 0.0811256542801857\n",
      "Epoch 15/20, Batch Loss: 0.11363376677036285\n",
      "Epoch 15/20, Batch Loss: 0.05100420489907265\n",
      "Epoch 15/20, Batch Loss: 0.24968138337135315\n",
      "Epoch 15/20, Batch Loss: 0.2171340435743332\n",
      "Epoch 15/20, Batch Loss: 0.2511696219444275\n",
      "Epoch 15/20, Batch Loss: 0.19517098367214203\n",
      "Epoch 15/20, Batch Loss: 0.11290391534566879\n",
      "Epoch 15/20, Batch Loss: 0.1176338866353035\n",
      "Epoch 15/20, Batch Loss: 0.09043103456497192\n",
      "Epoch 15/20, Batch Loss: 0.2644193172454834\n",
      "Epoch 15/20, Batch Loss: 0.27559539675712585\n",
      "Epoch 15/20, Batch Loss: 0.019944550469517708\n",
      "Epoch 15/20, Batch Loss: 0.011268305592238903\n",
      "Epoch 15/20, Batch Loss: 0.08547558635473251\n",
      "Epoch 15/20, Batch Loss: 0.1440528780221939\n",
      "Epoch 15/20, Batch Loss: 0.06428191810846329\n",
      "Epoch 15/20, Batch Loss: 0.43492987751960754\n",
      "Epoch 15/20, Batch Loss: 0.056950561702251434\n",
      "Epoch 15/20, Batch Loss: 0.14064063131809235\n",
      "Epoch 15/20, Batch Loss: 0.19269713759422302\n",
      "Epoch 15/20, Batch Loss: 0.24093759059906006\n",
      "Epoch 15/20, Batch Loss: 0.11475683003664017\n",
      "Epoch 15/20, Batch Loss: 0.09699743241071701\n",
      "Epoch 15/20, Batch Loss: 0.0955977588891983\n",
      "Epoch 15/20, Batch Loss: 0.14738759398460388\n",
      "Epoch 15/20, Batch Loss: 0.17113101482391357\n",
      "Epoch 15/20, Batch Loss: 0.09713636338710785\n",
      "Epoch 15/20, Batch Loss: 0.2713698744773865\n",
      "Epoch 15/20, Batch Loss: 0.21387439966201782\n",
      "Epoch 15/20, Batch Loss: 0.08796203136444092\n",
      "Epoch 15/20, Batch Loss: 0.25409185886383057\n",
      "Epoch 15/20, Batch Loss: 0.1901853084564209\n",
      "Epoch 15/20, Batch Loss: 0.1983753889799118\n",
      "Epoch 15/20, Batch Loss: 0.11144085228443146\n",
      "Epoch 15/20, Batch Loss: 0.020273933187127113\n",
      "Epoch 15/20, Batch Loss: 0.029681477695703506\n",
      "Epoch 15/20, Batch Loss: 0.13134320080280304\n",
      "Epoch 15/20, Batch Loss: 0.500860333442688\n",
      "Epoch 15/20, Batch Loss: 0.1439608484506607\n",
      "Epoch 15/20, Batch Loss: 0.5228348970413208\n",
      "Epoch 15/20, Batch Loss: 0.21707923710346222\n",
      "Epoch 15/20, Batch Loss: 0.0667688399553299\n",
      "Epoch 15/20, Batch Loss: 0.2312684804201126\n",
      "Epoch 15/20, Batch Loss: 0.15773801505565643\n",
      "Epoch 15/20, Batch Loss: 0.08035842329263687\n",
      "Epoch 15/20, Batch Loss: 0.26026496291160583\n",
      "Epoch 15/20, Batch Loss: 0.1852402538061142\n",
      "Epoch 15/20, Batch Loss: 0.11526156216859818\n",
      "Epoch 15/20, Batch Loss: 0.10006794333457947\n",
      "Epoch 15/20, Batch Loss: 0.16239188611507416\n",
      "Epoch 15/20, Batch Loss: 0.0428803451359272\n",
      "Epoch 15/20, Batch Loss: 0.23247940838336945\n",
      "Epoch 15/20, Batch Loss: 0.15799769759178162\n",
      "Epoch 15/20, Batch Loss: 0.1969255805015564\n",
      "Epoch 15/20, Batch Loss: 0.013508164323866367\n",
      "Epoch 15/20, Batch Loss: 0.04803023859858513\n",
      "Epoch 15/20, Batch Loss: 0.0530657060444355\n",
      "Epoch 15/20, Batch Loss: 0.5166288018226624\n",
      "Epoch 15/20, Batch Loss: 0.14992544054985046\n",
      "Epoch 15/20, Batch Loss: 0.3389257490634918\n",
      "Epoch 15/20, Batch Loss: 0.0713082104921341\n",
      "Epoch 15/20, Batch Loss: 0.08691783249378204\n",
      "Epoch 15/20, Batch Loss: 0.3621731698513031\n",
      "Epoch 15/20, Batch Loss: 0.2907860577106476\n",
      "Epoch 15/20, Batch Loss: 0.02047796919941902\n",
      "Epoch 15/20, Batch Loss: 0.23854246735572815\n",
      "Epoch 15/20, Batch Loss: 0.2262793332338333\n",
      "Epoch 15/20, Batch Loss: 0.46590015292167664\n",
      "Epoch 15/20, Batch Loss: 0.01570441760122776\n",
      "Epoch 15/20, Batch Loss: 0.19336971640586853\n",
      "Epoch 15/20, Batch Loss: 0.10569105297327042\n",
      "Epoch 15/20, Batch Loss: 0.15354502201080322\n",
      "Epoch 15/20, Batch Loss: 0.2487025409936905\n",
      "Epoch 15/20, Batch Loss: 0.16626866161823273\n",
      "Epoch 15/20, Batch Loss: 0.423617422580719\n",
      "Epoch 15/20, Batch Loss: 0.03861753270030022\n",
      "Epoch 15/20, Batch Loss: 0.17603282630443573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Batch Loss: 0.19013717770576477\n",
      "Epoch 15/20, Batch Loss: 0.3148745894432068\n",
      "Epoch 15/20, Batch Loss: 0.09901607781648636\n",
      "Epoch 15/20, Batch Loss: 0.24039316177368164\n",
      "Epoch 15/20, Batch Loss: 0.07671504467725754\n",
      "Epoch 15/20, Batch Loss: 0.1397416740655899\n",
      "Epoch 15/20, Batch Loss: 0.1015990749001503\n",
      "Epoch 15/20, Batch Loss: 0.2733100354671478\n",
      "Epoch 15/20, Batch Loss: 0.2375720739364624\n",
      "Epoch 15/20, Batch Loss: 0.13918901979923248\n",
      "Epoch 15/20, Batch Loss: 0.09152858704328537\n",
      "Epoch 15/20, Batch Loss: 0.2754032015800476\n",
      "Epoch 15/20, Batch Loss: 0.07145310938358307\n",
      "Epoch 15/20, Batch Loss: 0.1868813931941986\n",
      "Epoch 15/20, Batch Loss: 0.19092418253421783\n",
      "Epoch 15/20, Batch Loss: 0.12204782664775848\n",
      "Epoch 15/20, Batch Loss: 0.14708314836025238\n",
      "Epoch 15/20, Batch Loss: 0.2007797360420227\n",
      "Epoch 15/20, Batch Loss: 0.21827970445156097\n",
      "Epoch 15/20, Batch Loss: 0.02983647771179676\n",
      "Epoch 15/20, Batch Loss: 0.23305273056030273\n",
      "Epoch 15/20, Batch Loss: 0.1931701898574829\n",
      "Epoch 15/20, Batch Loss: 0.21577188372612\n",
      "Epoch 15/20, Batch Loss: 0.05938735231757164\n",
      "Epoch 15/20, Batch Loss: 0.08402958512306213\n",
      "Epoch 15/20, Batch Loss: 0.03354236111044884\n",
      "Epoch 15/20, Batch Loss: 0.2386694997549057\n",
      "Epoch 15/20, Batch Loss: 0.6244981288909912\n",
      "Epoch 15/20, Batch Loss: 0.005628250539302826\n",
      "Epoch 15/20, Average Training Loss: 0.1723737256313727\n",
      "Model saved for epoch 15 at Fine_tuned_epoch15_BERT_Large.pt\n",
      "Validation Accuracy for epoch 15: 0.8669354838709677\n",
      "Epoch 16/20, Batch Loss: 0.041225820779800415\n",
      "Epoch 16/20, Batch Loss: 0.0059810299426317215\n",
      "Epoch 16/20, Batch Loss: 0.12715110182762146\n",
      "Epoch 16/20, Batch Loss: 0.06483962386846542\n",
      "Epoch 16/20, Batch Loss: 0.11266303807497025\n",
      "Epoch 16/20, Batch Loss: 0.020605262368917465\n",
      "Epoch 16/20, Batch Loss: 0.07281731069087982\n",
      "Epoch 16/20, Batch Loss: 0.10214775800704956\n",
      "Epoch 16/20, Batch Loss: 0.07005029916763306\n",
      "Epoch 16/20, Batch Loss: 0.2809661030769348\n",
      "Epoch 16/20, Batch Loss: 0.03012694977223873\n",
      "Epoch 16/20, Batch Loss: 0.16914458572864532\n",
      "Epoch 16/20, Batch Loss: 0.09090515971183777\n",
      "Epoch 16/20, Batch Loss: 0.2750299572944641\n",
      "Epoch 16/20, Batch Loss: 0.19491344690322876\n",
      "Epoch 16/20, Batch Loss: 0.05380062013864517\n",
      "Epoch 16/20, Batch Loss: 0.2909605801105499\n",
      "Epoch 16/20, Batch Loss: 0.053233105689287186\n",
      "Epoch 16/20, Batch Loss: 0.06940419971942902\n",
      "Epoch 16/20, Batch Loss: 0.10038936883211136\n",
      "Epoch 16/20, Batch Loss: 0.25805774331092834\n",
      "Epoch 16/20, Batch Loss: 0.06957581639289856\n",
      "Epoch 16/20, Batch Loss: 0.06723275035619736\n",
      "Epoch 16/20, Batch Loss: 0.14127741754055023\n",
      "Epoch 16/20, Batch Loss: 0.04889523610472679\n",
      "Epoch 16/20, Batch Loss: 0.017958803102374077\n",
      "Epoch 16/20, Batch Loss: 0.20889919996261597\n",
      "Epoch 16/20, Batch Loss: 0.3413289189338684\n",
      "Epoch 16/20, Batch Loss: 0.008496854454278946\n",
      "Epoch 16/20, Batch Loss: 0.13451595604419708\n",
      "Epoch 16/20, Batch Loss: 0.06674990057945251\n",
      "Epoch 16/20, Batch Loss: 0.5165953636169434\n",
      "Epoch 16/20, Batch Loss: 0.47731760144233704\n",
      "Epoch 16/20, Batch Loss: 0.01553785614669323\n",
      "Epoch 16/20, Batch Loss: 0.10851337760686874\n",
      "Epoch 16/20, Batch Loss: 0.04933181032538414\n",
      "Epoch 16/20, Batch Loss: 0.04205288365483284\n",
      "Epoch 16/20, Batch Loss: 0.058874353766441345\n",
      "Epoch 16/20, Batch Loss: 0.15186218917369843\n",
      "Epoch 16/20, Batch Loss: 0.381575345993042\n",
      "Epoch 16/20, Batch Loss: 0.13663509488105774\n",
      "Epoch 16/20, Batch Loss: 0.18063117563724518\n",
      "Epoch 16/20, Batch Loss: 0.20765934884548187\n",
      "Epoch 16/20, Batch Loss: 0.1397596001625061\n",
      "Epoch 16/20, Batch Loss: 0.042761318385601044\n",
      "Epoch 16/20, Batch Loss: 0.013234562240540981\n",
      "Epoch 16/20, Batch Loss: 0.09727515280246735\n",
      "Epoch 16/20, Batch Loss: 0.10980290174484253\n",
      "Epoch 16/20, Batch Loss: 0.23174625635147095\n",
      "Epoch 16/20, Batch Loss: 0.008040061220526695\n",
      "Epoch 16/20, Batch Loss: 0.13074462115764618\n",
      "Epoch 16/20, Batch Loss: 0.006887970492243767\n",
      "Epoch 16/20, Batch Loss: 0.03054078295826912\n",
      "Epoch 16/20, Batch Loss: 0.055255576968193054\n",
      "Epoch 16/20, Batch Loss: 0.2752297520637512\n",
      "Epoch 16/20, Batch Loss: 0.11011935025453568\n",
      "Epoch 16/20, Batch Loss: 0.16346770524978638\n",
      "Epoch 16/20, Batch Loss: 0.07848156988620758\n",
      "Epoch 16/20, Batch Loss: 0.47592806816101074\n",
      "Epoch 16/20, Batch Loss: 0.11958017200231552\n",
      "Epoch 16/20, Batch Loss: 0.25081658363342285\n",
      "Epoch 16/20, Batch Loss: 0.1734837293624878\n",
      "Epoch 16/20, Batch Loss: 0.085873082280159\n",
      "Epoch 16/20, Batch Loss: 0.050802331417798996\n",
      "Epoch 16/20, Batch Loss: 0.21782925724983215\n",
      "Epoch 16/20, Batch Loss: 0.26679345965385437\n",
      "Epoch 16/20, Batch Loss: 0.4044957160949707\n",
      "Epoch 16/20, Batch Loss: 0.3041028380393982\n",
      "Epoch 16/20, Batch Loss: 0.1937629133462906\n",
      "Epoch 16/20, Batch Loss: 0.4491296708583832\n",
      "Epoch 16/20, Batch Loss: 0.13340173661708832\n",
      "Epoch 16/20, Batch Loss: 0.19534890353679657\n",
      "Epoch 16/20, Batch Loss: 0.16634514927864075\n",
      "Epoch 16/20, Batch Loss: 0.1488794982433319\n",
      "Epoch 16/20, Batch Loss: 0.06475212424993515\n",
      "Epoch 16/20, Batch Loss: 0.4394688308238983\n",
      "Epoch 16/20, Batch Loss: 0.18627400696277618\n",
      "Epoch 16/20, Batch Loss: 0.1787828505039215\n",
      "Epoch 16/20, Batch Loss: 0.1872735619544983\n",
      "Epoch 16/20, Batch Loss: 0.2550785541534424\n",
      "Epoch 16/20, Batch Loss: 0.022616347298026085\n",
      "Epoch 16/20, Batch Loss: 0.11326881498098373\n",
      "Epoch 16/20, Batch Loss: 0.08504533767700195\n",
      "Epoch 16/20, Batch Loss: 0.4657231271266937\n",
      "Epoch 16/20, Batch Loss: 0.042331062257289886\n",
      "Epoch 16/20, Batch Loss: 0.26521360874176025\n",
      "Epoch 16/20, Batch Loss: 0.2743634581565857\n",
      "Epoch 16/20, Batch Loss: 0.026978416368365288\n",
      "Epoch 16/20, Batch Loss: 0.3192073404788971\n",
      "Epoch 16/20, Batch Loss: 0.13759832084178925\n",
      "Epoch 16/20, Batch Loss: 0.16814661026000977\n",
      "Epoch 16/20, Batch Loss: 0.24231556057929993\n",
      "Epoch 16/20, Batch Loss: 0.1761496365070343\n",
      "Epoch 16/20, Batch Loss: 0.07757671922445297\n",
      "Epoch 16/20, Batch Loss: 0.19134965538978577\n",
      "Epoch 16/20, Batch Loss: 0.2746065557003021\n",
      "Epoch 16/20, Batch Loss: 0.14906620979309082\n",
      "Epoch 16/20, Batch Loss: 0.1626039445400238\n",
      "Epoch 16/20, Batch Loss: 0.2249976396560669\n",
      "Epoch 16/20, Batch Loss: 0.2787090241909027\n",
      "Epoch 16/20, Batch Loss: 0.2890450060367584\n",
      "Epoch 16/20, Batch Loss: 0.14288848638534546\n",
      "Epoch 16/20, Batch Loss: 0.08865300565958023\n",
      "Epoch 16/20, Batch Loss: 0.40126651525497437\n",
      "Epoch 16/20, Batch Loss: 0.07933469861745834\n",
      "Epoch 16/20, Batch Loss: 0.16957831382751465\n",
      "Epoch 16/20, Batch Loss: 0.12215673178434372\n",
      "Epoch 16/20, Batch Loss: 0.03605283796787262\n",
      "Epoch 16/20, Batch Loss: 0.08248090744018555\n",
      "Epoch 16/20, Batch Loss: 0.14455878734588623\n",
      "Epoch 16/20, Batch Loss: 0.1508917212486267\n",
      "Epoch 16/20, Batch Loss: 0.16754300892353058\n",
      "Epoch 16/20, Batch Loss: 0.07174446433782578\n",
      "Epoch 16/20, Batch Loss: 0.10473403334617615\n",
      "Epoch 16/20, Batch Loss: 0.13295909762382507\n",
      "Epoch 16/20, Batch Loss: 0.06112293154001236\n",
      "Epoch 16/20, Batch Loss: 0.20550183951854706\n",
      "Epoch 16/20, Batch Loss: 0.17014065384864807\n",
      "Epoch 16/20, Batch Loss: 0.08447113633155823\n",
      "Epoch 16/20, Batch Loss: 0.05210699141025543\n",
      "Epoch 16/20, Batch Loss: 0.06751802563667297\n",
      "Epoch 16/20, Batch Loss: 0.16581188142299652\n",
      "Epoch 16/20, Batch Loss: 0.015340466983616352\n",
      "Epoch 16/20, Average Training Loss: 0.15496950790407213\n",
      "Model saved for epoch 16 at Fine_tuned_epoch16_BERT_Large.pt\n",
      "Validation Accuracy for epoch 16: 0.8770161290322581\n",
      "Epoch 17/20, Batch Loss: 0.1608705073595047\n",
      "Epoch 17/20, Batch Loss: 0.06009698659181595\n",
      "Epoch 17/20, Batch Loss: 0.07987833023071289\n",
      "Epoch 17/20, Batch Loss: 0.024107113480567932\n",
      "Epoch 17/20, Batch Loss: 0.01429861318320036\n",
      "Epoch 17/20, Batch Loss: 0.03662916645407677\n",
      "Epoch 17/20, Batch Loss: 0.05771701782941818\n",
      "Epoch 17/20, Batch Loss: 0.2400154173374176\n",
      "Epoch 17/20, Batch Loss: 0.013321727514266968\n",
      "Epoch 17/20, Batch Loss: 0.16302666068077087\n",
      "Epoch 17/20, Batch Loss: 0.12865617871284485\n",
      "Epoch 17/20, Batch Loss: 0.06950987130403519\n",
      "Epoch 17/20, Batch Loss: 0.27980780601501465\n",
      "Epoch 17/20, Batch Loss: 0.1498813033103943\n",
      "Epoch 17/20, Batch Loss: 0.09051763266324997\n",
      "Epoch 17/20, Batch Loss: 0.060433756560087204\n",
      "Epoch 17/20, Batch Loss: 0.02288680337369442\n",
      "Epoch 17/20, Batch Loss: 0.013226596638560295\n",
      "Epoch 17/20, Batch Loss: 0.011020182631909847\n",
      "Epoch 17/20, Batch Loss: 0.22636477649211884\n",
      "Epoch 17/20, Batch Loss: 0.0387747623026371\n",
      "Epoch 17/20, Batch Loss: 0.01687575690448284\n",
      "Epoch 17/20, Batch Loss: 0.035956673324108124\n",
      "Epoch 17/20, Batch Loss: 0.051630038768053055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Batch Loss: 0.10128048807382584\n",
      "Epoch 17/20, Batch Loss: 0.11621648073196411\n",
      "Epoch 17/20, Batch Loss: 0.19030247628688812\n",
      "Epoch 17/20, Batch Loss: 0.009902235120534897\n",
      "Epoch 17/20, Batch Loss: 0.014426426962018013\n",
      "Epoch 17/20, Batch Loss: 0.12686313688755035\n",
      "Epoch 17/20, Batch Loss: 0.08638321608304977\n",
      "Epoch 17/20, Batch Loss: 0.0658082515001297\n",
      "Epoch 17/20, Batch Loss: 0.007705136202275753\n",
      "Epoch 17/20, Batch Loss: 0.02463855966925621\n",
      "Epoch 17/20, Batch Loss: 0.19893580675125122\n",
      "Epoch 17/20, Batch Loss: 0.012671394273638725\n",
      "Epoch 17/20, Batch Loss: 0.11689683794975281\n",
      "Epoch 17/20, Batch Loss: 0.2630070447921753\n",
      "Epoch 17/20, Batch Loss: 0.1663386970758438\n",
      "Epoch 17/20, Batch Loss: 0.05296388640999794\n",
      "Epoch 17/20, Batch Loss: 0.08943135291337967\n",
      "Epoch 17/20, Batch Loss: 0.11785611510276794\n",
      "Epoch 17/20, Batch Loss: 0.3053772449493408\n",
      "Epoch 17/20, Batch Loss: 0.21584977209568024\n",
      "Epoch 17/20, Batch Loss: 0.1367301493883133\n",
      "Epoch 17/20, Batch Loss: 0.08547291159629822\n",
      "Epoch 17/20, Batch Loss: 0.03729037195444107\n",
      "Epoch 17/20, Batch Loss: 0.08219273388385773\n",
      "Epoch 17/20, Batch Loss: 0.007218868937343359\n",
      "Epoch 17/20, Batch Loss: 0.43116483092308044\n",
      "Epoch 17/20, Batch Loss: 0.07902184873819351\n",
      "Epoch 17/20, Batch Loss: 0.13292132318019867\n",
      "Epoch 17/20, Batch Loss: 0.07358191907405853\n",
      "Epoch 17/20, Batch Loss: 0.08534687012434006\n",
      "Epoch 17/20, Batch Loss: 0.0656055212020874\n",
      "Epoch 17/20, Batch Loss: 0.05903559923171997\n",
      "Epoch 17/20, Batch Loss: 0.0739230290055275\n",
      "Epoch 17/20, Batch Loss: 0.07308626919984818\n",
      "Epoch 17/20, Batch Loss: 0.020003359764814377\n",
      "Epoch 17/20, Batch Loss: 0.025517351925373077\n",
      "Epoch 17/20, Batch Loss: 0.02876182645559311\n",
      "Epoch 17/20, Batch Loss: 0.02337636984884739\n",
      "Epoch 17/20, Batch Loss: 0.025427749380469322\n",
      "Epoch 17/20, Batch Loss: 0.04009170085191727\n",
      "Epoch 17/20, Batch Loss: 0.04127899184823036\n",
      "Epoch 17/20, Batch Loss: 0.22394432127475739\n",
      "Epoch 17/20, Batch Loss: 0.050605982542037964\n",
      "Epoch 17/20, Batch Loss: 0.01556837372481823\n",
      "Epoch 17/20, Batch Loss: 0.05795292556285858\n",
      "Epoch 17/20, Batch Loss: 0.10923325270414352\n",
      "Epoch 17/20, Batch Loss: 0.27972808480262756\n",
      "Epoch 17/20, Batch Loss: 0.028567805886268616\n",
      "Epoch 17/20, Batch Loss: 0.1269892305135727\n",
      "Epoch 17/20, Batch Loss: 0.05998696759343147\n",
      "Epoch 17/20, Batch Loss: 0.21141010522842407\n",
      "Epoch 17/20, Batch Loss: 0.05276571214199066\n",
      "Epoch 17/20, Batch Loss: 0.06559261679649353\n",
      "Epoch 17/20, Batch Loss: 0.04178309813141823\n",
      "Epoch 17/20, Batch Loss: 0.1530497521162033\n",
      "Epoch 17/20, Batch Loss: 0.0188534464687109\n",
      "Epoch 17/20, Batch Loss: 0.03635412082076073\n",
      "Epoch 17/20, Batch Loss: 0.05430157110095024\n",
      "Epoch 17/20, Batch Loss: 0.2827974855899811\n",
      "Epoch 17/20, Batch Loss: 0.05309358611702919\n",
      "Epoch 17/20, Batch Loss: 0.019666651263833046\n",
      "Epoch 17/20, Batch Loss: 0.13688819110393524\n",
      "Epoch 17/20, Batch Loss: 0.07386361807584763\n",
      "Epoch 17/20, Batch Loss: 0.045725416392087936\n",
      "Epoch 17/20, Batch Loss: 0.278725266456604\n",
      "Epoch 17/20, Batch Loss: 0.19529320299625397\n",
      "Epoch 17/20, Batch Loss: 0.009374149143695831\n",
      "Epoch 17/20, Batch Loss: 0.03558945655822754\n",
      "Epoch 17/20, Batch Loss: 0.09849396347999573\n",
      "Epoch 17/20, Batch Loss: 0.13759994506835938\n",
      "Epoch 17/20, Batch Loss: 0.11030124872922897\n",
      "Epoch 17/20, Batch Loss: 0.057117681950330734\n",
      "Epoch 17/20, Batch Loss: 0.07605515420436859\n",
      "Epoch 17/20, Batch Loss: 0.07126764953136444\n",
      "Epoch 17/20, Batch Loss: 0.07639423757791519\n",
      "Epoch 17/20, Batch Loss: 0.07082701474428177\n",
      "Epoch 17/20, Batch Loss: 0.14411590993404388\n",
      "Epoch 17/20, Batch Loss: 0.31217366456985474\n",
      "Epoch 17/20, Batch Loss: 0.027554703876376152\n",
      "Epoch 17/20, Batch Loss: 0.10914839059114456\n",
      "Epoch 17/20, Batch Loss: 0.21549786627292633\n",
      "Epoch 17/20, Batch Loss: 0.01161928754299879\n",
      "Epoch 17/20, Batch Loss: 0.05104688182473183\n",
      "Epoch 17/20, Batch Loss: 0.17205937206745148\n",
      "Epoch 17/20, Batch Loss: 0.11892279982566833\n",
      "Epoch 17/20, Batch Loss: 0.1354658007621765\n",
      "Epoch 17/20, Batch Loss: 0.24660424888134003\n",
      "Epoch 17/20, Batch Loss: 0.012613236904144287\n",
      "Epoch 17/20, Batch Loss: 0.09695611894130707\n",
      "Epoch 17/20, Batch Loss: 0.15855780243873596\n",
      "Epoch 17/20, Batch Loss: 0.018693290650844574\n",
      "Epoch 17/20, Batch Loss: 0.04390781372785568\n",
      "Epoch 17/20, Batch Loss: 0.03419818356633186\n",
      "Epoch 17/20, Batch Loss: 0.005589849781244993\n",
      "Epoch 17/20, Batch Loss: 0.06946682929992676\n",
      "Epoch 17/20, Batch Loss: 0.5526518225669861\n",
      "Epoch 17/20, Batch Loss: 0.06704828143119812\n",
      "Epoch 17/20, Batch Loss: 0.12118632346391678\n",
      "Epoch 17/20, Batch Loss: 0.6663227677345276\n",
      "Epoch 17/20, Average Training Loss: 0.1037607347077834\n",
      "Model saved for epoch 17 at Fine_tuned_epoch17_BERT_Large.pt\n",
      "Validation Accuracy for epoch 17: 0.8532706093189965\n",
      "Epoch 18/20, Batch Loss: 0.08722026646137238\n",
      "Epoch 18/20, Batch Loss: 0.08029105514287949\n",
      "Epoch 18/20, Batch Loss: 0.48264026641845703\n",
      "Epoch 18/20, Batch Loss: 0.1002495214343071\n",
      "Epoch 18/20, Batch Loss: 0.202773779630661\n",
      "Epoch 18/20, Batch Loss: 0.2577858567237854\n",
      "Epoch 18/20, Batch Loss: 0.06627165526151657\n",
      "Epoch 18/20, Batch Loss: 0.10970870405435562\n",
      "Epoch 18/20, Batch Loss: 0.1652185618877411\n",
      "Epoch 18/20, Batch Loss: 0.15801000595092773\n",
      "Epoch 18/20, Batch Loss: 0.2296397089958191\n",
      "Epoch 18/20, Batch Loss: 0.0032148179598152637\n",
      "Epoch 18/20, Batch Loss: 0.17049844563007355\n",
      "Epoch 18/20, Batch Loss: 0.33451932668685913\n",
      "Epoch 18/20, Batch Loss: 0.6014524102210999\n",
      "Epoch 18/20, Batch Loss: 0.07550883293151855\n",
      "Epoch 18/20, Batch Loss: 0.07798716425895691\n",
      "Epoch 18/20, Batch Loss: 0.04210461676120758\n",
      "Epoch 18/20, Batch Loss: 0.20435474812984467\n",
      "Epoch 18/20, Batch Loss: 0.05747425556182861\n",
      "Epoch 18/20, Batch Loss: 0.05237681046128273\n",
      "Epoch 18/20, Batch Loss: 0.1245749294757843\n",
      "Epoch 18/20, Batch Loss: 0.10393016040325165\n",
      "Epoch 18/20, Batch Loss: 0.156453475356102\n",
      "Epoch 18/20, Batch Loss: 0.10026129335165024\n",
      "Epoch 18/20, Batch Loss: 0.12617307901382446\n",
      "Epoch 18/20, Batch Loss: 0.10768432915210724\n",
      "Epoch 18/20, Batch Loss: 0.16844724118709564\n",
      "Epoch 18/20, Batch Loss: 0.23190371692180634\n",
      "Epoch 18/20, Batch Loss: 0.19486768543720245\n",
      "Epoch 18/20, Batch Loss: 0.06484262645244598\n",
      "Epoch 18/20, Batch Loss: 0.06819871068000793\n",
      "Epoch 18/20, Batch Loss: 0.13447293639183044\n",
      "Epoch 18/20, Batch Loss: 0.07474145293235779\n",
      "Epoch 18/20, Batch Loss: 0.05907490849494934\n",
      "Epoch 18/20, Batch Loss: 0.02659882977604866\n",
      "Epoch 18/20, Batch Loss: 0.029296817258000374\n",
      "Epoch 18/20, Batch Loss: 0.0533565953373909\n",
      "Epoch 18/20, Batch Loss: 0.213765487074852\n",
      "Epoch 18/20, Batch Loss: 0.1814153790473938\n",
      "Epoch 18/20, Batch Loss: 0.16174505650997162\n",
      "Epoch 18/20, Batch Loss: 0.09942615032196045\n",
      "Epoch 18/20, Batch Loss: 0.11626181751489639\n",
      "Epoch 18/20, Batch Loss: 0.0692605972290039\n",
      "Epoch 18/20, Batch Loss: 0.07458674907684326\n",
      "Epoch 18/20, Batch Loss: 0.10769440233707428\n",
      "Epoch 18/20, Batch Loss: 0.21233446896076202\n",
      "Epoch 18/20, Batch Loss: 0.022179264575242996\n",
      "Epoch 18/20, Batch Loss: 0.053385864943265915\n",
      "Epoch 18/20, Batch Loss: 0.051906146109104156\n",
      "Epoch 18/20, Batch Loss: 0.04129122197628021\n",
      "Epoch 18/20, Batch Loss: 0.027133308351039886\n",
      "Epoch 18/20, Batch Loss: 0.044862281531095505\n",
      "Epoch 18/20, Batch Loss: 0.01288644876331091\n",
      "Epoch 18/20, Batch Loss: 0.13352259993553162\n",
      "Epoch 18/20, Batch Loss: 0.11934679001569748\n",
      "Epoch 18/20, Batch Loss: 0.0470343641936779\n",
      "Epoch 18/20, Batch Loss: 0.007783136330544949\n",
      "Epoch 18/20, Batch Loss: 0.47914648056030273\n",
      "Epoch 18/20, Batch Loss: 0.08533744513988495\n",
      "Epoch 18/20, Batch Loss: 0.02710128203034401\n",
      "Epoch 18/20, Batch Loss: 0.05483207106590271\n",
      "Epoch 18/20, Batch Loss: 0.32753947377204895\n",
      "Epoch 18/20, Batch Loss: 0.025040047243237495\n",
      "Epoch 18/20, Batch Loss: 0.05627147853374481\n",
      "Epoch 18/20, Batch Loss: 0.042018257081508636\n",
      "Epoch 18/20, Batch Loss: 0.052041806280612946\n",
      "Epoch 18/20, Batch Loss: 0.007861770689487457\n",
      "Epoch 18/20, Batch Loss: 0.010225852951407433\n",
      "Epoch 18/20, Batch Loss: 0.09934430569410324\n",
      "Epoch 18/20, Batch Loss: 0.0525931641459465\n",
      "Epoch 18/20, Batch Loss: 0.009824943728744984\n",
      "Epoch 18/20, Batch Loss: 0.061316605657339096\n",
      "Epoch 18/20, Batch Loss: 0.19543316960334778\n",
      "Epoch 18/20, Batch Loss: 0.23857703804969788\n",
      "Epoch 18/20, Batch Loss: 0.04675845056772232\n",
      "Epoch 18/20, Batch Loss: 0.12622420489788055\n",
      "Epoch 18/20, Batch Loss: 0.061619941145181656\n",
      "Epoch 18/20, Batch Loss: 0.020223891362547874\n",
      "Epoch 18/20, Batch Loss: 0.03355155140161514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Batch Loss: 0.008050523698329926\n",
      "Epoch 18/20, Batch Loss: 0.2926294207572937\n",
      "Epoch 18/20, Batch Loss: 0.17167049646377563\n",
      "Epoch 18/20, Batch Loss: 0.16632629930973053\n",
      "Epoch 18/20, Batch Loss: 0.052897896617650986\n",
      "Epoch 18/20, Batch Loss: 0.09491753578186035\n",
      "Epoch 18/20, Batch Loss: 0.07684548944234848\n",
      "Epoch 18/20, Batch Loss: 0.03623305633664131\n",
      "Epoch 18/20, Batch Loss: 0.14714865386486053\n",
      "Epoch 18/20, Batch Loss: 0.0288859736174345\n",
      "Epoch 18/20, Batch Loss: 0.03675215318799019\n",
      "Epoch 18/20, Batch Loss: 0.1643875241279602\n",
      "Epoch 18/20, Batch Loss: 0.22752827405929565\n",
      "Epoch 18/20, Batch Loss: 0.037227120250463486\n",
      "Epoch 18/20, Batch Loss: 0.012126506306231022\n",
      "Epoch 18/20, Batch Loss: 0.27505046129226685\n",
      "Epoch 18/20, Batch Loss: 0.16207093000411987\n",
      "Epoch 18/20, Batch Loss: 0.0847943052649498\n",
      "Epoch 18/20, Batch Loss: 0.09003118425607681\n",
      "Epoch 18/20, Batch Loss: 0.038719650357961655\n",
      "Epoch 18/20, Batch Loss: 0.03900662809610367\n",
      "Epoch 18/20, Batch Loss: 0.17195899784564972\n",
      "Epoch 18/20, Batch Loss: 0.08024860918521881\n",
      "Epoch 18/20, Batch Loss: 0.0047275167889893055\n",
      "Epoch 18/20, Batch Loss: 0.03608148172497749\n",
      "Epoch 18/20, Batch Loss: 0.020213168114423752\n",
      "Epoch 18/20, Batch Loss: 0.10076531022787094\n",
      "Epoch 18/20, Batch Loss: 0.04373908415436745\n",
      "Epoch 18/20, Batch Loss: 0.03028595633804798\n",
      "Epoch 18/20, Batch Loss: 0.08059397339820862\n",
      "Epoch 18/20, Batch Loss: 0.03348726034164429\n",
      "Epoch 18/20, Batch Loss: 0.34146809577941895\n",
      "Epoch 18/20, Batch Loss: 0.0557398721575737\n",
      "Epoch 18/20, Batch Loss: 0.018589729443192482\n",
      "Epoch 18/20, Batch Loss: 0.6191413402557373\n",
      "Epoch 18/20, Batch Loss: 0.016262680292129517\n",
      "Epoch 18/20, Batch Loss: 0.039337337017059326\n",
      "Epoch 18/20, Batch Loss: 0.09745083749294281\n",
      "Epoch 18/20, Batch Loss: 0.2366074174642563\n",
      "Epoch 18/20, Batch Loss: 0.1702096313238144\n",
      "Epoch 18/20, Batch Loss: 0.0336335152387619\n",
      "Epoch 18/20, Batch Loss: 0.16596652567386627\n",
      "Epoch 18/20, Batch Loss: 0.0045759654603898525\n",
      "Epoch 18/20, Average Training Loss: 0.11332738823900984\n",
      "Model saved for epoch 18 at Fine_tuned_epoch18_BERT_Large.pt\n",
      "Validation Accuracy for epoch 18: 0.808915770609319\n",
      "Epoch 19/20, Batch Loss: 0.24521267414093018\n",
      "Epoch 19/20, Batch Loss: 0.20811903476715088\n",
      "Epoch 19/20, Batch Loss: 0.018634114414453506\n",
      "Epoch 19/20, Batch Loss: 0.04253668710589409\n",
      "Epoch 19/20, Batch Loss: 0.0437963530421257\n",
      "Epoch 19/20, Batch Loss: 0.012493373826146126\n",
      "Epoch 19/20, Batch Loss: 0.15558186173439026\n",
      "Epoch 19/20, Batch Loss: 0.08080072700977325\n",
      "Epoch 19/20, Batch Loss: 0.03654693067073822\n",
      "Epoch 19/20, Batch Loss: 0.1726188212633133\n",
      "Epoch 19/20, Batch Loss: 0.05894921347498894\n",
      "Epoch 19/20, Batch Loss: 0.007441556081175804\n",
      "Epoch 19/20, Batch Loss: 0.01690615527331829\n",
      "Epoch 19/20, Batch Loss: 0.004232640378177166\n",
      "Epoch 19/20, Batch Loss: 0.08495066314935684\n",
      "Epoch 19/20, Batch Loss: 0.11935380101203918\n",
      "Epoch 19/20, Batch Loss: 0.15655618906021118\n",
      "Epoch 19/20, Batch Loss: 0.19406042993068695\n",
      "Epoch 19/20, Batch Loss: 0.047098807990550995\n",
      "Epoch 19/20, Batch Loss: 0.04703209549188614\n",
      "Epoch 19/20, Batch Loss: 0.02919045276939869\n",
      "Epoch 19/20, Batch Loss: 0.023353731259703636\n",
      "Epoch 19/20, Batch Loss: 0.04896535724401474\n",
      "Epoch 19/20, Batch Loss: 0.04968928545713425\n",
      "Epoch 19/20, Batch Loss: 0.30319973826408386\n",
      "Epoch 19/20, Batch Loss: 0.17280393838882446\n",
      "Epoch 19/20, Batch Loss: 0.031674373894929886\n",
      "Epoch 19/20, Batch Loss: 0.025619205087423325\n",
      "Epoch 19/20, Batch Loss: 0.024208318442106247\n",
      "Epoch 19/20, Batch Loss: 0.08458215743303299\n",
      "Epoch 19/20, Batch Loss: 0.012693510390818119\n",
      "Epoch 19/20, Batch Loss: 0.014398782514035702\n",
      "Epoch 19/20, Batch Loss: 0.03133516013622284\n",
      "Epoch 19/20, Batch Loss: 0.022023526951670647\n",
      "Epoch 19/20, Batch Loss: 0.05338063836097717\n",
      "Epoch 19/20, Batch Loss: 0.07453576475381851\n",
      "Epoch 19/20, Batch Loss: 0.017265524715185165\n",
      "Epoch 19/20, Batch Loss: 0.024579375982284546\n",
      "Epoch 19/20, Batch Loss: 0.015730731189250946\n",
      "Epoch 19/20, Batch Loss: 0.013796862214803696\n",
      "Epoch 19/20, Batch Loss: 0.015211369842290878\n",
      "Epoch 19/20, Batch Loss: 0.09392355382442474\n",
      "Epoch 19/20, Batch Loss: 0.013897827826440334\n",
      "Epoch 19/20, Batch Loss: 0.27952489256858826\n",
      "Epoch 19/20, Batch Loss: 0.07345661520957947\n",
      "Epoch 19/20, Batch Loss: 0.11553148180246353\n",
      "Epoch 19/20, Batch Loss: 0.1723385453224182\n",
      "Epoch 19/20, Batch Loss: 0.01339969877153635\n",
      "Epoch 19/20, Batch Loss: 0.1047145426273346\n",
      "Epoch 19/20, Batch Loss: 0.37941479682922363\n",
      "Epoch 19/20, Batch Loss: 0.10735536366701126\n",
      "Epoch 19/20, Batch Loss: 0.0487506240606308\n",
      "Epoch 19/20, Batch Loss: 0.09788684546947479\n",
      "Epoch 19/20, Batch Loss: 0.02186753787100315\n",
      "Epoch 19/20, Batch Loss: 0.0658227950334549\n",
      "Epoch 19/20, Batch Loss: 0.016999585554003716\n",
      "Epoch 19/20, Batch Loss: 0.01660541445016861\n",
      "Epoch 19/20, Batch Loss: 0.026271652430295944\n",
      "Epoch 19/20, Batch Loss: 0.13001246750354767\n",
      "Epoch 19/20, Batch Loss: 0.12401296198368073\n",
      "Epoch 19/20, Batch Loss: 0.11341116577386856\n",
      "Epoch 19/20, Batch Loss: 0.021234609186649323\n",
      "Epoch 19/20, Batch Loss: 0.01626100204885006\n",
      "Epoch 19/20, Batch Loss: 0.08387959748506546\n",
      "Epoch 19/20, Batch Loss: 0.3063625395298004\n",
      "Epoch 19/20, Batch Loss: 0.03710704669356346\n",
      "Epoch 19/20, Batch Loss: 0.07235602289438248\n",
      "Epoch 19/20, Batch Loss: 0.035659074783325195\n",
      "Epoch 19/20, Batch Loss: 0.0037616174668073654\n",
      "Epoch 19/20, Batch Loss: 0.125279501080513\n",
      "Epoch 19/20, Batch Loss: 0.011778198182582855\n",
      "Epoch 19/20, Batch Loss: 0.0014930275501683354\n",
      "Epoch 19/20, Batch Loss: 0.003991805482655764\n",
      "Epoch 19/20, Batch Loss: 0.08338072150945663\n",
      "Epoch 19/20, Batch Loss: 0.002089889021590352\n",
      "Epoch 19/20, Batch Loss: 0.2531158924102783\n",
      "Epoch 19/20, Batch Loss: 0.23563337326049805\n",
      "Epoch 19/20, Batch Loss: 0.14448274672031403\n",
      "Epoch 19/20, Batch Loss: 0.07420175522565842\n",
      "Epoch 19/20, Batch Loss: 0.009951198473572731\n",
      "Epoch 19/20, Batch Loss: 0.04885689169168472\n",
      "Epoch 19/20, Batch Loss: 0.011853864416480064\n",
      "Epoch 19/20, Batch Loss: 0.22435307502746582\n",
      "Epoch 19/20, Batch Loss: 0.12707583606243134\n",
      "Epoch 19/20, Batch Loss: 0.09628982096910477\n",
      "Epoch 19/20, Batch Loss: 0.32173752784729004\n",
      "Epoch 19/20, Batch Loss: 0.026041250675916672\n",
      "Epoch 19/20, Batch Loss: 0.031638894230127335\n",
      "Epoch 19/20, Batch Loss: 0.1396469920873642\n",
      "Epoch 19/20, Batch Loss: 0.10556596517562866\n",
      "Epoch 19/20, Batch Loss: 0.2791076600551605\n",
      "Epoch 19/20, Batch Loss: 0.09469243884086609\n",
      "Epoch 19/20, Batch Loss: 0.07297500967979431\n",
      "Epoch 19/20, Batch Loss: 0.0238207895308733\n",
      "Epoch 19/20, Batch Loss: 0.042082950472831726\n",
      "Epoch 19/20, Batch Loss: 0.2640645205974579\n",
      "Epoch 19/20, Batch Loss: 0.061443839222192764\n",
      "Epoch 19/20, Batch Loss: 0.03984898701310158\n",
      "Epoch 19/20, Batch Loss: 0.0257378239184618\n",
      "Epoch 19/20, Batch Loss: 0.02843054011464119\n",
      "Epoch 19/20, Batch Loss: 0.014391828328371048\n",
      "Epoch 19/20, Batch Loss: 0.06535978615283966\n",
      "Epoch 19/20, Batch Loss: 0.29449254274368286\n",
      "Epoch 19/20, Batch Loss: 0.015602977946400642\n",
      "Epoch 19/20, Batch Loss: 0.17848186194896698\n",
      "Epoch 19/20, Batch Loss: 0.028232689946889877\n",
      "Epoch 19/20, Batch Loss: 0.08131735026836395\n",
      "Epoch 19/20, Batch Loss: 0.2919248342514038\n",
      "Epoch 19/20, Batch Loss: 0.017860176041722298\n",
      "Epoch 19/20, Batch Loss: 0.11983790993690491\n",
      "Epoch 19/20, Batch Loss: 0.01717723347246647\n",
      "Epoch 19/20, Batch Loss: 0.09551163017749786\n",
      "Epoch 19/20, Batch Loss: 0.006830466445535421\n",
      "Epoch 19/20, Batch Loss: 0.22934429347515106\n",
      "Epoch 19/20, Batch Loss: 0.0225476436316967\n",
      "Epoch 19/20, Batch Loss: 0.058373142033815384\n",
      "Epoch 19/20, Batch Loss: 0.14157892763614655\n",
      "Epoch 19/20, Batch Loss: 0.5055723190307617\n",
      "Epoch 19/20, Batch Loss: 0.14961031079292297\n",
      "Epoch 19/20, Batch Loss: 0.0087841572239995\n",
      "Epoch 19/20, Batch Loss: 0.07513007521629333\n",
      "Epoch 19/20, Batch Loss: 0.2749274969100952\n",
      "Epoch 19/20, Batch Loss: 0.0195272509008646\n",
      "Epoch 19/20, Average Training Loss: 0.09069967077102331\n",
      "Model saved for epoch 19 at Fine_tuned_epoch19_BERT_Large.pt\n",
      "Validation Accuracy for epoch 19: 0.8093637992831542\n",
      "Epoch 20/20, Batch Loss: 0.03505559638142586\n",
      "Epoch 20/20, Batch Loss: 0.0517764538526535\n",
      "Epoch 20/20, Batch Loss: 0.23224735260009766\n",
      "Epoch 20/20, Batch Loss: 0.05093798786401749\n",
      "Epoch 20/20, Batch Loss: 0.20043815672397614\n",
      "Epoch 20/20, Batch Loss: 0.043155960738658905\n",
      "Epoch 20/20, Batch Loss: 0.007620586082339287\n",
      "Epoch 20/20, Batch Loss: 0.010311244986951351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Batch Loss: 0.08106157928705215\n",
      "Epoch 20/20, Batch Loss: 0.1301436573266983\n",
      "Epoch 20/20, Batch Loss: 0.13358084857463837\n",
      "Epoch 20/20, Batch Loss: 0.013361196033656597\n",
      "Epoch 20/20, Batch Loss: 0.13468234241008759\n",
      "Epoch 20/20, Batch Loss: 0.08331257104873657\n",
      "Epoch 20/20, Batch Loss: 0.02119387686252594\n",
      "Epoch 20/20, Batch Loss: 0.24267525970935822\n",
      "Epoch 20/20, Batch Loss: 0.1249944344162941\n",
      "Epoch 20/20, Batch Loss: 0.02781441994011402\n",
      "Epoch 20/20, Batch Loss: 0.21030697226524353\n",
      "Epoch 20/20, Batch Loss: 0.008212164975702763\n",
      "Epoch 20/20, Batch Loss: 0.07069560140371323\n",
      "Epoch 20/20, Batch Loss: 0.06323979049921036\n",
      "Epoch 20/20, Batch Loss: 0.004910886287689209\n",
      "Epoch 20/20, Batch Loss: 0.03518778830766678\n",
      "Epoch 20/20, Batch Loss: 0.04432138055562973\n",
      "Epoch 20/20, Batch Loss: 0.06531185656785965\n",
      "Epoch 20/20, Batch Loss: 0.008474848233163357\n",
      "Epoch 20/20, Batch Loss: 0.04587271064519882\n",
      "Epoch 20/20, Batch Loss: 0.12203588336706161\n",
      "Epoch 20/20, Batch Loss: 0.006231021601706743\n",
      "Epoch 20/20, Batch Loss: 0.04391445219516754\n",
      "Epoch 20/20, Batch Loss: 0.010600653477013111\n",
      "Epoch 20/20, Batch Loss: 0.03445606306195259\n",
      "Epoch 20/20, Batch Loss: 0.007814079523086548\n",
      "Epoch 20/20, Batch Loss: 0.13751289248466492\n",
      "Epoch 20/20, Batch Loss: 0.01665552332997322\n",
      "Epoch 20/20, Batch Loss: 0.007052470464259386\n",
      "Epoch 20/20, Batch Loss: 0.04447731003165245\n",
      "Epoch 20/20, Batch Loss: 0.00534754479303956\n",
      "Epoch 20/20, Batch Loss: 0.08227607607841492\n",
      "Epoch 20/20, Batch Loss: 0.005601068492978811\n",
      "Epoch 20/20, Batch Loss: 0.011668667197227478\n",
      "Epoch 20/20, Batch Loss: 0.0289609432220459\n",
      "Epoch 20/20, Batch Loss: 0.005885058548301458\n",
      "Epoch 20/20, Batch Loss: 0.13580194115638733\n",
      "Epoch 20/20, Batch Loss: 0.12377472221851349\n",
      "Epoch 20/20, Batch Loss: 0.0011857075151056051\n",
      "Epoch 20/20, Batch Loss: 0.005967414006590843\n",
      "Epoch 20/20, Batch Loss: 0.007645438425242901\n",
      "Epoch 20/20, Batch Loss: 0.026750482618808746\n",
      "Epoch 20/20, Batch Loss: 0.007266540080308914\n",
      "Epoch 20/20, Batch Loss: 0.023716559633612633\n",
      "Epoch 20/20, Batch Loss: 0.057830560952425\n",
      "Epoch 20/20, Batch Loss: 0.6318235993385315\n",
      "Epoch 20/20, Batch Loss: 0.0044400859624147415\n",
      "Epoch 20/20, Batch Loss: 0.028368866071105003\n",
      "Epoch 20/20, Batch Loss: 0.01194745022803545\n",
      "Epoch 20/20, Batch Loss: 0.02024649642407894\n",
      "Epoch 20/20, Batch Loss: 0.01881522871553898\n",
      "Epoch 20/20, Batch Loss: 0.009183263406157494\n",
      "Epoch 20/20, Batch Loss: 0.36452344059944153\n",
      "Epoch 20/20, Batch Loss: 0.008613190613687038\n",
      "Epoch 20/20, Batch Loss: 0.050536274909973145\n",
      "Epoch 20/20, Batch Loss: 0.016795435920357704\n",
      "Epoch 20/20, Batch Loss: 0.017568644136190414\n",
      "Epoch 20/20, Batch Loss: 0.011147958226501942\n",
      "Epoch 20/20, Batch Loss: 0.03625722974538803\n",
      "Epoch 20/20, Batch Loss: 0.014065275900065899\n",
      "Epoch 20/20, Batch Loss: 0.0140402652323246\n",
      "Epoch 20/20, Batch Loss: 0.005063226912170649\n",
      "Epoch 20/20, Batch Loss: 0.008511176332831383\n",
      "Epoch 20/20, Batch Loss: 0.2017228901386261\n",
      "Epoch 20/20, Batch Loss: 0.03211161866784096\n",
      "Epoch 20/20, Batch Loss: 0.03240124136209488\n",
      "Epoch 20/20, Batch Loss: 0.12741483747959137\n",
      "Epoch 20/20, Batch Loss: 0.05295093357563019\n",
      "Epoch 20/20, Batch Loss: 0.13900843262672424\n",
      "Epoch 20/20, Batch Loss: 0.2890757918357849\n",
      "Epoch 20/20, Batch Loss: 0.14226724207401276\n",
      "Epoch 20/20, Batch Loss: 0.026055647060275078\n",
      "Epoch 20/20, Batch Loss: 0.003138054860755801\n",
      "Epoch 20/20, Batch Loss: 0.24409405887126923\n",
      "Epoch 20/20, Batch Loss: 0.0461147241294384\n",
      "Epoch 20/20, Batch Loss: 0.05002157390117645\n",
      "Epoch 20/20, Batch Loss: 0.27726873755455017\n",
      "Epoch 20/20, Batch Loss: 0.03537431359291077\n",
      "Epoch 20/20, Batch Loss: 0.025945838540792465\n",
      "Epoch 20/20, Batch Loss: 0.005981693509966135\n",
      "Epoch 20/20, Batch Loss: 0.13161654770374298\n",
      "Epoch 20/20, Batch Loss: 0.1706846058368683\n",
      "Epoch 20/20, Batch Loss: 0.009968671947717667\n",
      "Epoch 20/20, Batch Loss: 0.02938496135175228\n",
      "Epoch 20/20, Batch Loss: 0.24762950837612152\n",
      "Epoch 20/20, Batch Loss: 0.05135539174079895\n",
      "Epoch 20/20, Batch Loss: 0.012806289829313755\n",
      "Epoch 20/20, Batch Loss: 0.05511923134326935\n",
      "Epoch 20/20, Batch Loss: 0.017933661118149757\n",
      "Epoch 20/20, Batch Loss: 0.07381493598222733\n",
      "Epoch 20/20, Batch Loss: 0.0406087189912796\n",
      "Epoch 20/20, Batch Loss: 0.04188978299498558\n",
      "Epoch 20/20, Batch Loss: 0.03797127678990364\n",
      "Epoch 20/20, Batch Loss: 0.05090399459004402\n",
      "Epoch 20/20, Batch Loss: 0.01767660304903984\n",
      "Epoch 20/20, Batch Loss: 0.017332477495074272\n",
      "Epoch 20/20, Batch Loss: 0.08020172268152237\n",
      "Epoch 20/20, Batch Loss: 0.05010038614273071\n",
      "Epoch 20/20, Batch Loss: 0.046656280755996704\n",
      "Epoch 20/20, Batch Loss: 0.004661071579903364\n",
      "Epoch 20/20, Batch Loss: 0.08696675300598145\n",
      "Epoch 20/20, Batch Loss: 0.008466143161058426\n",
      "Epoch 20/20, Batch Loss: 0.03066430613398552\n",
      "Epoch 20/20, Batch Loss: 0.016788706183433533\n",
      "Epoch 20/20, Batch Loss: 0.024751635268330574\n",
      "Epoch 20/20, Batch Loss: 0.10224778205156326\n",
      "Epoch 20/20, Batch Loss: 0.052913300693035126\n",
      "Epoch 20/20, Batch Loss: 0.025113185867667198\n",
      "Epoch 20/20, Batch Loss: 0.1090732142329216\n",
      "Epoch 20/20, Batch Loss: 0.14435957372188568\n",
      "Epoch 20/20, Batch Loss: 0.14222809672355652\n",
      "Epoch 20/20, Batch Loss: 0.03538499400019646\n",
      "Epoch 20/20, Batch Loss: 0.016429824754595757\n",
      "Epoch 20/20, Batch Loss: 0.04112527519464493\n",
      "Epoch 20/20, Batch Loss: 0.05282697454094887\n",
      "Epoch 20/20, Average Training Loss: 0.0668445302711088\n",
      "Model saved for epoch 20 at Fine_tuned_epoch20_BERT_Large.pt\n",
      "Validation Accuracy for epoch 20: 0.8532706093189965\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuneÏö©\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[-8]  # nÎ≤àÏß∏ Î†àÏù¥Ïñ¥Ïùò hidden statesÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "data_A = pd.read_csv(\"output3.csv\")  # data set A ÌååÏùºÎ™ÖÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B ÌååÏùºÎ™ÖÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "# Î™®Îç∏ Î∂àÎü¨Ïò§Îäî Í≤ΩÎ°ú\n",
    "model_path = \"Pre_train_epoch2_BERT_Large.pt\"\n",
    "# Î™®Îç∏ Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "model_path2 = \"Fine-tuned.pt\"\n",
    "\n",
    "# X_train, Y_train ÏÉùÏÑ±\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():  # Ï§ëÎ≥µ Ï†úÍ±∞Î•º ÌïòÏßÄ ÏïäÍ≥† ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "#print(\"X_train\\n\", X_train[:10])\n",
    "#print(\"Y_train\\n\", Y_train[:10])\n",
    "        \n",
    "# BERT ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Î™®Îç∏ Î°úÎìú\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "# Î™®Îç∏Ïù¥ Ïù¥ÎØ∏ Ï†ÄÏû•ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥†, Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏûàÏúºÎ©¥ Î∂àÎü¨Ïò§Í≥† ÏóÜÏúºÎ©¥ ÏÉàÎ°úÏö¥ Î™®Îç∏ ÏÉùÏÑ±\n",
    "if os.path.exists(model_path):\n",
    "    # Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏûàÏùÑ Í≤ΩÏö∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Pre-train model loaded.\")\n",
    "else:\n",
    "    # Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏóÜÏùÑ Í≤ΩÏö∞ ÏÉàÎ°úÏö¥ Î™®Îç∏ ÏÉùÏÑ±\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î•º BERTÏùò ÏûÖÎ†• ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "max_len = 128  # ÏûÖÎ†• ÏãúÌÄÄÏä§Ïùò ÏµúÎåÄ Í∏∏Ïù¥\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # ÌôòÏûê Ï†ïÎ≥¥ Î∞è Ï¶ùÏÉÅ\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä\n",
    "                        max_length = max_len,         # ÏµúÎåÄ Í∏∏Ïù¥ ÏßÄÏ†ï\n",
    "                        pad_to_max_length = True,     # Ìå®Îî©ÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ ÏµúÎåÄ Í∏∏Ïù¥Î°ú ÎßûÏ∂§\n",
    "                        return_attention_mask = True, # Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±\n",
    "                        return_tensors = 'pt',        # PyTorch ÌÖêÏÑúÎ°ú Î∞òÌôò\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î∞è Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = 0.8\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=1-train_size, random_state=42)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# GPU ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Î™®Îç∏ÏùÑ GPUÎ°ú Ïù¥Îèô\n",
    "model.to(device)\n",
    "\n",
    "# ÏòµÌã∞ÎßàÏù¥Ï†Ä Î∞è ÌïôÏäµÎ•† ÏÑ§Ï†ï\n",
    "# Í∏∞Î≥∏ ÌïôÏäµÎ•† : 2e-6\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)\n",
    "\n",
    "# ÏóêÌè≠ ÏÑ§Ï†ï\n",
    "epochs = 20\n",
    "\n",
    "# ÌïôÏäµ Î£®ÌîÑ\n",
    "hidden_states_list = []  # Î™®Îì† ÏóêÌè≠Ïóê ÎåÄÌïú hidden stateÎ•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n",
    "# ÌïôÏäµ Î£®ÌîÑ\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[1]  # lossÍ∞Ä outputsÏùò Îëê Î≤àÏß∏ Í∞íÏûÖÎãàÎã§.\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Batch Loss: {loss.item()}')\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # Î™®Îç∏ Ï†ÄÏû• Î∞è ÌèâÍ∞Ä\n",
    "    model_save_path = f\"Fine_tuned_epoch{epoch + 1}_BERT_Large.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved for epoch {epoch + 1} at {model_save_path}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs[0]  # logitsÍ∞Ä outputsÏùò Ï≤´ Î≤àÏß∏ Í∞íÏûÖÎãàÎã§.\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "\n",
    "    print(f'Validation Accuracy for epoch {epoch + 1}: {val_accuracy / len(val_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93cb1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ ÎûúÎç§Î∂ÑÌï†(500/500/250)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sample_csv_and_additional(input_file, output_file_500, output_file_100, n_500):\n",
    "    # CSV ÌååÏùºÏùÑ ÏùΩÏñ¥ÏòµÎãàÎã§.\n",
    "    data = pd.read_csv(input_file)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞Î•º ÎûúÎç§ÌïòÍ≤å ÏÉòÌîåÎßÅÌï©ÎãàÎã§.\n",
    "    sampled_data_750 = data.sample(n=n_500, random_state=42)\n",
    "    \n",
    "    # Ï≤´ 250Í∞ú Îç∞Ïù¥ÌÑ∞Î•º output_file_500Í≥º output_file_100Ïóê ÏàúÏÑúÎåÄÎ°ú ÏÇΩÏûÖÌï©ÎãàÎã§.\n",
    "    first_250 = sampled_data_750[:250]\n",
    "    first_250.to_csv(output_file_500, index=False)\n",
    "    first_250.to_csv(output_file_100, index=False)\n",
    "    \n",
    "    # ÎÇòÎ®∏ÏßÄ 500Í∞ú Îç∞Ïù¥ÌÑ∞Î•º Ï†àÎ∞òÏúºÎ°ú ÎÇòÎàÑÏñ¥ Í∞ÅÍ∞Å output_file_500Í≥º output_file_100Ïóê Ï∂îÍ∞ÄÌï©ÎãàÎã§.\n",
    "    remaining_500 = sampled_data_750[250:]\n",
    "    split_idx = len(remaining_500) // 2\n",
    "    second_250_500 = remaining_500[:split_idx]\n",
    "    second_250_100 = remaining_500[split_idx:]\n",
    "    \n",
    "    # ÌååÏùºÏóê Ï∂îÍ∞ÄÌï©ÎãàÎã§.\n",
    "    second_250_500.to_csv(output_file_500, mode='a', header=False, index=False)\n",
    "    second_250_100.to_csv(output_file_100, mode='a', header=False, index=False)\n",
    "\n",
    "# ÏûÖÎ†• CSV ÌååÏùº Í≤ΩÎ°ú\n",
    "input_file = \"output6.csv\"\n",
    "\n",
    "# Ï∂úÎ†• CSV ÌååÏùº Í≤ΩÎ°ú\n",
    "output_file_500 = \"random_500_D.csv\"\n",
    "output_file_100 = \"random_500_C.csv\"\n",
    "\n",
    "# ÎûúÎç§ÌïòÍ≤å Ï∂îÏ∂úÌï† Îç∞Ïù¥ÌÑ∞ Í∞úÏàò\n",
    "n_500 = 750\n",
    "\n",
    "# Ìï®Ïàò Ìò∏Ï∂ú\n",
    "sample_csv_and_additional(input_file, output_file_500, output_file_100, n_500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97fe8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ ÎûúÎç§Î∂ÑÌï†(300/500)\n",
    "import pandas as pd\n",
    "\n",
    "def sample_csv_and_additional(input_file, output_file_500, output_file_100, n_500):\n",
    "    # CSV ÌååÏùºÏùÑ ÏùΩÏñ¥ÏòµÎãàÎã§.\n",
    "    data = pd.read_csv(input_file)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞Î•º ÎûúÎç§ÌïòÍ≤å ÏÉòÌîåÎßÅÌï©ÎãàÎã§.\n",
    "    sampled_data_500 = data.sample(n=n_500, random_state=42)\n",
    "    \n",
    "    # ÏÉòÌîåÎßÅÎêú 500Í∞úÏùò Îç∞Ïù¥ÌÑ∞Î•º CSV ÌååÏùºÎ°ú ÎÇ¥Î≥¥ÎÉÖÎãàÎã§.\n",
    "    sampled_data_500.to_csv(output_file_500, index=False)\n",
    "    \n",
    "    # sampled_data_500ÏóêÏÑú Ï≤´ 100Í∞úÏùò Îç∞Ïù¥ÌÑ∞Î•º ÏÑ†ÌÉùÌï©ÎãàÎã§.\n",
    "    sampled_data_100 = sampled_data_500.head(500)\n",
    "    \n",
    "    # ÏÑ†ÌÉùÎêú Ï≤´ 100Í∞úÏùò Îç∞Ïù¥ÌÑ∞Î•º CSV ÌååÏùºÎ°ú ÎÇ¥Î≥¥ÎÉÖÎãàÎã§.\n",
    "    sampled_data_100.to_csv(output_file_100, index=False)\n",
    "\n",
    "# ÏûÖÎ†• CSV ÌååÏùº Í≤ΩÎ°ú\n",
    "input_file = \"output6.csv\"\n",
    "\n",
    "# Ï∂úÎ†• CSV ÌååÏùº Í≤ΩÎ°ú\n",
    "output_file_500 = \"random_500.csv\"\n",
    "output_file_100 = \"random_300.csv\"\n",
    "\n",
    "# ÎûúÎç§ÌïòÍ≤å Ï∂îÏ∂úÌï† Îç∞Ïù¥ÌÑ∞ Í∞úÏàò\n",
    "n_500 = 1000\n",
    "\n",
    "# Ìï®Ïàò Ìò∏Ï∂ú\n",
    "sample_csv_and_additional(input_file, output_file_500, output_file_100, n_500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3936463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCC\\anaconda3\\envs\\biotf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Validation Accuracy: 0.8819444444444444\n"
     ]
    }
   ],
   "source": [
    "# smashed data ÏÉùÏÑ± (500/server side)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[12]  # nÎ≤àÏß∏ Î†àÏù¥Ïñ¥Ïùò hidden statesÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "data_A = pd.read_csv(\"random_500.csv\")  # data set A ÌååÏùºÎ™ÖÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B ÌååÏùºÎ™ÖÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "# Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "model_path = \"Pre_train_epoch2_BERT_Large.pt\"\n",
    "\n",
    "# X_train, Y_train ÏÉùÏÑ±\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():  # Ï§ëÎ≥µ Ï†úÍ±∞Î•º ÌïòÏßÄ ÏïäÍ≥† ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "#print(\"X_train\\n\", X_train[:10])\n",
    "#print(\"Y_train\\n\", Y_train[:10])\n",
    "        \n",
    "# BERT ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Î™®Îç∏ Î°úÎìú\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "# Î™®Îç∏Ïù¥ Ïù¥ÎØ∏ Ï†ÄÏû•ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥†, Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏûàÏúºÎ©¥ Î∂àÎü¨Ïò§Í≥† ÏóÜÏúºÎ©¥ ÏÉàÎ°úÏö¥ Î™®Îç∏ ÏÉùÏÑ±\n",
    "if os.path.exists(model_path):\n",
    "    # Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏûàÏùÑ Í≤ΩÏö∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Pre-train model loaded.\")\n",
    "else:\n",
    "    # Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏóÜÏùÑ Í≤ΩÏö∞ ÏÉàÎ°úÏö¥ Î™®Îç∏ ÏÉùÏÑ±\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î•º BERTÏùò ÏûÖÎ†• ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "max_len = 128  # ÏûÖÎ†• ÏãúÌÄÄÏä§Ïùò ÏµúÎåÄ Í∏∏Ïù¥\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # ÌôòÏûê Ï†ïÎ≥¥ Î∞è Ï¶ùÏÉÅ\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä\n",
    "                        max_length = max_len,         # ÏµúÎåÄ Í∏∏Ïù¥ ÏßÄÏ†ï\n",
    "                        pad_to_max_length = True,     # Ìå®Îî©ÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ ÏµúÎåÄ Í∏∏Ïù¥Î°ú ÎßûÏ∂§\n",
    "                        return_attention_mask = True, # Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±\n",
    "                        return_tensors = 'pt',        # PyTorch ÌÖêÏÑúÎ°ú Î∞òÌôò\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# GPU ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Î™®Îç∏ÏùÑ GPUÎ°ú Ïù¥Îèô\n",
    "model.to(device)\n",
    "\n",
    "# Î™®Îç∏ ÌèâÍ∞Ä\n",
    "model.eval()\n",
    "val_accuracy = 0\n",
    "hidden_states_list = []  # ÌèâÍ∞ÄÌï† Îïå hidden stateÎ•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n",
    "for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0],\n",
    "              'attention_mask': batch[1],\n",
    "              'labels': batch[2]}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs[0]  # logitsÍ∞Ä outputsÏùò Ï≤´ Î≤àÏß∏ Í∞íÏûÖÎãàÎã§.\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = inputs['labels'].cpu().numpy()\n",
    "    val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "    # hidden stateÎ•º Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "    hidden_states = outputs[2]\n",
    "    hidden_states_list.append(hidden_states)\n",
    "hidden_states_concat = torch.cat(hidden_states_list, dim=0)\n",
    "hidden_states_concat = hidden_states_concat[:, 0, :].cpu().detach().numpy()\n",
    "hidden_states_df = pd.DataFrame(hidden_states_concat)\n",
    "hidden_states_df.to_csv(\"Dictionary_smashed_data_layer24.csv\", index=False)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy / len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef0b701f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MCC\\anaconda3\\envs\\biotf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Validation Accuracy: 0.99609375\n"
     ]
    }
   ],
   "source": [
    "# smashed data ÏÉùÏÑ± (100/client side) #ÎùºÏù¥Î∏åÎü¨Î¶¨ Í∞úÎ≥Ä\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_hidden_states=True\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[12]  # nÎ≤àÏß∏ Î†àÏù¥Ïñ¥Ïùò hidden statesÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n",
    "        #for j in range(len(hidden_states)):  # hidden_statesÎäî 768Ï∞®ÏõêÏúºÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ÏûàÏùå\n",
    "        #    noise = np.random.normal(0, 10.0)  # ÌëúÏ§Ä Ï†ïÍ∑ú Î∂ÑÌè¨ÏóêÏÑú Ï†ÅÏ†àÌïú Î∂ÑÏÇ∞Í∞íÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎûúÎç§Ìïú ÎÖ∏Ïù¥Ï¶à ÏÉùÏÑ±\n",
    "        #    hidden_states[j] += noise  # hidden_statesÏùò Í∞íÏóê ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "data_A = pd.read_csv(\"random_300.csv\")  # data set A ÌååÏùºÎ™ÖÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "data_B = pd.read_csv(\"infected.csv\")  # data set B ÌååÏùºÎ™ÖÏóê ÎßûÍ≤å ÏàòÏ†ï\n",
    "# Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "model_path = \"Fine_tuned_epoch20_BERT_Large.pt\"\n",
    "\n",
    "# X_train, Y_train ÏÉùÏÑ±\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():  # Ï§ëÎ≥µ Ï†úÍ±∞Î•º ÌïòÏßÄ ÏïäÍ≥† ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©\n",
    "    patient_id = row[\"ID\"]\n",
    "    patient_info = [str(row[column]) for column in data_A.columns if column != \"ID\" and column != \"DESCRIPTION\"]\n",
    "    symptoms = \", \".join(data_A[data_A[\"ID\"] == patient_id][\"DESCRIPTION\"].tolist())\n",
    "    combined_info = \", \".join(patient_info) + \", \" + symptoms\n",
    "    X_train.append(combined_info)\n",
    "    if patient_id in data_B.values:\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "#print(\"X_train\\n\", X_train[:10])\n",
    "#print(\"Y_train\\n\", Y_train[:10])\n",
    "        \n",
    "# BERT ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Î™®Îç∏ Î°úÎìú\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "# Î™®Îç∏Ïù¥ Ïù¥ÎØ∏ Ï†ÄÏû•ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥†, Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏûàÏúºÎ©¥ Î∂àÎü¨Ïò§Í≥† ÏóÜÏúºÎ©¥ ÏÉàÎ°úÏö¥ Î™®Îç∏ ÏÉùÏÑ±\n",
    "if os.path.exists(model_path):\n",
    "    # Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏûàÏùÑ Í≤ΩÏö∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path), strict=False)\n",
    "    print(\"Pre-train model loaded.\")\n",
    "else:\n",
    "    # Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏóÜÏùÑ Í≤ΩÏö∞ ÏÉàÎ°úÏö¥ Î™®Îç∏ ÏÉùÏÑ±\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î•º BERTÏùò ÏûÖÎ†• ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "max_len = 128  # ÏûÖÎ†• ÏãúÌÄÄÏä§Ïùò ÏµúÎåÄ Í∏∏Ïù¥\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        info,                         # ÌôòÏûê Ï†ïÎ≥¥ Î∞è Ï¶ùÏÉÅ\n",
    "                        add_special_tokens = True,    # [CLS], [SEP] ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä\n",
    "                        max_length = max_len,         # ÏµúÎåÄ Í∏∏Ïù¥ ÏßÄÏ†ï\n",
    "                        pad_to_max_length = True,     # Ìå®Îî©ÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ ÏµúÎåÄ Í∏∏Ïù¥Î°ú ÎßûÏ∂§\n",
    "                        return_attention_mask = True, # Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±\n",
    "                        return_tensors = 'pt',        # PyTorch ÌÖêÏÑúÎ°ú Î∞òÌôò\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# GPU ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Î™®Îç∏ÏùÑ GPUÎ°ú Ïù¥Îèô\n",
    "model.to(device)\n",
    "\n",
    "# Î™®Îç∏ ÌèâÍ∞Ä\n",
    "model.eval()\n",
    "val_accuracy = 0\n",
    "hidden_states_list = []  # ÌèâÍ∞ÄÌï† Îïå hidden stateÎ•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n",
    "for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0],\n",
    "              'attention_mask': batch[1],\n",
    "              'labels': batch[2]}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs[0]  # logitsÍ∞Ä outputsÏùò Ï≤´ Î≤àÏß∏ Í∞íÏûÖÎãàÎã§.\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = inputs['labels'].cpu().numpy()\n",
    "    val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "    # hidden stateÎ•º Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "    hidden_states = outputs[2]\n",
    "    hidden_states_list.append(hidden_states)\n",
    "hidden_states_concat = torch.cat(hidden_states_list, dim=0)\n",
    "hidden_states_concat = hidden_states_concat[:, 0, :].cpu().detach().numpy()\n",
    "hidden_states_df = pd.DataFrame(hidden_states_concat)\n",
    "hidden_states_df.to_csv(\"Client_smashed_data_layer24.csv\", index=False)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy / len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "423f678a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For file: Client_smashed_data_layer24.csv\n",
      "Accuracy: 0.774\n",
      "Successful Mean Distance: 4.1270358728788334\n",
      "Unsuccessful Mean Distance: 4.406488580300589\n",
      "Successful Distance Variance: 0.12063768643880003\n",
      "Unsuccessful Distance Variance: 0.15130207213235836\n",
      "Success Indices: [(1, 1), (2, 1), (3, 2), (4, 1), (7, 1), (8, 1), (9, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 2), (18, 1), (19, 1), (20, 4), (21, 1), (22, 1), (25, 2), (27, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (37, 1), (38, 1), (40, 5), (41, 1), (42, 1), (43, 1), (44, 1), (45, 4), (46, 1), (47, 1), (48, 1), (49, 3), (50, 1), (53, 1), (54, 1), (55, 4), (56, 1), (58, 1), (59, 1), (61, 1), (63, 1), (65, 3), (66, 1), (67, 1), (68, 4), (70, 1), (72, 1), (74, 1), (75, 1), (76, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (87, 2), (89, 1), (91, 1), (92, 1), (93, 1), (94, 2), (95, 1), (97, 1), (98, 1), (99, 1), (100, 2), (101, 1), (102, 1), (103, 3), (104, 1), (108, 2), (109, 1), (110, 3), (111, 1), (112, 1), (114, 1), (115, 1), (117, 4), (118, 1), (119, 1), (121, 1), (123, 1), (126, 1), (127, 1), (128, 1), (129, 4), (130, 1), (131, 1), (132, 1), (133, 1), (134, 3), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (146, 1), (147, 1), (148, 4), (150, 4), (151, 1), (153, 1), (154, 2), (155, 5), (157, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 3), (166, 1), (167, 1), (168, 3), (169, 1), (170, 2), (171, 1), (172, 1), (174, 1), (175, 2), (176, 1), (179, 1), (180, 4), (181, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (190, 2), (192, 1), (193, 4), (194, 1), (195, 3), (196, 1), (197, 2), (198, 4), (199, 1), (200, 3), (201, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 2), (210, 2), (211, 3), (212, 1), (213, 4), (214, 1), (215, 5), (216, 4), (217, 2), (218, 2), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (225, 1), (227, 1), (228, 1), (230, 1), (232, 1), (233, 2), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (241, 1), (242, 2), (243, 1), (244, 1), (245, 1), (246, 1), (247, 3), (248, 1), (249, 2), (250, 1), (251, 1), (253, 1), (254, 1), (256, 2), (258, 2), (259, 1), (260, 1), (261, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 3), (269, 1), (270, 1), (271, 1), (272, 1), (274, 5), (275, 1), (276, 1), (277, 1), (279, 3), (280, 1), (281, 1), (282, 1), (283, 1), (284, 4), (285, 1), (286, 1), (287, 1), (289, 5), (290, 1), (291, 1), (292, 1), (294, 3), (296, 1), (297, 1), (299, 1), (300, 1), (301, 1), (302, 2), (303, 1), (305, 1), (306, 1), (307, 1), (308, 1), (310, 1), (311, 1), (312, 1), (314, 1), (316, 1), (317, 5), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (325, 1), (326, 1), (328, 1), (329, 1), (330, 5), (331, 1), (333, 1), (335, 1), (337, 1), (338, 1), (339, 2), (340, 1), (341, 4), (343, 1), (344, 1), (345, 1), (346, 1), (349, 1), (350, 2), (351, 4), (352, 1), (356, 3), (357, 1), (358, 1), (359, 5), (360, 1), (361, 2), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 2), (370, 1), (371, 1), (372, 1), (373, 1), (375, 1), (376, 1), (377, 1), (378, 1), (379, 1), (381, 1), (382, 1), (383, 1), (384, 1), (386, 1), (388, 2), (390, 3), (391, 2), (392, 1), (393, 1), (394, 1), (397, 2), (399, 1), (401, 2), (402, 1), (404, 1), (405, 1), (406, 4), (407, 1), (408, 1), (410, 1), (411, 1), (412, 1), (413, 1), (414, 1), (417, 2), (418, 1), (419, 1), (420, 2), (421, 4), (423, 1), (425, 1), (427, 5), (428, 4), (429, 1), (430, 1), (431, 2), (433, 1), (434, 1), (435, 1), (436, 1), (437, 1), (438, 1), (439, 1), (440, 1), (441, 1), (442, 2), (443, 1), (444, 1), (445, 1), (446, 1), (447, 1), (450, 1), (451, 1), (452, 1), (453, 1), (455, 1), (456, 1), (457, 1), (458, 1), (459, 1), (460, 1), (461, 1), (463, 1), (464, 1), (465, 1), (466, 2), (468, 1), (469, 1), (470, 1), (471, 1), (472, 1), (477, 1), (478, 1), (479, 4), (481, 1), (482, 1), (483, 1), (484, 1), (486, 1), (488, 1), (491, 1), (492, 2), (493, 4), (494, 1), (496, 1), (497, 5), (498, 1), (499, 1), (500, 1)]\n",
      "Success Ranks Count:\n",
      "Rank 1: 303 successes\n",
      "Rank 2: 37 successes\n",
      "Rank 3: 16 successes\n",
      "Rank 4: 21 successes\n",
      "Rank 5: 10 successes\n"
     ]
    }
   ],
   "source": [
    "# Ïú†ÌÅ¥Î¶¨Îìú Í±∞Î¶¨ Ïú†ÏÇ¨ÎèÑ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def calculate_accuracy_and_distance(client_file, dictionary_file, original_file_client, original_file_dictionary, n=5):\n",
    "    # Î≥ÄÌôòÎêú ÌååÏùºÏùÑ ÏùΩÏñ¥ÏòµÎãàÎã§.\n",
    "    client_data = pd.read_csv(client_file)\n",
    "    dictionary_data = pd.read_csv(dictionary_file)\n",
    "    \n",
    "    # ÏõêÎ≥∏ ÌååÏùºÏùÑ ÏùΩÏñ¥ÏòµÎãàÎã§.\n",
    "    original_client_data = pd.read_csv(original_file_client)\n",
    "    original_dictionary_data = pd.read_csv(original_file_dictionary)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏ Í∞ÑÏùò Ïú†ÌÅ¥Î¶¨Îìú Í±∞Î¶¨Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n",
    "    distances = euclidean_distances(client_data.values, dictionary_data.values)\n",
    "    # Top@n Ïú†ÏÇ¨ÎèÑÎ•º Ï∞æÏäµÎãàÎã§.\n",
    "    topn_similarities = np.argsort(distances, axis=1)[:, :n]\n",
    "    topn_values = np.sort(distances, axis=1)[:, :n]\n",
    "    \n",
    "    # Î™®Îì† Í≤∞Í≥ºÎ•º Ï∂úÎ†•ÌïòÍ≥† Ï†ïÌôïÎèÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n",
    "    successful_distances = []\n",
    "    unsuccessful_distances = []\n",
    "    successes = 0\n",
    "    success_indices = []  # ÏÑ±Í≥µÌïú Ïù∏Îç±Ïä§Î•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n",
    "    \n",
    "    success_ranks_count = {rank: 0 for rank in range(1, n+1)}  # Í∞Å ÏÑ±Í≥µÌïú ÏÑúÎ≤Ñ Ï∏° Îû≠ÌÅ¨Ïùò ÏàòÎ•º Ï†ÄÏû•Ìï† ÎîïÏÖîÎÑàÎ¶¨\n",
    "    for i, (indices, scores) in enumerate(zip(topn_similarities, topn_values)):\n",
    "        \"\"\"print(f\"\\nTop {n} inferences for client {i + 1}:\")\"\"\"\n",
    "        for rank, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "            \"\"\"print(f\"Server {idx + 1} with distance {score}\")\"\"\"\n",
    "            if original_client_data.iloc[i].equals(original_dictionary_data.iloc[idx]):\n",
    "                successes += 1\n",
    "                successful_distances.append(score)\n",
    "                success_indices.append((i + 1, rank))  # ÏÑ±Í≥µÌïú Ïù∏Îç±Ïä§Î•º Ï∂îÍ∞Ä\n",
    "                success_ranks_count[rank] += 1  # Ìï¥Îãπ Îû≠ÌÅ¨Ïùò ÏàòÎ•º Ï¶ùÍ∞ÄÏãúÌÇ¥\n",
    "            else:\n",
    "                unsuccessful_distances.append(score)\n",
    "        if successes == 0:\n",
    "            print(\"No successful match found.\")\n",
    "    \n",
    "    # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞\n",
    "    accuracy = successes / len(client_data)\n",
    "    \n",
    "    # ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏùºÏπòÌïòÎäî Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏ÏôÄ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏, Í∑∏Î¶¨Í≥† ÏùºÏπòÌïòÏßÄ ÏïäÎäî Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏ÏôÄ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏ Í∞ÑÏùò ÌèâÍ∑† Í±∞Î¶¨Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n",
    "    successful_mean_distance = np.mean(successful_distances)\n",
    "    unsuccessful_mean_distance = np.mean(unsuccessful_distances)\n",
    "    \n",
    "    # ÌèâÍ∑† Í±∞Î¶¨Ïùò Î∂ÑÏÇ∞ Í≥ÑÏÇ∞\n",
    "    successful_distance_variance = np.var(successful_distances)\n",
    "    unsuccessful_distance_variance = np.var(unsuccessful_distances)\n",
    "    \n",
    "    return accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count\n",
    "\n",
    "# Î≥ÄÌôòÎêú ÌååÏùº Í≤ΩÎ°ú\n",
    "dictionary_file = \"Dictionary_smashed_data_layer24.csv\"\n",
    "\n",
    "# ÏõêÎ≥∏ ÌååÏùº Í≤ΩÎ°ú\n",
    "original_file_client = \"random_300.csv\"\n",
    "original_file_dictionary = \"random_500.csv\"\n",
    "\n",
    "# Top n ÏÑ§Ï†ï\n",
    "n = 5\n",
    "\n",
    "# Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞ Î∞è ÌèâÍ∑† Í±∞Î¶¨ Í≥ÑÏÇ∞\n",
    "\n",
    "client_file = f'Client_smashed_data_layer24.csv'\n",
    "accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count = calculate_accuracy_and_distance(client_file, dictionary_file, original_file_client, original_file_dictionary, n)\n",
    "\n",
    "print(\"\\nFor file:\", client_file)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Successful Mean Distance:\", successful_mean_distance)\n",
    "print(\"Unsuccessful Mean Distance:\", unsuccessful_mean_distance)\n",
    "\n",
    "# Î∂ÑÏÇ∞ Ï∂úÎ†•\n",
    "print(\"Successful Distance Variance:\", successful_distance_variance)\n",
    "print(\"Unsuccessful Distance Variance:\", unsuccessful_distance_variance)\n",
    "\n",
    "# ÏÑ±Í≥µÌïú Ïù∏Îç±Ïä§Îì§ÏùÑ Ï∂úÎ†•Ìï©ÎãàÎã§.\n",
    "print(\"Success Indices:\", success_indices)\n",
    "\n",
    "# Í∞Å ÏÑ±Í≥µÌïú ÏÑúÎ≤Ñ Ï∏° Îû≠ÌÅ¨Ïùò ÏàòÎ•º Ï∂úÎ†•Ìï©ÎãàÎã§.\n",
    "print(\"Success Ranks Count:\")\n",
    "for rank, count in success_ranks_count.items():\n",
    "    print(f\"Rank {rank}: {count} successes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005ef4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
