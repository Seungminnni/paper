#!/usr/bin/env python3
"""
Data Matching Performance Analysis
(Restored) Client Smashed Data vs (Dictionary) Server Smashed Data
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt

def load_smashed_data(file_path):
    """Smashed data CSV 파일 로드"""
    df = pd.read_csv(file_path)
    vectors = df.values
    print(f"📊 Loaded {vectors.shape[0]} vectors of dimension {vectors.shape[1]} from {file_path}")
    return vectors

def calculate_best_match_similarity(client_vectors, server_vectors):
    """클라이언트 벡터 각각에 대해 서버에서 가장 유사한 벡터를 찾아 유사도 통계 계산"""
    print("\n🔍 Calculating best-match similarity statistics...")

    similarities = []

    # 각 클라이언트 벡터에 대해 가장 유사한 서버 벡터 찾기
    for client_vec in client_vectors:
        sim_scores = cosine_similarity([client_vec], server_vectors)[0]
        max_sim = np.max(sim_scores)
        similarities.append(max_sim)

    similarities = np.array(similarities)

    print("📈 Best-Match Similarity Statistics:")
    print(f"   • Mean best-match similarity: {np.mean(similarities):.4f}")
    print(f"   • Median best-match similarity: {np.median(similarities):.4f}")
    print(f"   • Min best-match similarity: {np.min(similarities):.4f}")
    print(f"   • Max best-match similarity: {np.max(similarities):.4f}")

    return similarities

def plot_similarity_distribution(similarities):
    """유사도 분포 시각화"""
    plt.figure(figsize=(10, 6))
    plt.hist(similarities, bins=30, alpha=0.7, color='blue', edgecolor='black')
    plt.title('Distribution of Best-Match Cosine Similarities')
    plt.xlabel('Cosine Similarity')
    plt.ylabel('Frequency')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('data_matching_distribution.png', dpi=300)
    print(f"\n✅ Plot saved as 'data_matching_distribution.png'")
    plt.close()

def main():
    print("🔬 Data Matching Performance Analysis")
    print("=" * 60)

    # 데이터 로드
    client_file = "restored_client_vectors.csv"
    server_file = "Dictionary_smashed_data_layer2.csv"

    try:
        client_vectors = load_smashed_data(client_file)
        server_vectors = load_smashed_data(server_file)
    except FileNotFoundError as e:
        print(f"❌ Error: {e}")
        print("Please ensure all necessary files are generated by the pipeline first.")
        return

    # 최고 매칭 유사도 분석
    similarities = calculate_best_match_similarity(client_vectors, server_vectors)

    if similarities is not None:
        # 시각화
        plot_similarity_distribution(similarities)

        # 결과 요약
        print("\n🎉 Analysis Complete!")
        print("=" * 60)
        print("📋 Summary of Data Matching Performance:")
        print(f"   • Client samples processed: {len(client_vectors)}")
        print(f"   • Server dictionary size: {len(server_vectors)}")
        print(f"   • Average of best-match similarities: {np.mean(similarities):.4f}")

        # 해석
        print("\n💡 Interpretation:")
        mean_sim = np.mean(similarities)
        if mean_sim > 0.9:
            print("   • High Confidence: Restored client vectors can be matched to the server dictionary with high confidence.")
        elif mean_sim > 0.7:
            print("   • Moderate Confidence: Matching is possible, but with some ambiguity.")
        else:
            print("   • Low Confidence: The pipeline significantly impacts matching performance.")

if __name__ == "__main__":
    main()
