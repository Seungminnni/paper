# ìœ ê¶Œìž ë°ì´í„° smashed data ìœ ì‚¬ë„ ê³„ì‚° - ìˆœí™˜ ì€í êµ¬ì¡° ì ìš©
# ì—°êµ¬ ì•„ì´ë””ì–´: ì€íëœ ë²¡í„° ê°„ ìœ ì‚¬ë„ ë¶„ì„ìœ¼ë¡œ ë³´ì•ˆ íš¨ê³¼ ê²€ì¦
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimilarityAnalyzer:
    """
    ìˆœí™˜ ì€í êµ¬ì¡°ì˜ smashed data ìœ ì‚¬ë„ ë¶„ì„
    ê³µê²©ìžê°€ ë²¡í„°ë¥¼ íƒˆì·¨í•˜ë”ë¼ë„ ì˜ë¯¸ ìžˆëŠ” íŒ¨í„´ì„ ì°¾ê¸° ì–´ë ¤ì›€ ê²€ì¦
    """
    def __init__(self):
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        
    def analyze_obfuscation_effectiveness(self, client_vectors, server_vectors):
        """
        ì€í íš¨ê³¼ ë¶„ì„: ì›ë³¸ vs ì€íëœ ë²¡í„°ì˜ ì°¨ì´ ë¶„ì„
        """
        # ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = 1 / (1 + euclidean_distances(client_vectors, server_vectors))
        
        # í†µê³„ ë¶„ì„
        mean_similarity = np.mean(similarities)
        std_similarity = np.std(similarities)
        max_similarity = np.max(similarities)
        min_similarity = np.min(similarities)
        
        return {
            'mean_similarity': mean_similarity,
            'std_similarity': std_similarity,
            'max_similarity': max_similarity,
            'min_similarity': min_similarity,
            'similarity_distribution': similarities
        }
    
    def detect_anomalies(self, vectors, threshold=0.1):
        """
        ì´ìƒì¹˜ íƒì§€: ì€íëœ ë²¡í„°ì˜ ë¹„ì •ìƒ íŒ¨í„´ íƒì§€
        ê³µê²©ìžê°€ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ì§€ ê²€ì¦
        """
        # ë²¡í„° ì •ê·œí™”
        normalized_vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cosine_sim = np.dot(normalized_vectors, normalized_vectors.T)
        
        # ìžê¸° ìžì‹ ê³¼ì˜ ìœ ì‚¬ë„ëŠ” 1ì´ë¯€ë¡œ ì œì™¸
        np.fill_diagonal(cosine_sim, 0)
        
        # ìž„ê³„ê°’ ì´ìƒì˜ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ì´ìƒì¹˜ íƒì§€
        anomalies = []
        for i in range(len(cosine_sim)):
            max_sim = np.max(cosine_sim[i])
            if max_sim > threshold:
                anomalies.append((i, max_sim))
        
        return anomalies, cosine_sim

def calculate_circular_obfuscation_similarity(client_file, dictionary_file, original_client_data, original_dictionary_data, n=5):
    """
    ìˆœí™˜ ì€í êµ¬ì¡°ì˜ smashed data ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    ê¸°ì¡´ ìœ ì‚¬ë„ ê³„ì‚°ì— ì€í íš¨ê³¼ ë¶„ì„ ì¶”ê°€
    """
    # ë³€í™˜ëœ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.
    client_data = pd.read_csv(client_file)
    dictionary_data = pd.read_csv(dictionary_file)
    
    # ìˆœí™˜ ì€í ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = SimilarityAnalyzer()
    
    # ê¸°ë³¸ ìœ ì‚¬ë„ ê³„ì‚° (ê¸°ì¡´ ë°©ì‹)
    distances = euclidean_distances(client_data.values, dictionary_data.values)
    topn_similarities = np.argsort(distances, axis=1)[:, :n]
    topn_values = np.sort(distances, axis=1)[:, :n]
    
    # ì€í íš¨ê³¼ ë¶„ì„
    obfuscation_analysis = analyzer.analyze_obfuscation_effectiveness(
        client_data.values, dictionary_data.values
    )
    
    # ì´ìƒì¹˜ íƒì§€
    client_anomalies, client_similarity_matrix = analyzer.detect_anomalies(
        client_data.values, threshold=0.1
    )
    server_anomalies, server_similarity_matrix = analyzer.detect_anomalies(
        dictionary_data.values, threshold=0.1
    )
    
    # ëª¨ë“  ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê³  ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    successful_distances = []
    unsuccessful_distances = []
    successes = 0
    success_indices = []  # ì„±ê³µí•œ ì¸ë±ìŠ¤ë¥¼ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸
    success_ranks_count = {rank: 0 for rank in range(1, n+1)}  # ê° ì„±ê³µí•œ ì„œë²„ ì¸¡ ëž­í¬ì˜ ìˆ˜ë¥¼ ì €ìž¥í•  ë”•ì…”ë„ˆë¦¬

    for i, (indices, scores) in enumerate(zip(topn_similarities, topn_values)):
        for rank, (idx, score) in enumerate(zip(indices, scores), 1):
            # ì›ë³¸ ë°ì´í„°ì˜ voter_idë¡œ ë¹„êµ
            client_voter_id = original_client_data.iloc[i]["voter_id"]
            dictionary_voter_id = original_dictionary_data.iloc[idx]["voter_id"]

            if client_voter_id == dictionary_voter_id:
                successes += 1
                successful_distances.append(score)
                success_indices.append((i + 1, rank))  # ì„±ê³µí•œ ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€
                success_ranks_count[rank] += 1  # í•´ë‹¹ ëž­í¬ì˜ ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚´
                break  # ê°™ì€ voter_idë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë” ì´ìƒ í™•ì¸í•˜ì§€ ì•ŠìŒ
            else:
                unsuccessful_distances.append(score)

    # ì •í™•ë„ ê³„ì‚°
    accuracy = successes / len(client_data)

    # ì„±ê³µì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ í‰ê·  ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    successful_mean_distance = np.mean(successful_distances) if successful_distances else 0
    unsuccessful_mean_distance = np.mean(unsuccessful_distances) if unsuccessful_distances else 0

    # í‰ê·  ê±°ë¦¬ì˜ ë¶„ì‚° ê³„ì‚°
    successful_distance_variance = np.var(successful_distances) if successful_distances else 0
    unsuccessful_distance_variance = np.var(unsuccessful_distances) if unsuccessful_distances else 0

    return {
        'accuracy': accuracy,
        'successful_mean_distance': successful_mean_distance,
        'unsuccessful_mean_distance': unsuccessful_mean_distance,
        'success_indices': success_indices,
        'successful_distance_variance': successful_distance_variance,
        'unsuccessful_distance_variance': unsuccessful_distance_variance,
        'success_ranks_count': success_ranks_count,
        'obfuscation_analysis': obfuscation_analysis,
        'client_anomalies': client_anomalies,
        'server_anomalies': server_anomalies,
        'client_similarity_matrix': client_similarity_matrix,
        'server_similarity_matrix': server_similarity_matrix
    }
    """
    ìœ ê¶Œìž ë°ì´í„°ì˜ smashed data ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜

    Parameters:
    - client_file: í´ë¼ì´ì–¸íŠ¸ ì¸¡ smashed data íŒŒì¼ ê²½ë¡œ
    - dictionary_file: ì„œë²„ ì¸¡ smashed data íŒŒì¼ ê²½ë¡œ
    - original_client_data: ì›ë³¸ í´ë¼ì´ì–¸íŠ¸ ë°ì´í„° (DataFrame)
    - original_dictionary_data: ì›ë³¸ ì„œë²„ ë°ì´í„° (DataFrame)
    - n: Top-N ìœ ì‚¬ë„ ê³„ì‚° ê°œìˆ˜
    """
    # ë³€í™˜ëœ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.
    client_data = pd.read_csv(client_file)
    dictionary_data = pd.read_csv(dictionary_file)

    # ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    distances = euclidean_distances(client_data.values, dictionary_data.values)

    # Top@n ìœ ì‚¬ë„ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    topn_similarities = np.argsort(distances, axis=1)[:, :n]
    topn_values = np.sort(distances, axis=1)[:, :n]

    # ëª¨ë“  ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê³  ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    successful_distances = []
    unsuccessful_distances = []
    successes = 0
    success_indices = []  # ì„±ê³µí•œ ì¸ë±ìŠ¤ë¥¼ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸
    success_ranks_count = {rank: 0 for rank in range(1, n+1)}  # ê° ì„±ê³µí•œ ì„œë²„ ì¸¡ ëž­í¬ì˜ ìˆ˜ë¥¼ ì €ìž¥í•  ë”•ì…”ë„ˆë¦¬

    for i, (indices, scores) in enumerate(zip(topn_similarities, topn_values)):
        for rank, (idx, score) in enumerate(zip(indices, scores), 1):
            # ì›ë³¸ ë°ì´í„°ì˜ voter_idë¡œ ë¹„êµ
            client_voter_id = original_client_data.iloc[i]["voter_id"]
            dictionary_voter_id = original_dictionary_data.iloc[idx]["voter_id"]

            if client_voter_id == dictionary_voter_id:
                successes += 1
                successful_distances.append(score)
                success_indices.append((i + 1, rank))  # ì„±ê³µí•œ ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€
                success_ranks_count[rank] += 1  # í•´ë‹¹ ëž­í¬ì˜ ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚´
                break  # ê°™ì€ voter_idë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë” ì´ìƒ í™•ì¸í•˜ì§€ ì•ŠìŒ
            else:
                unsuccessful_distances.append(score)

    # ì •í™•ë„ ê³„ì‚°
    accuracy = successes / len(client_data)

    # ì„±ê³µì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ í‰ê·  ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    successful_mean_distance = np.mean(successful_distances) if successful_distances else 0
    unsuccessful_mean_distance = np.mean(unsuccessful_distances) if unsuccessful_distances else 0

    # í‰ê·  ê±°ë¦¬ì˜ ë¶„ì‚° ê³„ì‚°
    successful_distance_variance = np.var(successful_distances) if successful_distances else 0
    unsuccessful_distance_variance = np.var(unsuccessful_distances) if unsuccessful_distances else 0

    return accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    print("Loading voter data for similarity calculation...")

    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
    full_data = pd.read_csv("ncvoterb.csv", encoding='latin-1')

    # ë°ì´í„° í¬ê¸° ì œí•œ: ì‹¤í—˜ì„ ìœ„í•´ 20,000ê°œë¡œ ì œí•œ
    # ì „ì²´ ë°ì´í„°: ì•½ 224,061ê°œ â†’ ì‹¤í—˜ìš©: 20,000ê°œ (ì•½ 8.9% ì‚¬ìš©)
    SAMPLE_SIZE = 20000
    if len(full_data) > SAMPLE_SIZE:
        print(f"ðŸ“Š Reducing data size from {len(full_data):,} to {SAMPLE_SIZE:,} for faster experimentation")
        full_data = full_data.sample(n=SAMPLE_SIZE, random_state=42)
        print(f"âœ… Data reduced successfully! Working with {len(full_data):,} records")

    print(f"âœ… Data loaded successfully! Total records: {len(full_data)}")

    # ì„œë²„ì¸¡ê³¼ í´ë¼ì´ì–¸íŠ¸ ì¸¡ ë°ì´í„° ë¶„ë¦¬ (ì´ì „ê³¼ ë™ì¼í•œ ë°©ì‹)
    # ì‹¤í—˜ìš© ë°ì´í„°ì—ì„œ 70% ì„œë²„, 30% í´ë¼ì´ì–¸íŠ¸ë¡œ ë¶„ë¦¬
    server_sample_size = int(len(full_data) * 0.7)
    server_data = full_data.sample(n=server_sample_size, random_state=42)
    client_data = full_data.drop(server_data.index).sample(frac=1.0, random_state=123)  # ë‚˜ë¨¸ì§€ ë°ì´í„° ì‚¬ìš©

    print(f"ðŸ“Š Server data size: {len(server_data)} (70% of {len(full_data)} = {len(server_data):,})")
    print(f"ðŸ“Š Client data size: {len(client_data)} (30% of {len(full_data)} = {len(client_data):,})")

    # ë³€í™˜ëœ íŒŒì¼ ê²½ë¡œ
    dictionary_file = "Dictionary_smashed_data.csv"
    client_file = "Client_smashed_data.csv"

    # Top n ì„¤ì •
    n = 5

    # íŒŒì¼ ì¡´ìž¬ í™•ì¸
    import os
    if not os.path.exists(client_file) or not os.path.exists(dictionary_file):
        print(f"Error: Required files not found!")
        print(f"Client file: {client_file} - {'Found' if os.path.exists(client_file) else 'Not found'}")
        print(f"Dictionary file: {dictionary_file} - {'Found' if os.path.exists(dictionary_file) else 'Not found'}")
        exit(1)

    # ìœ ì‚¬ë„ ê³„ì‚° (ìˆœí™˜ ì€í êµ¬ì¡° ì ìš©)
    print("ðŸ”„ Calculating similarity with circular obfuscation analysis...")
    print("   ðŸ“‹ Analysis includes: Basic similarity + Obfuscation effectiveness + Anomaly detection")
    
    results = calculate_circular_obfuscation_similarity(
        client_file, dictionary_file, client_data, server_data, n
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("CIRCULAR OBFUSCATION SMASHED DATA SIMILARITY RESULTS")
    print("="*70)
    print(f"ðŸŽ¯ Basic Accuracy: {results['accuracy']:.4f}")
    print(f"ðŸ“Š Successful Mean Distance: {results['successful_mean_distance']:.4f}")
    print(f"ðŸ“Š Unsuccessful Mean Distance: {results['unsuccessful_mean_distance']:.4f}")
    print(f"ðŸ“ˆ Successful Distance Variance: {results['successful_distance_variance']:.4f}")
    print(f"ðŸ“ˆ Unsuccessful Distance Variance: {results['unsuccessful_distance_variance']:.4f}")
    
    # ì€í íš¨ê³¼ ë¶„ì„ ê²°ê³¼
    print(f"\nðŸ›¡ï¸ OBFUSCATION EFFECTIVENESS ANALYSIS")
    print("-" * 40)
    obf = results['obfuscation_analysis']
    print(f"ðŸ“Š Mean Similarity: {obf['mean_similarity']:.4f}")
    print(f"ðŸ“Š Similarity Std Dev: {obf['std_similarity']:.4f}")
    print(f"ðŸ“Š Max Similarity: {obf['max_similarity']:.4f}")
    print(f"ðŸ“Š Min Similarity: {obf['min_similarity']:.4f}")
    
    # ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼
    print(f"\nðŸš¨ ANOMALY DETECTION RESULTS")
    print("-" * 40)
    print(f"ðŸ“Š Client Anomalies: {len(results['client_anomalies'])} detected")
    print(f"ðŸ“Š Server Anomalies: {len(results['server_anomalies'])} detected")
    
    if results['client_anomalies']:
        print("   Top client anomalies:")
        for idx, sim in results['client_anomalies'][:5]:
            print(f"     Sample {idx}: similarity {sim:.4f}")
    
    print(f"\nðŸ† SUCCESS RANK ANALYSIS (Top {n})")
    print("-" * 40)
    print(f"   Matches found: {len(results['success_indices'])}")
    for idx, rank in results['success_indices'][:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
        print(f"   Client {idx} matched at rank {rank}")

    print(f"\nðŸ“‹ SUCCESS RANKS COUNT:")
    for rank, count in results['success_ranks_count'].items():
        print(f"   Rank {rank}: {count} successes")

    print("\nðŸŽ‰ Circular obfuscation similarity analysis completed!")
    print("="*70)
    
    # ë³´ì•ˆ íš¨ê³¼ ìš”ì•½
    print("\nðŸ”’ SECURITY EFFECTIVENESS SUMMARY")
    print("-" * 40)
    print(f"   â€¢ Obfuscation Strength: {'Strong' if obf['std_similarity'] > 0.1 else 'Moderate'}")
    print(f"   â€¢ Anomaly Detection: {len(results['client_anomalies'] + results['server_anomalies'])} patterns found")
    print(f"   â€¢ Similarity Distribution: {obf['min_similarity']:.3f} - {obf['max_similarity']:.3f}")
    print(f"   â€¢ Attack Difficulty: {'High' if results['accuracy'] < 0.5 else 'Medium'}")
    print("="*70)
