#!/usr/bin/env python3
"""
Server-side Smashed Data Generation
ì„œë²„ ì‚¬ì´ë“œ Smashed Data ìƒì„± (500ê°œ ìƒ˜í”Œ)
"""

import os
import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
import torch

class CustomBertForSequenceClassification(BertForSequenceClassification):
    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        labels=None,
        output_hidden_states=True
    ):
        outputs = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            labels=labels,
            output_hidden_states=output_hidden_states
        )
        logits = outputs.logits
        hidden_states = outputs.hidden_states[-8]  # 8ë²ˆì§¸ ë ˆì´ì–´ì˜ hidden states
        loss = outputs.loss
        return logits, loss, hidden_states

def main():
    print("ğŸ–¥ï¸  Starting Server-side Smashed Data Generation")
    print("=" * 60)

    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    print("ğŸ“Š Loading data for server-side processing...")
    data_A = pd.read_csv("random_500.csv")  # 500ê°œ ìƒ˜í”Œ ë°ì´í„°ì…‹
    data_B = pd.read_csv("infected.csv")  # ê°ì—¼ ìƒíƒœ ë°ì´í„°ì…‹
    model_path = "Pre_train_final_BERT_Based.pt"

    print(f"ğŸ“ˆ Processing {len(data_A)} patient records for server-side")

    # X_train, Y_train ìƒì„±
    X_train = []
    Y_train = []

    for index, row in data_A.iterrows():
        patient_id = row["ID"]
        patient_info = [str(row[column]) for column in data_A.columns if column != "ID" and column != "DESCRIPTION"]
        symptoms = ", ".join(data_A[data_A["ID"] == patient_id]["DESCRIPTION"].tolist())
        combined_info = ", ".join(patient_info) + ", " + symptoms
        X_train.append(combined_info)
        if patient_id in data_B.values:
            Y_train.append(1)  # Infected
        else:
            Y_train.append(0)  # Not infected

    print(f"âœ… Processed {len(X_train)} patient records")
    print(f"ğŸ“‹ Sample combined info: {X_train[0][:100]}...")

    # BERT í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\nğŸ¤– Loading BERT tokenizer...")
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    # ëª¨ë¸ ë¡œë“œ
    print("ğŸ”§ Loading fine-tuned model...")
    if os.path.exists(model_path):
        model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
        model.load_state_dict(torch.load(model_path))
        print("âœ… Fine-tuned model loaded successfully")
    else:
        print("âŒ Fine-tuned model not found!")
        return

    # ì…ë ¥ ë°ì´í„° ë³€í™˜
    max_len = 128
    print(f"ğŸ”„ Tokenizing {len(X_train)} texts with max length {max_len}...")

    input_ids = []
    attention_masks = []

    for info in X_train:
        encoded_dict = tokenizer.encode_plus(
            info,
            add_special_tokens=True,
            max_length=max_len,
            padding=True,
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    labels = torch.tensor(Y_train)

    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
    dataset = TensorDataset(input_ids, attention_masks, labels)
    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)

    print(f"ğŸ“¦ Dataset created with {len(dataset)} samples")

    # GPU ì„¤ì •
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    model.to(device)
    model.eval()
    print(f"ğŸ–¥ï¸  Using device: {device}")

    # --- ì•„ì´ë””ì–´ ì ìš©: í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë°›ì€ ì´ë¯¸ì§€ ë°°ì—´ì„ ë²¡í„°ë¡œ ë³µì› ---
    import json

    def image_to_vector(img, original_dim, vmin, vmax):
        """
        ì ˆì°¨ì ìœ¼ë¡œ ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ ì›ë³¸ ë²¡í„°ë¡œ ë³µì›í•©ë‹ˆë‹¤.
        """
        img = np.asarray(img, dtype=np.float32)
        v_padded = img.flatten()
        v_scaled = v_padded[:original_dim]
        v_original = v_scaled * (vmax - vmin + 1e-8) + vmin
        return v_original

    # 1. í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° Smashed Image ë°°ì—´ì„ íŒŒì¼ì—ì„œ ë¡œë“œ
    image_file = "smashed_images.npy"
    config_file = "vector_image_config.json"

    print(f"â˜ï¸  Loading smashed images from '{image_file}'...")
    
    if not os.path.exists(image_file):
        print(f"âŒ Error: Image file not found at '{image_file}'!")
        print("Please run 'client_side.py' first to generate it.")
        return
        
    if not os.path.exists(config_file):
        print(f"âŒ Error: Config file not found at '{config_file}'!")
        print("Please run 'generate_config.py' first.")
        return

    smashed_images = np.load(image_file)
    print(f"-> Loaded a batch of {smashed_images.shape[0]} image arrays.")

    # 2. ê³µìœ  ì„¤ì • íŒŒì¼ ë¡œë“œ
    with open("vector_image_config.json", "r") as f:
        config = json.load(f)

    # 3. ì´ë¯¸ì§€ ë°°ì—´ì„ ë²¡í„°ë¡œ ë³µì› (ë°°ì¹˜ ì²˜ë¦¬)
    print("\nğŸ” Reconstructing vectors from image arrays...")
    reconstructed_vectors = []
    for image_array in smashed_images:
        vec = image_to_vector(
            image_array,
            original_dim=config["original_dim"],
            vmin=config["vmin"],
            vmax=config["vmax"]
        )
        reconstructed_vectors.append(vec)

    reconstructed_vectors = np.array(reconstructed_vectors, dtype=np.float32)

    print(f"âœ… Vectors reconstructed successfully.")
    print(f"   - Vector batch shape: {reconstructed_vectors.shape}")

    # 4. ë³µì›ëœ ë²¡í„°ë¥¼ ë‹¤ìŒ ë¶„ì„ì„ ìœ„í•´ íŒŒì¼ë¡œ ì €ì¥
    output_file = "restored_client_vectors.csv"
    pd.DataFrame(reconstructed_vectors).to_csv(output_file, index=False)
    print(f"\nğŸ’¾ Reconstructed vectors saved to '{output_file}' for analysis.")

    print("\nğŸ‰ Server-side process completed!")

if __name__ == "__main__":
    main()
