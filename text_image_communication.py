#!/usr/bin/env python3
"""
Text-Image Communication System
- Client: Text ‚Üí Image (Encoder)
- Server: Image ‚Üí Text (Decoder)
- Vector spaces are completely different but models can interpret each other
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Conv2D, Conv2DTranspose
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import random

# GPU setup
print("=== Text-Image Communication System ===")

try:
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"GPU ready: {len(gpus)} GPU(s)")
        gpu_available = True
    else:
        print("Using CPU")
        gpu_available = False
except Exception as e:
    print(f"GPU error: {e}")
    gpu_available = False

device_name = '/GPU:0' if gpu_available else '/CPU:0'
print(f"Device: {device_name}")

# =============================================================================
# Simple Message Dataset
# =============================================================================

def create_messages():
    """Create simple message dataset"""
    
    messages = [
        "hello", "world", "cat", "dog", "good", 
        "bad", "yes", "no", "start", "stop",
        "go", "come", "help", "thank", "sorry"
    ]
    
    vocab_size = len(messages)
    msg_to_idx = {msg: idx for idx, msg in enumerate(messages)}
    idx_to_msg = {idx: msg for msg, idx in msg_to_idx.items()}
    
    print(f"Messages: {vocab_size} words")
    print(f"Examples: {messages[:5]}")
    
    return messages, msg_to_idx, idx_to_msg, vocab_size

messages, msg_to_idx, idx_to_msg, vocab_size = create_messages()

# One-hot encoding
def text_to_onehot(texts, msg_to_idx, vocab_size):
    onehot = np.zeros((len(texts), vocab_size))
    for i, text in enumerate(texts):
        if text in msg_to_idx:
            onehot[i, msg_to_idx[text]] = 1
    return onehot

# Generate training data
train_texts = []
test_texts = []

# Each message repeated many times
for msg in messages:
    train_texts.extend([msg] * 50)  # 50 times each
    test_texts.extend([msg] * 10)   # 10 times each

random.shuffle(train_texts)
random.shuffle(test_texts)

train_onehot = text_to_onehot(train_texts, msg_to_idx, vocab_size)
test_onehot = text_to_onehot(test_texts, msg_to_idx, vocab_size)

print(f"Training data: {len(train_texts)} samples")
print(f"Test data: {len(test_texts)} samples")

# =============================================================================
# Model Parameters
# =============================================================================

LATENT_DIM = 32        # Shared latent space
IMAGE_SIZE = 28        # Image size 28x28

print(f"Architecture:")
print(f"  Vocab size: {vocab_size}")
print(f"  Latent dim: {LATENT_DIM}")
print(f"  Image size: {IMAGE_SIZE}x{IMAGE_SIZE}")

with tf.device(device_name):
    
    # =============================================================================
    # CLIENT SIDE: Text ‚Üí Image Model
    # =============================================================================
    print("Building CLIENT model (Text ‚Üí Image)...")
    
    # Text Encoder
    text_input = Input(shape=(vocab_size,), name='text_input')
    x = Dense(64, activation='relu')(text_input)
    x = Dense(LATENT_DIM, activation='relu')(x)
    text_encoded = Dense(LATENT_DIM, activation='sigmoid', name='text_latent')(x)
    
    text_encoder = Model(text_input, text_encoded, name='text_encoder')
    
    # Image Generator (from text latent)
    latent_input = Input(shape=(LATENT_DIM,), name='latent_input')
    x = Dense(7 * 7 * 16, activation='relu')(latent_input)
    x = Reshape((7, 7, 16))(x)
    x = Conv2DTranspose(16, (4, 4), strides=(2, 2), padding='same', activation='relu')(x)  # 14x14
    x = Conv2DTranspose(8, (4, 4), strides=(2, 2), padding='same', activation='relu')(x)   # 28x28
    generated_image = Conv2DTranspose(1, (3, 3), padding='same', activation='sigmoid')(x)
    
    image_generator = Model(latent_input, generated_image, name='image_generator')
    
    # Complete CLIENT model: Text ‚Üí Image
    client_model = Model(text_input, image_generator(text_encoder(text_input)), name='client_text_to_image')
    
    # =============================================================================
    # SERVER SIDE: Image ‚Üí Text Model  
    # =============================================================================
    print("Building SERVER model (Image ‚Üí Text)...")
    
    # Image Encoder
    image_input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name='image_input')
    x = Conv2D(8, (3, 3), activation='relu', padding='same')(image_input)
    x = Conv2D(16, (4, 4), strides=(2, 2), activation='relu', padding='same')(x)  # 14x14
    x = Conv2D(16, (4, 4), strides=(2, 2), activation='relu', padding='same')(x)  # 7x7
    x = Flatten()(x)
    x = Dense(64, activation='relu')(x)
    image_encoded = Dense(LATENT_DIM, activation='sigmoid', name='image_latent')(x)
    
    image_encoder = Model(image_input, image_encoded, name='image_encoder')
    
    # Text Decoder (from image latent)
    latent_to_text_input = Input(shape=(LATENT_DIM,), name='latent_to_text_input')
    x = Dense(64, activation='relu')(latent_to_text_input)
    x = Dense(vocab_size, activation='softmax')(x)
    
    text_decoder = Model(latent_to_text_input, x, name='text_decoder')
    
    # Complete SERVER model: Image ‚Üí Text
    server_model = Model(image_input, text_decoder(image_encoder(image_input)), name='server_image_to_text')

print("Model Summary:")
print("\nCLIENT - Text Encoder:")
text_encoder.summary()
print("\nCLIENT - Image Generator:")
image_generator.summary()
print("\nSERVER - Image Encoder:")
image_encoder.summary()
print("\nSERVER - Text Decoder:")
text_decoder.summary()

# =============================================================================
# Model Compilation
# =============================================================================
print("Compiling models...")

client_model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])
server_model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# =============================================================================
# Generate Random Target Images (No correlation with text content)
# =============================================================================
print("Generating completely random target images...")

def generate_consistent_target_images(texts, image_size=28):
    """Generate consistent images - same text always gets same image!
    This allows neural networks to learn stable arbitrary mappings.
    'hello' ‚Üí always same random pattern A
    'cat' ‚Üí always same random pattern B  
    """
    n_samples = len(texts)
    images = np.zeros((n_samples, image_size, image_size, 1), dtype='float32')
    
    # Create a unique fixed pattern for each unique word
    unique_patterns = {}
    
    for i, text in enumerate(texts):
        if text not in unique_patterns:
            # Generate a consistent pattern for this text using its hash as seed
            text_seed = abs(hash(text)) % 10000
            np.random.seed(text_seed)
            
            # Create unique pattern for this word
            pattern_type = text_seed % 3
            base_image = np.random.rand(image_size, image_size, 1).astype('float32')
            
            if pattern_type == 0:  # Circular pattern
                center_x, center_y = (text_seed % 15) + 6, ((text_seed // 15) % 15) + 6
                radius = (text_seed % 5) + 4
                y, x = np.ogrid[:image_size, :image_size]
                mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
                base_image[mask] = 0.8 + 0.2 * np.random.random()
                
            elif pattern_type == 1:  # Rectangular pattern
                x1, y1 = (text_seed % 10) + 2, ((text_seed // 10) % 10) + 2
                x2, y2 = x1 + ((text_seed % 8) + 6), y1 + (((text_seed // 8) % 8) + 6)
                x2, y2 = min(x2, image_size-2), min(y2, image_size-2)
                base_image[x1:x2, y1:y2] = 0.1 + 0.3 * np.random.random()
                
            else:  # Diagonal lines pattern
                for j in range(0, image_size, 3):
                    if j + (text_seed % 3) < image_size:
                        base_image[j + (text_seed % 3), :] = 0.6 + 0.4 * np.random.random()
                    if j + (text_seed % 3) < image_size:
                        base_image[:, j + (text_seed % 3)] = 0.3 + 0.2 * np.random.random()
            
            unique_patterns[text] = base_image
            print(f"üìù Created consistent pattern for '{text}' (seed: {text_seed})")
        
        # Use the consistent pattern for this text
        images[i] = unique_patterns[text].copy()
    
    print(f"Generated {n_samples} images with {len(unique_patterns)} unique consistent patterns")
    print("üí° Same text ‚Üí Same image (enabling stable learning!)")
    print("üí° Different text ‚Üí Different image (arbitrary but learnable mappings)")
    
    return images

# Generate target images for training
train_target_images = generate_consistent_target_images(train_texts)
test_target_images = generate_consistent_target_images(test_texts)

print(f"Target images generated: {train_target_images.shape}")

# =============================================================================
# Training Phase - Learning Arbitrary Mappings
# =============================================================================
print("Starting training - Learning arbitrary text ‚Üî image mappings...")
print("üß† PHASE 1: Teaching CLIENT to convert text to random images")
print("üß† PHASE 2: Teaching SERVER to convert those random images back to text")

with tf.device(device_name):
    
    print("\nPhase 1: Training CLIENT (Text ‚Üí Random Image)...")
    print("   Goal: 'hello' ‚Üí random pattern A, 'world' ‚Üí random pattern B, etc.")
    client_history = client_model.fit(
        train_onehot, train_target_images,
        epochs=15, batch_size=32,
        validation_data=(test_onehot, test_target_images),
        verbose=1
    )
    
    print("\nPhase 2: Training SERVER (Random Image ‚Üí Text)...")
    print("   Goal: random pattern A ‚Üí 'hello', random pattern B ‚Üí 'world', etc.")  
    server_history = server_model.fit(
        train_target_images, train_onehot,
        epochs=15, batch_size=32, 
        validation_data=(test_target_images, test_onehot),
        verbose=1
    )

print("Training completed!")
print("üéâ Networks learned to communicate through arbitrary image patterns!")

# =============================================================================
# Test Communication System
# =============================================================================
print("Testing communication system...")

# Test with diverse messages to see real learning
test_messages = ["hello", "world", "cat", "dog", "help"]
print(f"Test messages (diverse): {test_messages}")

# Prepare test data
test_indices = [msg_to_idx[msg] for msg in test_messages if msg in msg_to_idx]
test_input_onehot = np.zeros((len(test_indices), vocab_size))
for i, idx in enumerate(test_indices):
    test_input_onehot[i, idx] = 1

# Step 1: CLIENT converts text to random images
print("CLIENT: Converting text to random images...")
print("   'hello' ‚Üí random pattern, 'world' ‚Üí different random pattern, etc.")
client_generated_images = client_model.predict(test_input_onehot)

# Step 2: SERVER interprets random images back to original text
print("SERVER: Interpreting random images back to text...")
print("   random pattern ‚Üí 'hello', different pattern ‚Üí 'world', etc.")
server_predictions = server_model.predict(client_generated_images)
server_predicted_indices = np.argmax(server_predictions, axis=1)
server_predicted_texts = [idx_to_msg[idx] for idx in server_predicted_indices]

# Check diversity of generated images
print("\nAnalyzing generated image diversity...")
image_similarities = []
for i in range(len(client_generated_images)):
    for j in range(i+1, len(client_generated_images)):
        img1 = client_generated_images[i].flatten()
        img2 = client_generated_images[j].flatten()
        similarity = np.corrcoef(img1, img2)[0, 1]
        image_similarities.append(similarity)

avg_similarity = np.mean(image_similarities) if image_similarities else 0
print(f"Average similarity between different text images: {avg_similarity:.4f}")
print(f"(Lower values = more diverse images = better learning)")

# =============================================================================
# Results Visualization
# =============================================================================
print("Visualizing results...")

plt.figure(figsize=(15, 10))

num_samples = len(test_messages)

for i in range(num_samples):
    # Original text
    plt.subplot(3, num_samples, i+1)
    plt.text(0.5, 0.5, f"Original:\n'{test_messages[i]}'", 
             fontsize=12, ha='center', va='center',
             bbox=dict(boxstyle="round", facecolor="lightblue"))
    plt.title(f'CLIENT Input')
    plt.axis('off')
    
    # Generated image (CLIENT output)
    plt.subplot(3, num_samples, i+1+num_samples)
    plt.imshow(client_generated_images[i].reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('CLIENT Output\n(Generated Image)')
    plt.axis('off')
    
    # Predicted text (SERVER output)
    plt.subplot(3, num_samples, i+1+num_samples*2)
    predicted_text = server_predicted_texts[i]
    is_correct = (test_messages[i] == predicted_text)
    color = "lightgreen" if is_correct else "lightcoral"
    
    plt.text(0.5, 0.5, f"Predicted:\n'{predicted_text}'", 
             fontsize=12, ha='center', va='center',
             bbox=dict(boxstyle="round", facecolor=color))
    plt.title(f'SERVER Output\n{"‚úì" if is_correct else "‚úó"}')
    plt.axis('off')

plt.suptitle('Text-Image Communication System\n(CLIENT: Text‚ÜíImage, SERVER: Image‚ÜíText)', fontsize=16)
plt.tight_layout()
plt.show()

# =============================================================================
# Performance Analysis
# =============================================================================
print("Performance Analysis:")

# Calculate accuracy
correct_predictions = sum(1 for orig, pred in zip(test_messages, server_predicted_texts) if orig == pred)
accuracy = correct_predictions / len(test_messages)

print(f"Communication Accuracy: {accuracy:.2%} ({correct_predictions}/{len(test_messages)})")

# Analysis of true learning vs memorization
print("\n" + "="*50)
print("NEURAL NETWORK LEARNING ANALYSIS")
print("="*50)

if avg_similarity < 0.8:
    print("üéâ EXCELLENT: Different texts generate DIVERSE images")
    print("   ‚Üí Neural networks learned to create different patterns for different words")
    print("   ‚Üí This is TRUE cross-modality learning!")
    
    if accuracy > 0.6:
        print("üéØ SUCCESS: High accuracy with diverse images")
        print("   ‚Üí Networks successfully learned arbitrary text ‚Üî image mappings")
        print("   ‚Üí 'hello' ‚Üí random pattern A ‚Üí 'hello' (through learned weights)")
        print("   ‚Üí 'world' ‚Üí random pattern B ‚Üí 'world' (through learned weights)")
    else:
        print("ÔøΩ LEARNING IN PROGRESS: Accuracy improving with more training")
        print("   ‚Üí Networks are learning but need more epochs")
        
elif avg_similarity > 0.95:
    print("‚ö†Ô∏è  WARNING: Images are too similar")
    print("   ‚Üí Networks might not be learning diverse mappings")
    print("   ‚Üí May need more training or different architecture")
else:
    print("üìä MODERATE: Some diversity in generated images")
    print("   ‚Üí Networks showing signs of learning")
    print("   ‚Üí Could benefit from more training")

print(f"\nImage diversity metric: {avg_similarity:.4f}")
print(f"(Lower = more diverse = better learning)")

print(f"\nüß† How it works:")
print(f"   1. Text Encoder: Converts words to {LATENT_DIM}D vectors")  
print(f"   2. Image Decoder: Converts {LATENT_DIM}D vectors to 28√ó28 images")
print(f"   3. Image Encoder: Converts 28√ó28 images to {LATENT_DIM}D vectors") 
print(f"   4. Text Decoder: Converts {LATENT_DIM}D vectors back to words")
print(f"   5. Shared latent space allows communication!")

print("\nNext step recommendation:")
if avg_similarity < 0.8 and accuracy > 0.6:
    print("‚Üí System is working well! Try with real images (dogs, cats, etc.)")
else:
    print("‚Üí Continue training or adjust architecture for better learning")

# Analyze vector differences
print("Vector Space Analysis:")

# Get text vectors (original)
text_vectors = test_input_onehot

# Get image vectors (after CLIENT conversion)
client_latents = text_encoder.predict(test_input_onehot)
image_vectors = client_generated_images.reshape(len(client_generated_images), -1)

# Calculate vector distances
vector_distances = []
for i in range(len(text_vectors)):
    text_norm = np.linalg.norm(text_vectors[i])
    image_norm = np.linalg.norm(image_vectors[i])
    
    if text_norm > 0 and image_norm > 0:
        # Cosine distance
        cosine_sim = np.dot(text_vectors[i], image_vectors[i][:len(text_vectors[i])]) / (text_norm * image_norm)
        vector_distances.append(1 - cosine_sim)

avg_distance = np.mean(vector_distances) if vector_distances else 0

print(f"Average Vector Distance: {avg_distance:.4f}")
print(f"Vector Space Separation: {'HIGH' if avg_distance > 0.8 else 'MEDIUM' if avg_distance > 0.5 else 'LOW'}")

print("Detailed Results:")
for i, (orig, pred) in enumerate(zip(test_messages, server_predicted_texts)):
    status = "‚úì" if orig == pred else "‚úó"
    print(f"  {i+1}. '{orig}' ‚Üí '{pred}' {status}")

print(f"\nCommunication System Complete!")
print(f"Key Points:")
print(f"  ‚Ä¢ Text and Image vectors are in completely different spaces")
print(f"  ‚Ä¢ CLIENT converts text to images")
print(f"  ‚Ä¢ SERVER interprets images back to text") 
print(f"  ‚Ä¢ Models learn to communicate through shared latent space")
print(f"  ‚Ä¢ Vector distance: {avg_distance:.4f} (different spaces)")
print(f"  ‚Ä¢ Communication accuracy: {accuracy:.2%}")
