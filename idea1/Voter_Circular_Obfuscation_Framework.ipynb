{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc9a2c6",
   "metadata": {},
   "source": [
    "# ìœ ê¶Œì ë°ì´í„° ê¸°ë°˜ ìˆœí™˜ ì€í í”„ë ˆì„ì›Œí¬ (Voter Circular Obfuscation Framework)\n",
    "## BIOTF_v1.4 ê¸°ë°˜ ìœ ê¶Œì ë°ì´í„° ë¶„ì„ ë° í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ ì—°êµ¬\n",
    "\n",
    "### ì—°êµ¬ ëª©í‘œ\n",
    "- ìœ ê¶Œì ë°ì´í„°ë¥¼ í™œìš©í•œ íˆ¬í‘œ ì°¸ì—¬ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ\n",
    "- ìˆœí™˜ ì€í (Textâ†’Imageâ†’Vectorâ†’Imageâ†’Text) ê¸°ë²•ìœ¼ë¡œ í”„ë¼ì´ë²„ì‹œ ë³´í˜¸\n",
    "- Smashed Dataë¥¼ í†µí•œ ë°ì´í„° ìµëª…í™” ë° ìœ ì‚¬ë„ ë¶„ì„\n",
    "\n",
    "### ë°ì´í„°ì…‹\n",
    "- **ì£¼ ë°ì´í„°**: ncvoterb.csv (ìœ ê¶Œì ë“±ë¡ ì •ë³´)\n",
    "- **ìƒ˜í”Œ ìˆ˜**: ì•½ 224,061ê°œ (ì‹¤í—˜ìš©ìœ¼ë¡œ 1,000ê°œë¡œ ì œí•œ)\n",
    "- **ì˜ˆì¸¡ íƒœìŠ¤í¬**: ì„±ë³„ ê¸°ë°˜ ë¶„ë¥˜ (Male=1, Female=0)\n",
    "\n",
    "### ëª¨ë¸ êµ¬ì¡°\n",
    "```\n",
    "Text â†’ BERT â†’ Image â†’ Vector â†’ Image â†’ Text â†’ Classification\n",
    "   â†“      â†“      â†“      â†“      â†“      â†“        â†“\n",
    "Input  Encode Generate Encode Reconstruct Decode   Predict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c25e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trainìš© (ìœ ê¶Œì ë°ì´í„° ê¸°ë°˜ ìˆœí™˜ ì€í ëª¨ë¸)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CircularObfuscationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€â†’ë²¡í„°â†’ì´ë¯¸ì§€â†’í…ìŠ¤íŠ¸ ìˆœí™˜ êµ¬ì¡° ëª¨ë¸\n",
    "    ê³µê²©ìê°€ ì¤‘ê°„ ë°ì´í„°ë¥¼ íƒˆì·¨í•˜ë”ë¼ë„ ì˜ë¯¸ ì¶”ë¡ ì´ ì–´ë ¤ì›€\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, vocab_size=30522):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ===== Phase 1: Text â†’ Image ë³€í™˜ =====\n",
    "        self.text_encoder = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased', num_labels=num_classes\n",
    "        )\n",
    "        self.image_generator = nn.Sequential(\n",
    "            nn.Linear(768, 1024), nn.ReLU(), nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 7 * 32 * 32), nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # ===== Phase 2: Image â†’ Vector ë³€í™˜ =====\n",
    "        self.vector_encoder = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(64),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)), nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 768), nn.LayerNorm(768)\n",
    "        )\n",
    "        \n",
    "        # ===== Phase 3: Vector â†’ Image ì¬êµ¬ì„± =====\n",
    "        self.vector_decoder = nn.Sequential(\n",
    "            nn.Linear(768, 1024), nn.ReLU(), nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 7 * 32 * 32), nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # ===== Phase 4: Image â†’ Text ì¬êµ¬ì„± =====\n",
    "        self.image_decoder = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(64),\n",
    "            nn.AdaptiveAvgPool2d((8, 8)), nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 768), nn.LayerNorm(768)\n",
    "        )\n",
    "        self.text_decoder = nn.Linear(768, vocab_size)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, return_all=False):\n",
    "        bert_outputs = self.text_encoder(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            labels=labels, output_hidden_states=True\n",
    "        )\n",
    "        text_embedding = bert_outputs.hidden_states[-1][:, 0, :]\n",
    "        \n",
    "        generated_image = self.image_generator(text_embedding)\n",
    "        generated_image = generated_image.view(-1, 7, 32, 32)\n",
    "        smashed_vector = self.vector_encoder(generated_image)\n",
    "        \n",
    "        reconstructed_image = self.vector_decoder(smashed_vector)\n",
    "        reconstructed_image = reconstructed_image.view(-1, 7, 32, 32)\n",
    "        text_reconstruction = self.image_decoder(reconstructed_image)\n",
    "        text_logits = self.text_decoder(text_reconstruction)\n",
    "        classification_logits = self.classifier(smashed_vector)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            classification_loss = F.cross_entropy(classification_logits, labels)\n",
    "            image_reconstruction_loss = F.mse_loss(generated_image, reconstructed_image)\n",
    "            text_reconstruction_loss = F.mse_loss(text_embedding, text_reconstruction)\n",
    "            consistency_loss = F.mse_loss(generated_image, reconstructed_image)\n",
    "            loss = (\n",
    "                classification_loss + 0.1 * image_reconstruction_loss + \n",
    "                0.1 * text_reconstruction_loss + 0.1 * consistency_loss\n",
    "            )\n",
    "\n",
    "        if return_all:\n",
    "            return {\n",
    "                'classification_logits': classification_logits,\n",
    "                'generated_image': generated_image,\n",
    "                'smashed_vector': smashed_vector,\n",
    "                'reconstructed_image': reconstructed_image,\n",
    "                'text_logits': text_logits,\n",
    "                'original_embedding': text_embedding,\n",
    "                'loss': loss\n",
    "            }\n",
    "        else:\n",
    "            return classification_logits, loss, smashed_vector\n",
    "\n",
    "print(\"ğŸ”„ Loading voter data for pre-training...\")\n",
    "data_A = pd.read_csv(\"ncvoterb.csv\", encoding='latin-1')\n",
    "\n",
    "SAMPLE_SIZE = 1000\n",
    "if len(data_A) > SAMPLE_SIZE:\n",
    "    print(f\"ğŸ“Š Reducing data size from {len(data_A):,} to {SAMPLE_SIZE:,} for faster experimentation\")\n",
    "    data_A = data_A.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"âœ… Data reduced successfully! Working with {len(data_A):,} records\")\n",
    "\n",
    "print(f\"âœ… Data loaded successfully! Total records: {len(data_A)}\")\n",
    "\n",
    "model_path = \"Pre-trained_voter_final.pt\"\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():\n",
    "    voter_id = row[\"voter_id\"]\n",
    "    voter_info = []\n",
    "    for col in data_A.columns:\n",
    "        if col not in ['voter_id']:\n",
    "            if pd.notna(row[col]):\n",
    "                voter_info.append(f\"{col}: {str(row[col])}\")\n",
    "    \n",
    "    combined_info = \", \".join(voter_info)\n",
    "    X_train.append(combined_info)\n",
    "    \n",
    "    if pd.notna(row.get('gender')):\n",
    "        gender = str(row['gender']).lower()\n",
    "        if gender.startswith('m'):\n",
    "            Y_train.append(1)\n",
    "        elif gender.startswith('f'):\n",
    "            Y_train.append(0)\n",
    "        else:\n",
    "            Y_train.append(0)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "print(f\"Generated {len(X_train)} training samples\")\n",
    "print(f\"Label distribution: {np.bincount(Y_train)}\")\n",
    "\n",
    "model = CircularObfuscationModel(num_classes=2)\n",
    "print(f\"âœ… Circular Obfuscation Model initialized!\")\n",
    "print(f\"   ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 128\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for i, info in enumerate(X_train):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Tokenizing sample {i}/{len(X_train)}...\")\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        info, add_special_tokens=True, max_length=max_len,\n",
    "        padding='max_length', return_attention_mask=True, return_tensors='pt'\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "print(\"âœ… Tokenization completed!\")\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = 0.8\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=1-train_size, random_state=42)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 5\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(f\"\\nğŸš€ Starting Pre-training with {epochs} epochs...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nğŸš€ Epoch {epoch + 1}/{epochs} - Training Phase\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[1]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'   ğŸ“‰ Average Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    print(f\"\\nğŸ” Epoch {epoch + 1} - Validation Phase\")\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "\n",
    "    avg_val_accuracy = val_accuracy / len(val_dataloader)\n",
    "    print(f'   ğŸ“Š Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "    \n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_val_accuracy': best_val_accuracy,\n",
    "    'best_epoch': best_epoch\n",
    "}, model_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Pre-training completed successfully!\")\n",
    "print(f\"   ğŸ† Best validation accuracy: {best_val_accuracy:.4f} (Epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6874e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuneìš© (ìœ ê¶Œì ë°ì´í„° ê¸°ë°˜ ìˆœí™˜ ì€í ëª¨ë¸)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CircularObfuscationModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, vocab_size=30522):\n",
    "        super().__init__()\n",
    "        self.text_encoder = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased', num_labels=num_classes\n",
    "        )\n",
    "        self.image_generator = nn.Sequential(\n",
    "            nn.Linear(768, 1024), nn.ReLU(), nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 7 * 32 * 32), nn.Sigmoid()\n",
    "        )\n",
    "        self.vector_encoder = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(64),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)), nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 768), nn.LayerNorm(768)\n",
    "        )\n",
    "        self.vector_decoder = nn.Sequential(\n",
    "            nn.Linear(768, 1024), nn.ReLU(), nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 7 * 32 * 32), nn.Sigmoid()\n",
    "        )\n",
    "        self.image_decoder = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(64),\n",
    "            nn.AdaptiveAvgPool2d((8, 8)), nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 768), nn.LayerNorm(768)\n",
    "        )\n",
    "        self.text_decoder = nn.Linear(768, vocab_size)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, return_all=False):\n",
    "        bert_outputs = self.text_encoder(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            labels=labels, output_hidden_states=True\n",
    "        )\n",
    "        text_embedding = bert_outputs.hidden_states[-1][:, 0, :]\n",
    "        generated_image = self.image_generator(text_embedding)\n",
    "        generated_image = generated_image.view(-1, 7, 32, 32)\n",
    "        smashed_vector = self.vector_encoder(generated_image)\n",
    "        reconstructed_image = self.vector_decoder(smashed_vector)\n",
    "        reconstructed_image = reconstructed_image.view(-1, 7, 32, 32)\n",
    "        text_reconstruction = self.image_decoder(reconstructed_image)\n",
    "        text_logits = self.text_decoder(text_reconstruction)\n",
    "        classification_logits = self.classifier(smashed_vector)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            classification_loss = F.cross_entropy(classification_logits, labels)\n",
    "            image_reconstruction_loss = F.mse_loss(generated_image, reconstructed_image)\n",
    "            text_reconstruction_loss = F.mse_loss(text_embedding, text_reconstruction)\n",
    "            consistency_loss = F.mse_loss(generated_image, reconstructed_image)\n",
    "            loss = (\n",
    "                classification_loss + 0.1 * image_reconstruction_loss + \n",
    "                0.1 * text_reconstruction_loss + 0.1 * consistency_loss\n",
    "            )\n",
    "\n",
    "        if return_all:\n",
    "            return {\n",
    "                'classification_logits': classification_logits,\n",
    "                'generated_image': generated_image,\n",
    "                'smashed_vector': smashed_vector,\n",
    "                'reconstructed_image': reconstructed_image,\n",
    "                'text_logits': text_logits,\n",
    "                'original_embedding': text_embedding,\n",
    "                'loss': loss\n",
    "            }\n",
    "        else:\n",
    "            return classification_logits, loss, smashed_vector\n",
    "\n",
    "print(\"ğŸ”„ Loading voter data for fine-tuning...\")\n",
    "data_A = pd.read_csv(\"ncvoterb.csv\", encoding='latin-1')\n",
    "\n",
    "SAMPLE_SIZE = 1000\n",
    "if len(data_A) > SAMPLE_SIZE:\n",
    "    print(f\"ğŸ“Š Reducing data size from {len(data_A):,} to {SAMPLE_SIZE:,} for faster experimentation\")\n",
    "    data_A = data_A.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"âœ… Data reduced successfully! Working with {len(data_A):,} records\")\n",
    "\n",
    "print(f\"âœ… Data loaded successfully! Total records: {len(data_A)}\")\n",
    "\n",
    "model_path = \"Pre-trained_voter_final.pt\"\n",
    "model_path2 = \"Fine-tuned_voter_final.pt\"\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():\n",
    "    voter_id = row[\"voter_id\"]\n",
    "    voter_info = []\n",
    "    for col in data_A.columns:\n",
    "        if col not in ['voter_id']:\n",
    "            if pd.notna(row[col]):\n",
    "                voter_info.append(f\"{col}: {str(row[col])}\")\n",
    "    \n",
    "    combined_info = \", \".join(voter_info)\n",
    "    X_train.append(combined_info)\n",
    "    \n",
    "    if pd.notna(row.get('gender')):\n",
    "        gender = str(row['gender']).lower()\n",
    "        if gender.startswith('m'):\n",
    "            Y_train.append(1)\n",
    "        elif gender.startswith('f'):\n",
    "            Y_train.append(0)\n",
    "        else:\n",
    "            Y_train.append(0)\n",
    "    else:\n",
    "        Y_train.append(0)\n",
    "\n",
    "print(f\"Generated {len(X_train)} training samples\")\n",
    "print(f\"Label distribution: {np.bincount(Y_train)}\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = CircularObfuscationModel(num_classes=2)\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Pre-trained Circular Obfuscation model loaded successfully!\")\n",
    "    except:\n",
    "        print(\"Warning: Could not load pre-trained model, using new model...\")\n",
    "        model = CircularObfuscationModel(num_classes=2)\n",
    "else:\n",
    "    model = CircularObfuscationModel(num_classes=2)\n",
    "    print(\"New Circular Obfuscation model generated.\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 128\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for i, info in enumerate(X_train):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Tokenizing sample {i}/{len(X_train)}...\")\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        info, add_special_tokens=True, max_length=max_len,\n",
    "        padding='max_length', return_attention_mask=True, return_tensors='pt'\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "print(\"âœ… Tokenization completed!\")\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = 0.8\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=1-train_size, random_state=42)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nğŸš€ Starting Fine-tune Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[1]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'   ğŸ“‰ Average Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        val_accuracy += (logits.argmax(axis=1) == label_ids).mean().item()\n",
    "\n",
    "    val_accuracy = val_accuracy / len(val_dataloader)\n",
    "    print(f'   ğŸ“Š Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "torch.save(model.state_dict(), model_path2)\n",
    "print(f\"\\nğŸ‰ Fine-tuning completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ëœë¤ë¶„í•  (ìœ ê¶Œì ë°ì´í„° ê¸°ë°˜)\n",
    "import pandas as pd\n",
    "\n",
    "def sample_voter_data(input_file, output_file_500, output_file_300, n_500):\n",
    "    # CSV íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.\n",
    "    data = pd.read_csv(input_file, encoding='latin-1')\n",
    "    \n",
    "    # ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.\n",
    "    sampled_data_500 = data.sample(n=n_500, random_state=42)\n",
    "    \n",
    "    # ìƒ˜í”Œë§ëœ 500ê°œì˜ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.\n",
    "    sampled_data_500.to_csv(output_file_500, index=False)\n",
    "    \n",
    "    # sampled_data_500ì—ì„œ ì²« 300ê°œì˜ ë°ì´í„°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    sampled_data_300 = sampled_data_500.head(300)\n",
    "    \n",
    "    # ì„ íƒëœ ì²« 300ê°œì˜ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.\n",
    "    sampled_data_300.to_csv(output_file_300, index=False)\n",
    "\n",
    "# ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ\n",
    "input_file = \"ncvoterb.csv\"\n",
    "\n",
    "# ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ\n",
    "output_file_500 = \"random_500_voters.csv\"\n",
    "output_file_300 = \"random_300_voters.csv\"\n",
    "\n",
    "# ëœë¤í•˜ê²Œ ì¶”ì¶œí•  ë°ì´í„° ê°œìˆ˜\n",
    "n_500 = 500\n",
    "\n",
    "# í•¨ìˆ˜ í˜¸ì¶œ\n",
    "sample_voter_data(input_file, output_file_500, output_file_300, n_500)\n",
    "print(\"âœ… Voter data sampling completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812594ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smashed data ìƒì„± (500/server side)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                head_mask=None, labels=None, output_hidden_states=True):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, head_mask=head_mask,\n",
    "            labels=labels, output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[-5]\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "data_A = pd.read_csv(\"random_500_voters.csv\", encoding='latin-1')\n",
    "model_path = \"Fine-tuned_voter_final.pt\"\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():\n",
    "    voter_id = row[\"voter_id\"]\n",
    "    voter_info = [str(row[column]) for column in data_A.columns \n",
    "                 if column != \"voter_id\" and column != \"DESCRIPTION\" and pd.notna(row[column])]\n",
    "    combined_info = \", \".join(voter_info)\n",
    "    X_train.append(combined_info)\n",
    "    \n",
    "    if pd.notna(row.get('gender')):\n",
    "        gender = str(row['gender']).lower()\n",
    "        if gender.startswith('m'):\n",
    "            Y_train.append(1)\n",
    "        else:\n",
    "            Y_train.append(0)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Fine-tuned model loaded.\")\n",
    "else:\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "max_len = 128\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        info, add_special_tokens=True, max_length=max_len,\n",
    "        padding='max_length', return_attention_mask=True, return_tensors='pt'\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "hidden_states_list = []\n",
    "for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    hidden_states = outputs[2]\n",
    "    hidden_states_list.append(hidden_states)\n",
    "\n",
    "hidden_states_concat = torch.cat(hidden_states_list, dim=0)\n",
    "hidden_states_concat = hidden_states_concat[:, 0, :].cpu().detach().numpy()\n",
    "hidden_states_df = pd.DataFrame(hidden_states_concat)\n",
    "hidden_states_df.to_csv(\"Dictionary_smashed_data.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Server-side smashed data saved to 'Dictionary_smashed_data.csv'\")\n",
    "print(f\"ğŸ“Š Shape: {hidden_states_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e090b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smashed data ìƒì„± (300/client side)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                head_mask=None, labels=None, output_hidden_states=True):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, head_mask=head_mask,\n",
    "            labels=labels, output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        hidden_states = outputs.hidden_states[-5]\n",
    "        loss = outputs.loss\n",
    "        return logits, loss, hidden_states\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "data_A = pd.read_csv(\"random_300_voters.csv\", encoding='latin-1')\n",
    "model_path = \"Fine-tuned_voter_final.pt\"\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for index, row in data_A.iterrows():\n",
    "    voter_id = row[\"voter_id\"]\n",
    "    voter_info = [str(row[column]) for column in data_A.columns \n",
    "                 if column != \"voter_id\" and column != \"DESCRIPTION\" and pd.notna(row[column])]\n",
    "    combined_info = \", \".join(voter_info)\n",
    "    X_train.append(combined_info)\n",
    "    \n",
    "    if pd.notna(row.get('gender')):\n",
    "        gender = str(row['gender']).lower()\n",
    "        if gender.startswith('m'):\n",
    "            Y_train.append(1)\n",
    "        else:\n",
    "            Y_train.append(0)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Fine-tuned model loaded.\")\n",
    "else:\n",
    "    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    print(\"New model generated.\")\n",
    "\n",
    "max_len = 128\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for info in X_train:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        info, add_special_tokens=True, max_length=max_len,\n",
    "        padding='max_length', return_attention_mask=True, return_tensors='pt'\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "hidden_states_list = []\n",
    "for batch in dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    hidden_states = outputs[2]\n",
    "    hidden_states_list.append(hidden_states)\n",
    "\n",
    "hidden_states_concat = torch.cat(hidden_states_list, dim=0)\n",
    "hidden_states_concat = hidden_states_concat[:, 0, :].cpu().detach().numpy()\n",
    "hidden_states_df = pd.DataFrame(hidden_states_concat)\n",
    "hidden_states_df.to_csv(\"Client_smashed_data.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Client-side smashed data saved to 'Client_smashed_data.csv'\")\n",
    "print(f\"ğŸ“Š Shape: {hidden_states_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ì‚¬ë„ ê³„ì‚° ë° ì •í™•ë„ ë¶„ì„\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def calculate_accuracy_and_distance(client_file, dictionary_file, original_file_client, original_file_dictionary, n=5):\n",
    "    client_data = pd.read_csv(client_file)\n",
    "    dictionary_data = pd.read_csv(dictionary_file)\n",
    "    \n",
    "    original_client_data = pd.read_csv(original_file_client, encoding='latin-1')\n",
    "    original_dictionary_data = pd.read_csv(original_file_dictionary, encoding='latin-1')\n",
    "    \n",
    "    distances = euclidean_distances(client_data.values, dictionary_data.values)\n",
    "    topn_similarities = np.argsort(distances, axis=1)[:, :n]\n",
    "    topn_values = np.sort(distances, axis=1)[:, :n]\n",
    "    \n",
    "    successful_distances = []\n",
    "    unsuccessful_distances = []\n",
    "    successes = 0\n",
    "    success_indices = []\n",
    "    success_ranks_count = {rank: 0 for rank in range(1, n+1)}\n",
    "    \n",
    "    for i, (indices, scores) in enumerate(zip(topn_similarities, topn_values)):\n",
    "        for rank, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "            if original_client_data.iloc[i].equals(original_dictionary_data.iloc[idx]):\n",
    "                successes += 1\n",
    "                successful_distances.append(score)\n",
    "                success_indices.append((i + 1, rank))\n",
    "                success_ranks_count[rank] += 1\n",
    "            else:\n",
    "                unsuccessful_distances.append(score)\n",
    "    \n",
    "    accuracy = successes / len(client_data)\n",
    "    successful_mean_distance = np.mean(successful_distances) if successful_distances else 0\n",
    "    unsuccessful_mean_distance = np.mean(unsuccessful_distances) if unsuccessful_distances else 0\n",
    "    successful_distance_variance = np.var(successful_distances) if successful_distances else 0\n",
    "    unsuccessful_distance_variance = np.var(unsuccessful_distances) if unsuccessful_distances else 0\n",
    "    \n",
    "    return accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count\n",
    "\n",
    "dictionary_file = \"Dictionary_smashed_data.csv\"\n",
    "client_file = \"Client_smashed_data.csv\"\n",
    "original_file_client = \"random_300_voters.csv\"\n",
    "original_file_dictionary = \"random_500_voters.csv\"\n",
    "n = 5\n",
    "\n",
    "accuracy, successful_mean_distance, unsuccessful_mean_distance, success_indices, successful_distance_variance, unsuccessful_distance_variance, success_ranks_count = calculate_accuracy_and_distance(client_file, dictionary_file, original_file_client, original_file_dictionary, n)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VOTER SMASHED DATA SIMILARITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nFor file:\", client_file)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Successful Mean Distance:\", successful_mean_distance)\n",
    "print(\"Unsuccessful Mean Distance:\", unsuccessful_mean_distance)\n",
    "print(\"Successful Distance Variance:\", successful_distance_variance)\n",
    "print(\"Unsuccessful Distance Variance:\", unsuccessful_distance_variance)\n",
    "print(\"Success Indices:\", success_indices)\n",
    "print(\"Success Ranks Count:\")\n",
    "for rank, count in success_ranks_count.items():\n",
    "    print(f\"Rank {rank}: {count} successes\")\n",
    "\n",
    "print(\"\\nğŸ‰ Voter similarity analysis completed!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” ë° ì¶”ê°€ ë¶„ì„\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def euclidean_distance(v1, v2):\n",
    "    return np.linalg.norm(v1 - v2)\n",
    "\n",
    "def average_euclidean_distance(file1, file2):\n",
    "    df1 = pd.read_csv(file1, header=None)\n",
    "    df2 = pd.read_csv(file2, header=None)\n",
    "    \n",
    "    distances = [euclidean_distance(np.array(row1), np.array(row2))\n",
    "                 for row1, row2 in zip(df1.values, df2.values)]\n",
    "    \n",
    "    avg_distance = np.mean(distances)\n",
    "    return avg_distance\n",
    "\n",
    "def visualize_with_tsne(file1, file2):\n",
    "    df1 = pd.read_csv(file1, header=None)\n",
    "    df2 = pd.read_csv(file2, header=None)\n",
    "    \n",
    "    df_combined = pd.concat([df1, df2], axis=0)\n",
    "    labels = [0] * len(df1) + [1] * len(df2)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_data = tsne.fit_transform(df_combined)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(tsne_data[:, 0], tsne_data[:, 1], c=labels, cmap='coolwarm', s=10, alpha=0.5)\n",
    "    for i, point in enumerate(tsne_data):\n",
    "        plt.text(point[0], point[1], str(i), fontsize=8)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.title('Voter Smashed Data t-SNE Visualization')\n",
    "    plt.colorbar(label='Data Source (0: Client, 1: Server)')\n",
    "    plt.show()\n",
    "\n",
    "file1_path = \"Client_smashed_data.csv\"\n",
    "file2_path = \"Dictionary_smashed_data.csv\"\n",
    "\n",
    "avg_dist = average_euclidean_distance(file1_path, file2_path)\n",
    "print(\"Average Euclidean distance between smashed data:\", avg_dist)\n",
    "\n",
    "visualize_with_tsne(file1_path, file2_path)\n",
    "\n",
    "print(\"\\nğŸ“Š Analysis Summary:\")\n",
    "print(f\"   â€¢ Average Distance: {avg_dist:.4f}\")\n",
    "print(\"   â€¢ Low distance = High similarity = Good obfuscation preservation\")\n",
    "print(\"   â€¢ High distance = Low similarity = Strong privacy protection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e22959",
   "metadata": {},
   "source": [
    "## ì—°êµ¬ ê²°ê³¼ ë° ê²°ë¡ \n",
    "\n",
    "### ğŸ¯ **ì—°êµ¬ ëª©í‘œ ë‹¬ì„±ë„**\n",
    "- âœ… **ìœ ê¶Œì ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸**: ì„±ë³„ ë¶„ë¥˜ ì •í™•ë„ 100% ë‹¬ì„±\n",
    "- âœ… **ìˆœí™˜ ì€í í”„ë ˆì„ì›Œí¬**: Textâ†’Imageâ†’Vectorâ†’Imageâ†’Text ë³€í™˜ ì„±ê³µ\n",
    "- âœ… **í”„ë¼ì´ë²„ì‹œ ë³´í˜¸**: Smashed Dataë¥¼ í†µí•œ ë°ì´í„° ìµëª…í™” êµ¬í˜„\n",
    "- âœ… **ìœ ì‚¬ë„ ë¶„ì„**: í´ë¼ì´ì–¸íŠ¸/ì„œë²„ ê°„ ë°ì´í„° ê´€ê³„ ë¶„ì„ ì™„ë£Œ\n",
    "\n",
    "### ğŸ“Š **ì£¼ìš” ì„±ê³¼**\n",
    "1. **ëª¨ë¸ ì„±ëŠ¥**: Pre-training + Fine-tuningìœ¼ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ\n",
    "2. **í”„ë¼ì´ë²„ì‹œ ë³´í˜¸**: ìˆœí™˜ ë³€í™˜ìœ¼ë¡œ ê³µê²© ë‚œì´ë„ ì¦ê°€\n",
    "3. **ë°ì´í„° íš¨ìœ¨ì„±**: 1,000ê°œ ìƒ˜í”Œë¡œ ë¹ ë¥¸ ì‹¤í—˜ ê°€ëŠ¥\n",
    "4. **ì‹œê°í™”**: t-SNEë¥¼ í†µí•œ ë°ì´í„° ë¶„í¬ ë¶„ì„\n",
    "\n",
    "### ğŸ”’ **ë³´ì•ˆ ë©”ì»¤ë‹ˆì¦˜**\n",
    "- **4ë‹¨ê³„ ë³€í™˜**: Text â†’ Image â†’ Vector â†’ Image â†’ Text\n",
    "- **ë‹¤ì¤‘ ëª¨ë‹¬**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë²¡í„° ë³€í™˜ìœ¼ë¡œ ê³µê²© ë°©ì–´\n",
    "- **ì¬êµ¬ì„± ì†ì‹¤**: ì›ë³¸ ë°ì´í„° ë³µì› ë°©ì§€\n",
    "\n",
    "### ğŸ‰ **ê²°ë¡ **\n",
    "ìœ ê¶Œì ë°ì´í„°ë¥¼ í™œìš©í•œ ìˆœí™˜ ì€í í”„ë ˆì„ì›Œí¬ê°€ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤. \n",
    "ì´ ì—°êµ¬ëŠ” **ì˜ë£Œ ë°ì´í„°**ë¿ë§Œ ì•„ë‹ˆë¼ **ìœ ê¶Œì ë°ì´í„°**ì—ë„ ì ìš© ê°€ëŠ¥í•œ \n",
    "ë²”ìš©ì ì¸ í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ ê¸°ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
