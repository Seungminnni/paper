#!/usr/bin/env python3
"""
Data Matching Performance Analysis
(Restored) Client Smashed Data vs (Dictionary) Server Smashed Data
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt

def load_smashed_data(file_path):
    """Smashed data CSV íŒŒì¼ ë¡œë“œ"""
    df = pd.read_csv(file_path)
    vectors = df.values
    print(f"ðŸ“Š Loaded {vectors.shape[0]} vectors of dimension {vectors.shape[1]} from {file_path}")
    return vectors

def calculate_best_match_similarity(client_vectors, server_vectors):
    """í´ë¼ì´ì–¸íŠ¸ ë²¡í„° ê°ê°ì— ëŒ€í•´ ì„œë²„ì—ì„œ ê°€ìž¥ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ì°¾ì•„ ìœ ì‚¬ë„ í†µê³„ ê³„ì‚°"""
    print("\nðŸ” Calculating best-match similarity statistics...")

    similarities = []

    # ê° í´ë¼ì´ì–¸íŠ¸ ë²¡í„°ì— ëŒ€í•´ ê°€ìž¥ ìœ ì‚¬í•œ ì„œë²„ ë²¡í„° ì°¾ê¸°
    for client_vec in client_vectors:
        sim_scores = cosine_similarity([client_vec], server_vectors)[0]
        max_sim = np.max(sim_scores)
        similarities.append(max_sim)

    similarities = np.array(similarities)

    print("ðŸ“ˆ Best-Match Similarity Statistics:")
    print(f"   â€¢ Mean best-match similarity: {np.mean(similarities):.4f}")
    print(f"   â€¢ Median best-match similarity: {np.median(similarities):.4f}")
    print(f"   â€¢ Min best-match similarity: {np.min(similarities):.4f}")
    print(f"   â€¢ Max best-match similarity: {np.max(similarities):.4f}")

    return similarities

def plot_similarity_distribution(similarities):
    """ìœ ì‚¬ë„ ë¶„í¬ ì‹œê°í™”"""
    plt.figure(figsize=(10, 6))
    plt.hist(similarities, bins=30, alpha=0.7, color='blue', edgecolor='black')
    plt.title('Distribution of Best-Match Cosine Similarities')
    plt.xlabel('Cosine Similarity')
    plt.ylabel('Frequency')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('data_matching_distribution.png', dpi=300)
    print(f"\nâœ… Plot saved as 'data_matching_distribution.png'")
    plt.close()

def main():
    print("ðŸ”¬ Data Matching Performance Analysis")
    print("=" * 60)

    # ë°ì´í„° ë¡œë“œ
    client_file = "restored_client_vectors.csv"
    server_file = "Dictionary_smashed_data_layer2.csv"

    try:
        client_vectors = load_smashed_data(client_file)
        server_vectors = load_smashed_data(server_file)
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        print("Please ensure all necessary files are generated by the pipeline first.")
        return

    # ìµœê³  ë§¤ì¹­ ìœ ì‚¬ë„ ë¶„ì„
    similarities = calculate_best_match_similarity(client_vectors, server_vectors)

    if similarities is not None:
        # ì‹œê°í™”
        plot_similarity_distribution(similarities)

        # ê²°ê³¼ ìš”ì•½
        print("\nðŸŽ‰ Analysis Complete!")
        print("=" * 60)
        print("ðŸ“‹ Summary of Data Matching Performance:")
        print(f"   â€¢ Client samples processed: {len(client_vectors)}")
        print(f"   â€¢ Server dictionary size: {len(server_vectors)}")
        print(f"   â€¢ Average of best-match similarities: {np.mean(similarities):.4f}")

        # í•´ì„
        print("\nðŸ’¡ Interpretation:")
        mean_sim = np.mean(similarities)
        if mean_sim > 0.9:
            print("   â€¢ High Confidence: Restored client vectors can be matched to the server dictionary with high confidence.")
        elif mean_sim > 0.7:
            print("   â€¢ Moderate Confidence: Matching is possible, but with some ambiguity.")
        else:
            print("   â€¢ Low Confidence: The pipeline significantly impacts matching performance.")

if __name__ == "__main__":
    main()
