#!/usr/bin/env python3
"""
Pipeline Information-Loss Analysis
(Before) Original vs (After) Restored Smashed Data
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import euclidean
import matplotlib.pyplot as plt

def load_smashed_data(file_path):
    """Smashed data CSV íŒŒì¼ ë¡œë“œ"""
    df = pd.read_csv(file_path)
    # ê° í–‰ì´ í•˜ë‚˜ì˜ ë²¡í„°
    vectors = df.values
    print(f"ðŸ“Š Loaded {vectors.shape[0]} vectors of dimension {vectors.shape[1]} from {file_path}")
    return vectors

def calculate_direct_similarity(original_vectors, restored_vectors):
    """ë‘ ë°ì´í„°ì…‹ì˜ ê° ë²¡í„°ë¥¼ 1:1ë¡œ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ í†µê³„ ê³„ì‚°"""
    print("\nðŸ” Calculating 1-to-1 similarity statistics...")

    # ê° ìŒì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì§ì ‘ ê³„ì‚°
    similarities = [
        cosine_similarity([original], [restored])[0][0] 
        for original, restored in zip(original_vectors, restored_vectors)
    ]

    similarities = np.array(similarities)

    print("ðŸ“ˆ Similarity Statistics (Original vs. Restored):")
    print(f"   â€¢ Mean similarity: {np.mean(similarities):.6f}")
    print(f"   â€¢ Median similarity: {np.median(similarities):.6f}")
    print(f"   â€¢ Std Dev similarity: {np.std(similarities):.6f}")
    print(f"   â€¢ Min similarity: {np.min(similarities):.6f} (Most information loss)")
    print(f"   â€¢ Max similarity: {np.max(similarities):.6f} (Least information loss)")

    return similarities

def plot_similarity_distribution(similarities):
    """ìœ ì‚¬ë„ ë¶„í¬ ì‹œê°í™”"""
    plt.figure(figsize=(10, 6))
    plt.hist(similarities, bins=50, alpha=0.75, color='green', edgecolor='black')
    plt.title('Distribution of Cosine Similarities (Original vs. Restored Vectors)')
    plt.xlabel('Cosine Similarity (1.0 = Perfect Reconstruction)')
    plt.ylabel('Frequency')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('information_loss_distribution.png', dpi=300)
    print("\nâœ… Plot saved as 'information_loss_distribution.png'")
    plt.close()

def main():
    print("ðŸ”¬ Pipeline Information-Loss Analysis")
    print("=" * 60)

    # ë°ì´í„° ë¡œë“œ
    original_file = "Client_smashed_data_layer2.csv"
    restored_file = "restored_client_vectors.csv"

    try:
        original_vectors = load_smashed_data(original_file)
        restored_vectors = load_smashed_data(restored_file)
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        print("Please run the full pipeline (client_smashed_data_generation.py -> ... -> server_side.py) first.")
        return

    if len(original_vectors) != len(restored_vectors):
        print("âŒ Error: The number of vectors in both files do not match.")
        return

    # 1:1 ìœ ì‚¬ë„ ë¶„ì„
    similarities = calculate_direct_similarity(original_vectors, restored_vectors)

    if similarities is not None:
        # ì‹œê°í™”
        plot_similarity_distribution(similarities)

        # ê²°ê³¼ ìš”ì•½
        print("\nðŸŽ‰ Analysis Complete!")
        print("=" * 60)
        print("ðŸ“‹ Summary of Pipeline Information Loss:")
        print(f"   â€¢ Compared {len(similarities)} pairs of vectors.")
        print(f"   â€¢ Average Cosine Similarity: {np.mean(similarities):.6f}")
        print(f"   â€¢ Minimum Similarity (Worst Case): {np.min(similarities):.6f}")

        # í•´ì„
        print("\nðŸ’¡ Interpretation:")
        mean_sim = np.mean(similarities)
        if mean_sim > 0.9999:
            print("   â€¢ Excellent: The image conversion pipeline causes negligible information loss.")
        elif mean_sim > 0.99:
            print("   â€¢ Good: The pipeline causes very minor information loss.")
        elif mean_sim > 0.95:
            print("   â€¢ Moderate: The pipeline causes some information loss, which might affect performance.")
        else:
            print("   â€¢ High: The pipeline significantly distorts the data.")

if __name__ == "__main__":
    main()
