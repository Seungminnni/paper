# í´ë¼ì´ì–¸íŠ¸ ì¸¡ smashed data ìƒì„± (ìœ ê¶Œì ë°ì´í„° ê¸°ë°˜)
import os
import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
import torch
from sklearn.model_selection import train_test_split

class CustomBertForSequenceClassification(BertForSequenceClassification):
    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        labels=None,
        output_hidden_states=True
    ):
        outputs = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            labels=labels,
            output_hidden_states=output_hidden_states
        )
        logits = outputs.logits
        hidden_states = outputs.hidden_states[7]  # 7ë²ˆì§¸ ë ˆì´ì–´ì˜ hidden statesë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        loss = outputs.loss
        return logits, loss, hidden_states

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
print("ğŸ”„ Loading voter data for client-side smashed data generation...")
data = pd.read_csv("ncvoterb.csv", encoding='latin-1')

# ë°ì´í„° í¬ê¸° ì œí•œ: ì‹¤í—˜ì„ ìœ„í•´ 20,000ê°œë¡œ ì œí•œ
# ì „ì²´ ë°ì´í„°: ì•½ 224,061ê°œ â†’ ì‹¤í—˜ìš©: 20,000ê°œ (ì•½ 8.9% ì‚¬ìš©)
SAMPLE_SIZE = 20000
if len(data) > SAMPLE_SIZE:
    print(f"ğŸ“Š Reducing data size from {len(data):,} to {SAMPLE_SIZE:,} for faster experimentation")
    data = data.sample(n=SAMPLE_SIZE, random_state=42)
    print(f"âœ… Data reduced successfully! Working with {len(data):,} records")

print(f"âœ… Data loaded successfully! Total records: {len(data)}")

# í´ë¼ì´ì–¸íŠ¸ ì¸¡ ë°ì´í„°ë¡œ ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜ (ì „ì²´ì˜ 30%)
# ì‹¤í—˜ìš© ë°ì´í„°ì—ì„œ 30% = 6,000ê°œ ì‚¬ìš©
client_sample_size = int(len(data) * 0.3)
client_data = data.sample(n=client_sample_size, random_state=123)  # ë‹¤ë¥¸ random_state ì‚¬ìš©
print(f"ğŸ“Š Client-side data size: {len(client_data)} (30% of {len(data)} = {len(client_data):,})")

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ë¡œ (Fine-tuned ëª¨ë¸)
model_path = "Fine_tuned_voter_epoch20_BERT_Medium.pt"

# X_train ìƒì„± (ë ˆì´ë¸”ì€ smashed data ìƒì„±ì— í•„ìš” ì—†ìŒ)
X_train = []

# ìœ ê¶Œì ë°ì´í„° ì»¬ëŸ¼ë“¤
text_columns = ['first_name', 'middle_name', 'last_name', 'age', 'gender', 'race', 'ethnic',
                'street_address', 'city', 'state', 'zip_code', 'birth_place']

for index, row in client_data.iterrows():
    voter_id = row["voter_id"]

    # í…ìŠ¤íŠ¸ ì •ë³´ ê²°í•© (ì´ë¦„, ì£¼ì†Œ, ì¸êµ¬í†µê³„ ì •ë³´)
    voter_info = []
    for col in text_columns:
        if pd.notna(row[col]):
            voter_info.append(f"{col}: {str(row[col])}")

    # ì¶”ê°€ ì •ë³´ ê²°í•© (ë“±ë¡ì¼, ì „í™”ë²ˆí˜¸ ë“±)
    if pd.notna(row.get('register_date')):
        voter_info.append(f"register_date: {str(row['register_date'])}")
    if pd.notna(row.get('full_phone_num')):
        voter_info.append(f"phone: {str(row['full_phone_num'])}")

    combined_info = ", ".join(voter_info)
    X_train.append(combined_info)

print(f"Generated {len(X_train)} training samples for client-side")

# BERT í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# ëª¨ë¸ì´ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³  ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±
if os.path.exists(model_path):
    # ì €ì¥ëœ ëª¨ë¸ì´ ìˆì„ ê²½ìš° ë¶ˆëŸ¬ì˜¤ê¸°
    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    model.load_state_dict(torch.load(model_path), strict=False)
    print("Fine-tuned model loaded for client-side.")
else:
    # ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì„ ê²½ìš° ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±
    model = CustomBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    print("New model generated for client-side.")

# ì…ë ¥ ë°ì´í„°ë¥¼ BERTì˜ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
max_len = 128  # ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´
print(f"ğŸ”„ Tokenizing {len(X_train)} client-side samples...")

input_ids = []
attention_masks = []

for i, info in enumerate(X_train):
    if i % 1000 == 0:  # 1000ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
        print(f"  Tokenizing sample {i}/{len(X_train)}...")
    encoded_dict = tokenizer.encode_plus(
                        info,                         # ìœ ê¶Œì ì •ë³´
                        add_special_tokens = True,    # [CLS], [SEP] í† í° ì¶”ê°€
                        max_length = max_len,         # ìµœëŒ€ ê¸¸ì´ ì§€ì •
                        padding = 'max_length',       # íŒ¨ë”©ì„ ì¶”ê°€í•˜ì—¬ ìµœëŒ€ ê¸¸ì´ë¡œ ë§ì¶¤
                        return_attention_mask = True, # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
                        return_tensors = 'pt',        # PyTorch í…ì„œë¡œ ë°˜í™˜
                   )

    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

print("âœ… Client-side tokenization completed!")
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)

# ë”ë¯¸ ë¼ë²¨ ìƒì„± (smashed data ìƒì„±ì—ëŠ” ì‹¤ì œ ë¼ë²¨ì´ í•„ìš” ì—†ìŒ)
dummy_labels = torch.zeros(len(X_train), dtype=torch.long)

# ë°ì´í„°ì…‹ ìƒì„±
dataset = TensorDataset(input_ids, attention_masks, dummy_labels)

# ë°ì´í„°ë¡œë” ìƒì„±
dataloader = DataLoader(dataset, batch_size=16, shuffle=False)

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (Apple M4ìš© MPS)
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device for client-side: {device}")

# ëª¨ë¸ì„ GPUë¡œ ì´ë™
model.to(device)

# ëª¨ë¸ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
model.eval()
hidden_states_list = []  # í‰ê°€í•  ë•Œ hidden stateë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

print("ğŸ”„ Generating smashed data for client-side...")
import time
start_time = time.time()

for batch_idx, batch in enumerate(dataloader):
    if batch_idx % 10 == 0:  # 10ë°°ì¹˜ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
        print(f"  Processing batch {batch_idx}/{len(dataloader)}...")
    
    batch = tuple(t.to(device) for t in batch)
    inputs = {'input_ids': batch[0],
              'attention_mask': batch[1],
              'labels': batch[2]}
    with torch.no_grad():
        outputs = model(**inputs)

    # hidden stateë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    hidden_states = outputs[2]
    hidden_states_list.append(hidden_states)

generation_time = time.time() - start_time
print(f"âœ… Client-side smashed data generation completed in {generation_time:.2f}s")

# hidden statesë¥¼ í•˜ë‚˜ì˜ í…ì„œë¡œ ê²°í•©
hidden_states_concat = torch.cat(hidden_states_list, dim=0)
hidden_states_concat = hidden_states_concat[:, 0, :].cpu().detach().numpy()

# DataFrameìœ¼ë¡œ ë³€í™˜ ë° CSV ì €ì¥
hidden_states_df = pd.DataFrame(hidden_states_concat)
hidden_states_df.to_csv("Client_smashed_data.csv", index=False)

print(f"ğŸ’¾ Client-side smashed data saved to 'Client_smashed_data.csv'")
print(f"ğŸ“Š Shape: {hidden_states_concat.shape}")
print("ğŸ‰ Client-side smashed data generation completed successfully!")
