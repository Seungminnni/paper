#!/usr/bin/env python3
"""
ê°œì„  ë²„ì „: ì»¨ë³¼ë£¨ì…˜ Autoencoder ì••ì¶• + Residual Attention CNN í•™ìŠµ
CSV í…ìŠ¤íŠ¸ â†’ Word2Vec â†’ ëª¨ìì´í¬ ì´ë¯¸ì§€ â†’ ì••ì¶• â†’ CNN â†’ ë²¡í„°
- íš¨ìœ¨ì„± ë° ì •í™•ë„ í‰ê°€ ì‹œìŠ¤í…œ ì¶”ê°€
- 10,000ê°œ ë°ì´í„° ì¤‘ 7,000ê°œ í•™ìŠµ, 3,000ê°œ ê²€ì¦
- ìµœì í™”ëœ ì—í¬í¬ ë° ë°°ì¹˜ ì‚¬ì´ì¦ˆ
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
try:
    from tensorflow.keras.optimizers import AdamW
except Exception:
    from tensorflow.keras.optimizers.experimental import AdamW
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import time
from datetime import datetime
import math

# -----------------------------------
# Word2Vec processor (ê°„ë‹¨ ë²„ì „)
# -----------------------------------
from simple_word2vec_mosaic import SimpleWord2VecProcessor

# -----------------------------------
# Residual Block
# -----------------------------------
def residual_block(x, filters, kernel_size=3):
    shortcut = x
    x = Conv2D(filters, kernel_size, padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Conv2D(filters, kernel_size, padding="same")(x)
    x = BatchNormalization()(x)
    x = Add()([shortcut, x])
    x = Activation("relu")(x)
    return x

# -----------------------------------
# Attention (Squeeze-and-Excitation)
# -----------------------------------
def se_block(x, reduction=16):
    filters = x.shape[-1]
    if filters is None:
        filters = tf.shape(x)[-1]
    se = GlobalAveragePooling2D()(x)
    se = Dense(filters // reduction, activation='relu')(se)
    se = Dense(filters, activation='sigmoid')(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([x, se])

# -----------------------------------
# Autoencoder for Compression
# -----------------------------------
def build_autoencoder(input_shape):
    """ì‹¤ì œ ì••ì¶•ì´ ê°€ëŠ¥í•œ Autoencoder êµ¬ì¶• - ì •ê·œí™” ê°•í™” ë° ì•ˆì •ì„± ê°œì„ """
    inputs = Input(shape=input_shape)
    print(f"   ğŸ”§ Building autoencoder for input shape: {input_shape}")

    # Encoder - ì ì§„ì  ì••ì¶• (ë“œë¡­ì•„ì›ƒ ë° ì •ê·œí™” ê°•í™”)
    x = Conv2D(64, (3, 3), strides=2, padding="same", activation="relu")(inputs)  # /2
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)  # ë“œë¡­ì•„ì›ƒ ì¶”ê°€
    x = residual_block(x, 64)
    x = se_block(x)
    print(f"   ğŸ“ After Conv1: {x.shape}")

    x = Conv2D(128, (3, 3), strides=2, padding="same", activation="relu")(x)  # /4
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)  # ë“œë¡­ì•„ì›ƒ ì¦ê°€
    x = residual_block(x, 128)
    x = se_block(x)
    print(f"   ğŸ“ After Conv2: {x.shape}")

    x = Conv2D(256, (3, 3), strides=2, padding="same", activation="relu")(x)  # /8
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)  # ë” ê°•í•œ ì •ê·œí™”
    x = residual_block(x, 256)
    x = se_block(x)
    print(f"   ğŸ“ After Conv3: {x.shape}")

    # ê°•ë ¥í•œ ì••ì¶•ì„ ìœ„í•œ ì¶”ê°€ ë ˆì´ì–´ (ì •ê·œí™” ê°•í™”)
    x = Conv2D(512, (3, 3), strides=1, padding="same", activation="relu")(x)  
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)  # ìµœê³  ìˆ˜ì¤€ ì •ê·œí™”
    
    # ì••ì¶•ëœ í‘œí˜„ (ë” ì‘ì€ ì°¨ì›ìœ¼ë¡œ ê°•í™”)
    encoded = Conv2D(128, (3, 3), strides=1, padding="same", activation="relu")(x)  # 256â†’128ë¡œ ì¶•ì†Œ
    print(f"   ğŸ—œï¸ Bottleneck (compressed): {encoded.shape}")
    
    # Decoder - ì •í™•í•œ ë³µì›ì„ ìœ„í•œ í¬ê¸° ë§ì¶¤ (ì •ê·œí™” í¬í•¨)
    x = Conv2D(256, (3, 3), strides=1, padding="same", activation="relu")(encoded)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    
    x = Conv2D(512, (3, 3), strides=1, padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Conv2DTranspose(256, (3, 3), strides=2, padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = residual_block(x, 256)
    
    x = Conv2DTranspose(128, (3, 3), strides=2, padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    x = residual_block(x, 128)
    
    x = Conv2DTranspose(64, (3, 3), strides=2, padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Dropout(0.1)(x)
    
    # ìµœì¢… ì¶œë ¥ - ì›ë³¸ê³¼ ì •í™•íˆ ê°™ì€ í¬ê¸°ë¡œ ë§ì¶¤
    decoded = Conv2D(1, (3, 3), padding="same", activation="sigmoid")(x)
    
    # í¬ê¸° ë¶ˆì¼ì¹˜ ì‹œ í¬ë¡­í•‘ìœ¼ë¡œ ì¡°ì •
    if decoded.shape[1] != input_shape[0] or decoded.shape[2] != input_shape[1]:
        decoded = Lambda(lambda x: x[:, :input_shape[0], :input_shape[1], :])(decoded)
    
    print(f"   ğŸ”„ Reconstructed: {decoded.shape}")

    # Full autoencoder (L2 ì •ê·œí™” ì¶”ê°€)
    autoencoder = Model(inputs, decoded)
    autoencoder.compile(
        optimizer=AdamW(learning_rate=5e-4, weight_decay=1e-4),
        loss='mse'
    )
    
    # Encoder only (ì••ì¶•ìš©)
    encoder = Model(inputs, encoded)
    
    return autoencoder, encoder

# -----------------------------------
# Residual Attention CNN for Prediction
# -----------------------------------
def build_residual_attention_cnn(input_shape, output_size):
    """ë‹¨ìˆœí™”ëœ CNN ëª¨ë¸ - í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ"""
    inputs = Input(shape=input_shape)

    # ë” ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ë³€ê²½
    x = Conv2D(32, (3, 3), padding="same", activation="relu")(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)

    x = Conv2D(64, (3, 3), strides=2, padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)

    x = Conv2D(128, (3, 3), strides=2, padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)

    x = GlobalAveragePooling2D()(x)
    
    # ë” ì‘ì€ Dense ë ˆì´ì–´
    x = Dense(128, activation="relu")(x)
    x = Dropout(0.5)(x)
    x = Dense(64, activation="relu")(x)  # íƒ€ê¹ƒ í¬ê¸°ì™€ ë§ì¶¤
    x = Dropout(0.3)(x)
    outputs = Dense(output_size, activation="linear")(x)

    model = Model(inputs, outputs)
    # ë” ë†’ì€ í•™ìŠµë¥ ë¡œ ì¡°ì •
    model.compile(
        optimizer=AdamW(learning_rate=5e-4, weight_decay=1e-5),
        loss="mse",
        metrics=["mae"]
    )
    return model

# -----------------------------------
# Load & Preprocess Data with Evaluation
# -----------------------------------
def load_text_data(max_samples=10000):
    """í™˜ì ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë” í’ë¶€í•œ í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±"""
    print(f"ğŸ“Š Loading patient data (max {max_samples} samples)...")
    df = pd.read_csv("patients.csv", nrows=max_samples).dropna(subset=["FIRST", "LAST"])
    
    # ë” í’ë¶€í•œ í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±
    texts = []
    for _, row in df.iterrows():
        parts = []
        # ì´ë¦„ ì •ë³´
        if pd.notna(row.get('FIRST')): parts.append(str(row['FIRST']))
        if pd.notna(row.get('LAST')): parts.append(str(row['LAST']))
        # ì§€ì—­ ì •ë³´
        if pd.notna(row.get('CITY')): parts.append(str(row['CITY']))
        if pd.notna(row.get('STATE')): parts.append(str(row['STATE']))
        # ì¸êµ¬í†µê³„ ì •ë³´
        if pd.notna(row.get('RACE')): parts.append(str(row['RACE']))
        if pd.notna(row.get('ETHNICITY')): parts.append(str(row['ETHNICITY']))
        if pd.notna(row.get('GENDER')): parts.append(str(row['GENDER']))
        
        text = ' '.join(parts).lower()
        texts.append(text)
    
    print(f"âœ… Loaded {len(texts)} patient records")
    print(f"   Sample text: '{texts[0]}'")
    return texts

def generate_mosaic(texts, processor, vector_size=256):
    """í…ìŠ¤íŠ¸ë¥¼ ëª¨ìì´í¬ ì´ë¯¸ì§€(ìƒ˜í”Œë³„ ì‘ì€ ì´ë¯¸ì§€)ë¡œ ë³€í™˜
    - ë°˜í™˜ X: (N, H, W, C)
    - ë°˜í™˜ vectors: (N, vector_size)
    - processor: ì‚¬ì „ì— í•™ìŠµëœ SimpleWord2VecProcessor
    """
    print(f"ğŸ¨ Generating per-sample mosaic from {len(texts)} texts...")
    if hasattr(processor, "vectorize"):
        vectors = processor.vectorize(texts)
    else:
        print("âš ï¸ processor.vectorize() not found; falling back to train_and_vectorize(). This may be slower.")
        vectors = processor.train_and_vectorize(texts)

    # ì •ê·œí™” (ìƒ˜í”Œë³„ min-maxê°€ ì•„ë‹Œ ì „ì²´ min-maxë¡œ ê³ ì •)
    vmin, vmax = vectors.min(), vectors.max()
    vectors = (vectors - vmin) / (vmax - vmin + 1e-8)

    # ë²¡í„°ë¥¼ ì •ì‚¬ê° ì´ë¯¸ì§€ë¡œ reshape (ì˜ˆ: 256D -> 16x16x1)
    side = int(math.sqrt(vector_size))
    assert side * side == vector_size, "vector_sizeëŠ” ì™„ì „ì œê³±ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: 256=16x16"
    N = vectors.shape[0]
    X = vectors.reshape(N, side, side, 1)
    print(f"   Mosaic batch shape: {X.shape} (N,H,W,C)")

    return X, vectors

# -----------------------------------
# Evaluation Metrics
# -----------------------------------
def calculate_metrics(y_true, y_pred, name=""):
    """í¬ê´„ì ì¸ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° - ì•ˆì •ì„± ê°œì„ """
    # ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    # NaN ë˜ëŠ” inf ê°’ ì œê±°
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if not np.any(valid_mask):
        print(f"   âš ï¸ Warning: No valid values found in {name}")
        return {
            'mse': float('inf'),
            'mae': float('inf'),
            'r2': -float('inf'),
            'correlation': 0.0,
            'smape': 100.0
        }
    
    y_true_clean = y_true[valid_mask]
    y_pred_clean = y_pred[valid_mask]
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
    mse = mean_squared_error(y_true_clean, y_pred_clean)
    mae = mean_absolute_error(y_true_clean, y_pred_clean)
    
    # ì•ˆì „í•œ RÂ² ê³„ì‚°
    try:
        # ë¶„ì‚°ì´ 0ì— ê°€ê¹Œìš´ ê²½ìš° ì²˜ë¦¬
        y_var = np.var(y_true_clean)
        if y_var < 1e-10:
            r2 = 0.0  # ìƒìˆ˜ íƒ€ê¹ƒì˜ ê²½ìš°
        else:
            r2 = r2_score(y_true_clean, y_pred_clean)
            # RÂ² ê°’ì´ ë¹„ì •ìƒì ì¸ ê²½ìš° í´ë¦¬í•‘
            if np.isnan(r2) or np.isinf(r2):
                r2 = -1.0  # ìµœì†Œê°’ìœ¼ë¡œ ì„¤ì •
            elif r2 < -10:
                r2 = -10.0  # í•˜í•œ ì„¤ì •
    except:
        r2 = -1.0
    
    # ì•ˆì „í•œ ìƒê´€ê´€ê³„ ê³„ì‚°
    try:
        if len(np.unique(y_true_clean)) < 2 or len(np.unique(y_pred_clean)) < 2:
            correlation = 0.0  # ìƒìˆ˜ ë°°ì—´ì˜ ê²½ìš°
        else:
            correlation = np.corrcoef(y_true_clean, y_pred_clean)[0, 1]
            if np.isnan(correlation) or np.isinf(correlation):
                correlation = 0.0
    except:
        correlation = 0.0
    
    # ì•ˆì „í•œ SMAPE ê³„ì‚°
    try:
        denominator = np.abs(y_true_clean) + np.abs(y_pred_clean) + 1e-8
        smape = 2.0 * np.mean(np.abs(y_true_clean - y_pred_clean) / denominator) * 100
        if np.isnan(smape) or np.isinf(smape):
            smape = 100.0
    except:
        smape = 100.0
    
    metrics = {
        'mse': float(mse),
        'mae': float(mae),
        'r2': float(r2),
        'correlation': float(correlation),
        'smape': float(smape)
    }
    
    if name:
        print(f"ğŸ“Š {name} Metrics:")
        print(f"   â€¢ MSE: {mse:.6f}")
        print(f"   â€¢ MAE: {mae:.4f}")
        print(f"   â€¢ RÂ²: {r2:.4f}")
        print(f"   â€¢ Correlation: {correlation:.4f}")
        print(f"   â€¢ SMAPE: {smape:.2f}%")
        print(f"   â€¢ Valid samples: {len(y_true_clean)}/{len(y_true)}")
    
    return metrics

def plot_results(history_ae, history_cnn, train_metrics, val_metrics):
    """ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Autoencoder í•™ìŠµ ê³¡ì„ 
    axes[0, 0].plot(history_ae.history['loss'], label='Train Loss')
    axes[0, 0].plot(history_ae.history['val_loss'], label='Val Loss')
    axes[0, 0].set_title('Autoencoder Training')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # CNN í•™ìŠµ ê³¡ì„ 
    axes[0, 1].plot(history_cnn.history['loss'], label='Train Loss')
    axes[0, 1].plot(history_cnn.history['val_loss'], label='Val Loss')
    axes[0, 1].set_title('CNN Training')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # MAE ë¹„êµ
    mae_comparison = [train_metrics['mae'], val_metrics['mae']]
    axes[0, 2].bar(['Training', 'Validation'], mae_comparison, color=['blue', 'orange'])
    axes[0, 2].set_title('MAE Comparison')
    axes[0, 2].set_ylabel('MAE')
    
    # ìƒê´€ê´€ê³„ ë¹„êµ
    corr_comparison = [train_metrics['correlation'], val_metrics['correlation']]
    axes[1, 0].bar(['Training', 'Validation'], corr_comparison, color=['green', 'red'])
    axes[1, 0].set_title('Correlation Comparison')
    axes[1, 0].set_ylabel('Correlation')
    
    # RÂ² ë¹„êµ
    r2_comparison = [train_metrics['r2'], val_metrics['r2']]
    axes[1, 1].bar(['Training', 'Validation'], r2_comparison, color=['purple', 'brown'])
    axes[1, 1].set_title('RÂ² Score Comparison')
    axes[1, 1].set_ylabel('RÂ² Score')
    
    # ì„±ëŠ¥ ìš”ì•½
    summary_text = f"""Performance Summary:

Training:
â€¢ MAE: {train_metrics['mae']:.4f}
â€¢ Correlation: {train_metrics['correlation']:.4f}
â€¢ RÂ²: {train_metrics['r2']:.4f}

Validation:
â€¢ MAE: {val_metrics['mae']:.4f}
â€¢ Correlation: {val_metrics['correlation']:.4f}
â€¢ RÂ²: {val_metrics['r2']:.4f}

Generalization:
â€¢ MAE Ratio: {val_metrics['mae']/train_metrics['mae']:.2f}
â€¢ Correlation Drop: {train_metrics['correlation'] - val_metrics['correlation']:.4f}
"""
    axes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes,
                    fontsize=10, verticalalignment='top', fontfamily='monospace')
    axes[1, 2].set_title('Performance Summary')
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… Results visualization saved as 'training_results.png'")

# -----------------------------------
# Main Test Function with Comprehensive Evaluation
# -----------------------------------
# -----------------------------------
# Main Test Function with Comprehensive Evaluation
# -----------------------------------
def fit_word2vec_processor(all_texts, vector_size=256):
    print(f"ğŸ§© Fitting Word2Vec processor on {len(all_texts)} texts (once, shared for train/val)...")
    processor = SimpleWord2VecProcessor(vector_size=vector_size)
    # train_and_vectorizeëŠ” ë²¡í„°ë¥¼ ë°”ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë‚´ë¶€ ëª¨ë¸ë§Œ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ í•œ ë²ˆ í˜¸ì¶œ í›„ ì €ì¥
    _ = processor.train_and_vectorize(all_texts)
    return processor

def test_pipeline():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ - ê°œì„ ëœ í‰ê°€ ì‹œìŠ¤í…œ"""
    print("ğŸš€ Starting Comprehensive Training and Evaluation Pipeline")
    print("="*70)
    
    start_time = time.time()
    
    # 1. ë°ì´í„° ë¡œë”©
    texts = load_text_data(max_samples=10000)

    print(f"\nğŸ§© Fit shared textâ†’vector processor")
    processor = fit_word2vec_processor(texts, vector_size=256)

    print(f"\nğŸ“Š Data Split: 7,000 training / 3,000 validation")
    train_texts, val_texts = train_test_split(
        texts, train_size=7000, test_size=3000, random_state=42, shuffle=True
    )
    print(f"   âœ… Actual split: {len(train_texts)} train, {len(val_texts)} validation")

    print(f"\nğŸ¨ Phase 1: Mosaic Generation (per-sample)")
    train_mosaic, train_vec = generate_mosaic(train_texts, processor=processor, vector_size=256)
    val_mosaic, val_vec = generate_mosaic(val_texts, processor=processor, vector_size=256)

    input_shape = train_mosaic.shape[1:]  # (H,W,C)
    print(f"   Input shape: {input_shape}")
    
    # 3. Autoencoder ì••ì¶• í•™ìŠµ (ì‹¤ì œ ì••ì¶•ì´ ê°€ëŠ¥í•œ ë²„ì „)
    print(f"\nğŸ¤– Phase 2: Autoencoder Training with Real Compression")
    autoencoder, encoder = build_autoencoder(input_shape)
    
    # ì½œë°± ì„¤ì • (ì¡°ê¸° ì¢…ë£Œ ê°•í™”)
    ae_callbacks = [
        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, min_delta=1e-5),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)
    ]
    
    # Autoencoder í•™ìŠµ (ì‹¤ì œ ì••ì¶• ìµœì í™”)
    print("   Training autoencoder for real compression...")
    history_ae = autoencoder.fit(
        train_mosaic, train_mosaic,
        epochs=40,           # ì—í¬í¬ ê°ì†Œ (50â†’40)
        batch_size=32,       # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (16â†’32)
        validation_data=(val_mosaic, val_mosaic),
        callbacks=ae_callbacks,
        verbose=1
    )
    
    # 4. ì‹¤ì œ ì••ì¶•ë³¸ ìƒì„± (encoderë§Œ ì‚¬ìš©)
    print(f"\nğŸ—œï¸ Phase 3: Generating Real Compressed Representations")
    compressed_train = encoder.predict(train_mosaic, verbose=0)
    compressed_val = encoder.predict(val_mosaic, verbose=0)
    
    print(f"   Compressed train shape: {compressed_train.shape}")
    print(f"   Compressed val shape: {compressed_val.shape}")
    
    # ì••ì¶• íš¨ìœ¨ì„± ê³„ì‚°
    original_size = np.prod(train_mosaic.shape[1:])
    compressed_size = np.prod(compressed_train.shape[1:])
    compression_ratio = original_size / compressed_size
    print(f"   Per-sample sizes â†’ original: {original_size}, compressed: {compressed_size}")
    print(f"   Compression ratio: {compression_ratio:.2f}:1")
    
    # 5. CNN ì˜ˆì¸¡ í•™ìŠµ (ë°ì´í„° ì¦ê°• í¬í•¨)
    print(f"\nğŸ§  Phase 4: CNN Prediction Training with Data Augmentation")
    vector_size = train_vec.shape[1]
    cnn = build_residual_attention_cnn(compressed_train.shape[1:], output_size=vector_size)

    # ìƒ˜í”Œë³„ íƒ€ê¹ƒ: ì›ë˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ê·¸ëŒ€ë¡œ íšŒê·€
    y_train = train_vec  # (N, vector_size)
    y_val = val_vec      # (N, vector_size)
    print(f"   Target vector dimension: {vector_size} (per-sample regression to original embedding)")

    # ë°ì´í„° ì¦ê°•: ë” ë¶€ë“œëŸ¬ìš´ ì¦ê°• ì ìš©
    print("   Applying gentle data augmentation...")
    augmented_train = [compressed_train]
    augmented_targets = [y_train]
    for i in range(2):
        noise_level = 0.001 * (i + 1)
        augmented_batch = compressed_train + np.random.normal(0, noise_level, compressed_train.shape)
        augmented_train.append(augmented_batch)
        augmented_targets.append(y_train)
    X_train_aug = np.vstack(augmented_train)
    y_train_aug = np.vstack(augmented_targets)
    print(f"   Augmented training data shape: {X_train_aug.shape}, targets: {y_train_aug.shape}")
    
    # CNN ì½œë°± ì„¤ì • (ë” ê´€ëŒ€í•œ ì¡°ê¸° ì¢…ë£Œ)
    cnn_callbacks = [
        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, min_delta=1e-6),
        ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=8, min_lr=1e-6, verbose=1)
    ]
    
    # CNN í•™ìŠµ (ë” ê¸´ í•™ìŠµ í—ˆìš©)
    print("   Training simplified CNN...")
    history_cnn = cnn.fit(
        X_train_aug, y_train_aug,
        epochs=120,         # ì—í¬í¬ ì¦ê°€ 
        batch_size=8,       # ì›ë˜ ë°°ì¹˜ í¬ê¸°ë¡œ ë³µì›
        validation_data=(compressed_val, y_val),
        callbacks=cnn_callbacks,
        verbose=1
    )
    
    # 6. ìµœì¢… í‰ê°€ ë° Robustness í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“Š Phase 5: Comprehensive Evaluation with Robustness Analysis")

    # ì˜ˆì¸¡ ìˆ˜í–‰
    train_pred = cnn.predict(compressed_train, verbose=0)
    val_pred = cnn.predict(compressed_val, verbose=0)

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    train_metrics = calculate_metrics(y_train, train_pred, "Training")
    val_metrics = calculate_metrics(y_val, val_pred, "Validation")
    
    # Robustness í…ŒìŠ¤íŠ¸ (ë…¸ì´ì¦ˆ ì¶”ê°€ëœ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì¼ê´€ì„± í™•ì¸)
    print(f"\nğŸ›¡ï¸ Robustness Analysis:")
    robustness_scores = []
    for noise_level in [0.001, 0.005, 0.01]:
        noisy_val = compressed_val + np.random.normal(0, noise_level, compressed_val.shape)
        noisy_pred = cnn.predict(noisy_val, verbose=0)
        consistency = np.corrcoef(val_pred.flatten(), noisy_pred.flatten())[0, 1]
        robustness_scores.append(consistency)
        print(f"   â€¢ Noise level {noise_level:.3f}: {consistency:.3f} consistency")
    
    avg_robustness = np.mean(robustness_scores)
    print(f"   â€¢ Average robustness: {avg_robustness:.3f}")
    
    if avg_robustness > 0.9:
        robustness_grade = "ğŸ›¡ï¸ Highly robust"
    elif avg_robustness > 0.8:
        robustness_grade = "ğŸ‘ Moderately robust"
    else:
        robustness_grade = "âš ï¸ Low robustness"
    
    print(f"   â€¢ Robustness grade: {robustness_grade}")
    
    # 7. ì¼ë°˜í™” ì„±ëŠ¥ ë¶„ì„ (ê°œì„ ëœ ê¸°ì¤€)
    print(f"\nğŸ¯ Generalization Analysis:")
    mae_ratio = val_metrics['mae'] / train_metrics['mae']
    corr_drop = train_metrics['correlation'] - val_metrics['correlation']
    
    print(f"   â€¢ MAE Ratio (val/train): {mae_ratio:.3f}")
    print(f"   â€¢ Correlation Drop: {corr_drop:.4f}")
    print(f"   â€¢ Compression Efficiency: {compression_ratio:.1f}:1")
    
    # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì • (ê°œì„ ëœ ê¸°ì¤€)
    if val_metrics['correlation'] > 0.85 and val_metrics['r2'] > 0.7 and mae_ratio < 1.2:
        grade = "ğŸ† Excellent"
    elif val_metrics['correlation'] > 0.75 and val_metrics['r2'] > 0.5 and mae_ratio < 1.5:
        grade = "âœ… Good"  
    elif val_metrics['correlation'] > 0.6 and val_metrics['r2'] > 0.3:
        grade = "ğŸ‘ Moderate"
    else:
        grade = "âš ï¸ Needs Improvement"
    
    print(f"   â€¢ Overall Performance: {grade}")
    
    # ì¼ë°˜í™” ë“±ê¸‰
    if mae_ratio < 1.0 and corr_drop < 0.05:
        gen_grade = "ğŸ¯ Excellent Generalization"
    elif mae_ratio < 1.2 and corr_drop < 0.1:
        gen_grade = "ğŸ‘ Good Generalization"
    elif mae_ratio < 1.5:
        gen_grade = "ğŸ‘ Moderate Generalization" 
    else:
        gen_grade = "âš ï¸ Poor Generalization"
    
    print(f"   â€¢ Generalization: {gen_grade}")
    
    # 8. ì‹œê°í™” ë° ê²°ê³¼ ì €ì¥
    print(f"\nğŸ“ˆ Generating Results Visualization...")
    plot_results(history_ae, history_cnn, train_metrics, val_metrics)
    
    # ì‹¤í–‰ ì‹œê°„
    total_time = time.time() - start_time
    print(f"\nâ±ï¸ Total execution time: {total_time:.2f} seconds")
    
    # 9. ìµœì¢… ìš”ì•½ ë¦¬í¬íŠ¸ (ê°œì„ ëœ ë²„ì „)
    print(f"\n{'='*70}")
    print("ğŸ“ ENHANCED PERFORMANCE REPORT")
    print(f"{'='*70}")
    print(f"ğŸ“Š Dataset: {len(texts)} samples ({len(train_texts)} train / {len(val_texts)} val)")
    print(f"ğŸ—œï¸ Compression: {compression_ratio:.1f}:1 ratio")
    print(f"âš™ï¸ Optimization: Batch size 8 + Data Augmentation + Low LR (0.0005)")
    print(f"ğŸ“ˆ Training Performance:")
    print(f"   â€¢ Correlation: {train_metrics['correlation']:.4f} ({train_metrics['correlation']*100:.1f}%)")
    print(f"   â€¢ RÂ² Score: {train_metrics['r2']:.4f} ({train_metrics['r2']*100:.1f}%)")
    print(f"   â€¢ MAE: {train_metrics['mae']:.4f}")
    print(f"ğŸ¯ Validation Performance:")
    print(f"   â€¢ Correlation: {val_metrics['correlation']:.4f} ({val_metrics['correlation']*100:.1f}%)")
    print(f"   â€¢ RÂ² Score: {val_metrics['r2']:.4f} ({val_metrics['r2']*100:.1f}%)")
    print(f"   â€¢ MAE: {val_metrics['mae']:.4f}")
    print(f"ğŸ“Š Generalization Metrics:")
    print(f"   â€¢ MAE Ratio: {mae_ratio:.3f}")
    print(f"   â€¢ Correlation Drop: {corr_drop:.4f}")
    print(f"   â€¢ Evaluation: {gen_grade}")
    print(f"ğŸ›¡ï¸ Robustness: {avg_robustness:.3f} ({robustness_grade})")
    print(f"ğŸ† Overall Grade: {grade}")
    print(f"â±ï¸ Training Time: {total_time:.1f}s")
    print(f"{'='*70}")
    
    return {
        'train_metrics': train_metrics,
        'val_metrics': val_metrics,
        'compression_ratio': compression_ratio,
        'training_time': total_time,
        'grade': grade,
        'generalization_grade': gen_grade,
        'robustness_score': avg_robustness,
        'robustness_grade': robustness_grade,
        'mae_ratio': mae_ratio
    }

if __name__ == "__main__":
    test_pipeline()
