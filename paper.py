
import tensorflow as tf
from tensorflow.keras.layers import Input, TextVectorization, Embedding, LSTM, Dense, Reshape, Conv2D, MaxPooling2D, GlobalAveragePooling2D
from tensorflow.keras.models import Model
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# --- 1. ê¸°ë³¸ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„ ---

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
tf.random.set_seed(42)
np.random.seed(42)

def load_patient_data(file_path, n_samples=2000):
    """patients.csv íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"--- Loading data from {file_path} ---")
    try:
        df = pd.read_csv(file_path, nrows=n_samples)
        print(f"âœ… Successfully loaded {len(df)} records.")

        # í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ì»¬ëŸ¼ ì„ íƒ
        # ì—¬ëŸ¬ ì¸êµ¬í†µê³„í•™ì  ì •ë³´ë¥¼ ì¡°í•©í•˜ì—¬ í™˜ìì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤.
        cols_to_use = ['RACE', 'ETHNICITY', 'GENDER', 'MARITAL', 'BIRTHPLACE']
        df[cols_to_use] = df[cols_to_use].fillna('unknown') # ê²°ì¸¡ê°’ ì²˜ë¦¬

        # ì„ íƒëœ ì»¬ëŸ¼ë“¤ì„ í•©ì³ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        patient_texts = df[cols_to_use].apply(lambda row: ' '.join(row.values.astype(str)), axis=1).tolist()
        print(f"âœ… Generated {len(patient_texts)} text samples.")
        print(f"   Example: "{patient_texts[0]}""")
        return patient_texts

    except FileNotFoundError:
        print(f"âŒ ERROR: Data file not found at '{file_path}'")
        print("Please ensure 'patients.csv' is in the same directory.")
        return None
    except Exception as e:
        print(f"âŒ An error occurred: {e}")
        return None

# ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
VOCAB_SIZE = 2000  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸° (ë°ì´í„°ê°€ ë‹¤ì–‘í•´ì¡Œìœ¼ë¯€ë¡œ ì¦ê°€)
MAX_SEQUENCE_LENGTH = 25  # ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ (ì»¬ëŸ¼ ì¡°í•©ìœ¼ë¡œ ê¸¸ì–´ì§)
EMBEDDING_DIM = 128
LATENT_DIM = 256
IMAGE_SIZE = 32

# ë°ì´í„° ë¡œë“œ
sample_texts = load_patient_data('patients.csv')

# ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í•˜ë©´ ìŠ¤í¬ë¦½íŠ¸ ì¤‘ë‹¨
if sample_texts is None:
    exit()

# í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ë ˆì´ì–´
text_vectorizer = TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_sequence_length=MAX_SEQUENCE_LENGTH,
    name="text_vectorizer"
)
# í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì–´íœ˜ ì‚¬ì „ì„ êµ¬ì¶•
text_vectorizer.adapt(sample_texts)


# --- 2. ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ëª¨ë¸ ì •ì˜ ---

class EndToEndTextToImageSystem:
    def __init__(self, text_vectorizer, embedding_dim, latent_dim, image_size):
        self.text_vectorizer = text_vectorizer
        self.vocab_size = text_vectorizer.get_vocabulary_size()
        self.embedding_dim = embedding_dim
        self.latent_dim = latent_dim
        self.image_size = image_size
        self.image_shape = (image_size, image_size, 3)

    def build_client_model(self):
        text_input = Input(shape=(1,), dtype=tf.string, name="client_text_input")
        vectorized_text = self.text_vectorizer(text_input)
        embedded_text = Embedding(self.vocab_size, self.embedding_dim, name="embedding")(vectorized_text)
        latent_vector = LSTM(self.latent_dim, name="lstm_encoder")(embedded_text)
        x = Dense(512, activation='relu', name="img_gen_dense_1")(latent_vector)
        x = Dense(1024, activation='relu', name="img_gen_dense_2")(x)
        x = Dense(self.image_size * self.image_size * 3, activation='sigmoid', name="img_gen_dense_3")(x)
        image_output = Reshape(self.image_shape, name="client_image_output")(x)
        client_model = Model(text_input, image_output, name="client_text_to_image_model")
        print("âœ… Client Model (Text -> Image) built.")
        return client_model

    def build_enhanced_server_model(self):
        """
        ì„œë²„ ì¸¡ ëª¨ë¸ (Image -> Vector)
        - ì†ì‹¤ë¥ ì„ ì¤„ì´ê¸° ìœ„í•´ Conv2D ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ê¹Šê²Œ ìŒ“ì€ êµ¬ì¡°
        """
        image_input = Input(shape=self.image_shape, name="server_image_input")

        # --- (A) ì´ë¯¸ì§€ ì¸ì½”ë” ë¶€ë¶„ (ë” ê¹Šì–´ì§„ CNN êµ¬ì¡°) ---
        # 1ì°¨ íŠ¹ì§• ì¶”ì¶œ
        x = Conv2D(64, 3, activation='relu', padding='same')(image_input)
        x = Conv2D(64, 3, activation='relu', padding='same')(x)
        x = MaxPooling2D((2, 2))(x)  # ì´ë¯¸ì§€ í¬ê¸° ì ˆë°˜ìœ¼ë¡œ ì¤„ì„ (16x16)

        # 2ì°¨ íŠ¹ì§• ì¶”ì¶œ
        x = Conv2D(128, 3, activation='relu', padding='same')(x)
        x = Conv2D(128, 3, activation='relu', padding='same')(x)
        x = MaxPooling2D((2, 2))(x)  # ì´ë¯¸ì§€ í¬ê¸° ë‹¤ì‹œ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„ (8x8)
        
        # 3ì°¨ íŠ¹ì§• ì¶”ì¶œ
        x = Conv2D(256, 3, activation='relu', padding='same')(x)
        x = Conv2D(256, 3, activation='relu', padding='same')(x)
        
        # íŠ¹ì§• ìš”ì•½
        x = GlobalAveragePooling2D(name="gap_pooling")(x)
        
        # ì˜ë¯¸ ë²¡í„° ë³µì›
        reconstructed_latent_vector = Dense(self.latent_dim, name="reconstructed_latent_vector")(x)

        server_model = Model(image_input, reconstructed_latent_vector, name="server_image_to_vector_model")
        print("âœ… Enhanced Server Model (Image -> Vector) built.")
        return server_model

    def build_end_to_end_for_training(self, client_model, server_model):
        text_input = client_model.input
        latent_vector_true = client_model.get_layer("lstm_encoder").output
        generated_image = client_model(text_input)
        reconstructed_latent_vector = server_model(generated_image)
        training_model = Model(text_input, reconstructed_latent_vector, name="training_autoencoder")
        print("âœ… End-to-End Training Model built.")
        return training_model


# --- 3. ëª¨ë¸ ìƒì„±, ì»´íŒŒì¼ ë° í•™ìŠµ ---

print("\n--- Building and Training Model ---")
# ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
text_to_image_system = EndToEndTextToImageSystem(text_vectorizer, EMBEDDING_DIM, LATENT_DIM, IMAGE_SIZE)

# í´ë¼ì´ì–¸íŠ¸ì™€ ì„œë²„ ëª¨ë¸ ë¹Œë“œ (ì„œë²„ëŠ” ê°•í™”ëœ ë²„ì „ ì‚¬ìš©)
client_model = text_to_image_system.build_client_model()
server_model = text_to_image_system.build_enhanced_server_model()

# í›ˆë ¨ìš© ëª¨ë¸ ë¹Œë“œ ë° ì»´íŒŒì¼
training_model = text_to_image_system.build_end_to_end_for_training(client_model, server_model)
training_model.compile(optimizer='adam', loss='mse')

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
training_model.summary()

# ë°ì´í„°ì…‹ ìƒì„±
# TextVectorization ë ˆì´ì–´ëŠ” tf.data.Datasetì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
train_dataset = tf.data.Dataset.from_tensor_slices(sample_texts).batch(32)

# í›ˆë ¨ì„ ìœ„í•œ ì…ë ¥(x)ê³¼ ì •ë‹µ(y)ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
# ì´ ëª¨ë¸ì˜ ëª©í‘œëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„, ê·¸ í…ìŠ¤íŠ¸ì˜ 'ì˜ë¯¸ ë²¡í„°'ë¥¼ ë³µì›í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
text_to_vector_model = Model(client_model.input, client_model.get_layer("lstm_encoder").output)

def get_training_data(text_batch):
    original_vectors = text_to_vector_model(text_batch)
    return text_batch, original_vectors

train_dataset = train_dataset.map(get_training_data)

print("\n--- Starting Model Training ---")
# ëª¨ë¸ í›ˆë ¨ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë” ë§ì€ epoch í•„ìš”)
history = training_model.fit(train_dataset, epochs=15, verbose=1)
print("âœ… Training finished.")


# --- 4. ê²°ê³¼ ì‹œì—° ---

print("\n--- Demonstrating the End-to-End Flow (After Training) ---")

# 1. í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ì„ íƒ
test_text = sample_texts[10]
print(f"Original Text: '{test_text}'")
input_text_tensor = tf.constant([test_text])

# 2. í´ë¼ì´ì–¸íŠ¸: í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
generated_image = client_model.predict(input_text_tensor)

# 3. ì„œë²„: ì´ë¯¸ì§€ë¥¼ ì¤‘ê°„ ë²¡í„°ë¡œ ë³µì›
reconstructed_vector = server_model.predict(generated_image)

# 4. ë¹„êµ: ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì‹¤ì œ ì¤‘ê°„ ë²¡í„°ì™€ ì„œë²„ê°€ ë³µì›í•œ ë²¡í„° ë¹„êµ
original_vector = text_to_vector_model.predict(input_text_tensor)
mse_loss = np.mean((original_vector - reconstructed_vector)**2)
print(f"MSE between original and reconstructed vectors (after training): {mse_loss:.6f}")
print("\nğŸ’¡ A low MSE after training would prove that the image successfully carried the text's information.")

# 5. ì‹œê°í™”
plt.figure(figsize=(12, 5))
plt.subplot(1, 3, 1)
plt.plot(history.history['loss'])
plt.title('Training Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')

plt.subplot(1, 3, 2)
plt.imshow(generated_image[0])
plt.title("Generated Mosaic Image")
plt.axis('off')

plt.subplot(1, 3, 3)
plt.plot(original_vector[0], label='Original Vector', color='blue', alpha=0.7)
plt.plot(reconstructed_vector[0], label='Reconstructed Vector', color='red', linestyle='--', alpha=0.7)
plt.title("Vector Comparison")
plt.legend()
plt.suptitle("End-to-End Pipeline Results (Trained)")
plt.tight_layout()
plt.show()
